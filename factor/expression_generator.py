"""
è¡¨è¾¾å¼ç”Ÿæˆå™¨æ¨¡å—
è´Ÿè´£ç”Ÿæˆå’ŒéªŒè¯å› å­è¡¨è¾¾å¼
"""

import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from torch.distributions import Categorical
from typing import Dict, List, Tuple, Optional
import logging

try:
    from torch.amp import autocast
except ImportError:
    from torch.cuda.amp import autocast

logger = logging.getLogger(__name__)


class ExpressionGenerator:
    """è¡¨è¾¾å¼ç”Ÿæˆå™¨ - ä½¿ç”¨PPOç­–ç•¥ç½‘ç»œç”Ÿæˆå› å­è¡¨è¾¾å¼"""

    def __init__(self,
                 actor_critic,
                 vocab: List[str],
                 token_to_id: Dict[str, int],
                 id_to_token: Dict[int, str],
                 operators: Dict,
                 feature_names: List[str],
                 feature_scales: Dict[str, float],
                 max_expr_len: int = 20,
                 device: torch.device = None,
                 use_amp: bool = False):
        """
        åˆå§‹åŒ–è¡¨è¾¾å¼ç”Ÿæˆå™¨

        Args:
            actor_critic: PPOçš„Actor-Criticç½‘ç»œ
            vocab: è¯æ±‡è¡¨
            token_to_id: tokenåˆ°idçš„æ˜ å°„
            id_to_token: idåˆ°tokençš„æ˜ å°„
            operators: æ“ä½œç¬¦å­—å…¸
            feature_names: ç‰¹å¾åç§°åˆ—è¡¨
            feature_scales: ç‰¹å¾æ•°é‡çº§å­—å…¸
            max_expr_len: æœ€å¤§è¡¨è¾¾å¼é•¿åº¦
            device: è®¡ç®—è®¾å¤‡
            use_amp: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦
        """
        self.actor_critic = actor_critic
        self.vocab = vocab
        self.token_to_id = token_to_id
        self.id_to_token = id_to_token
        self.operators = operators
        self.feature_names = feature_names
        self.feature_scales = feature_scales
        self.max_expr_len = max_expr_len
        self.device = device or torch.device('cpu')
        self.use_amp = use_amp

        self.pad_token_id = token_to_id['<PAD>']

    def generate_expression_batch(self, batch_size: int = 8) -> List[Tuple[List[str], List[int], Dict]]:
        """
        æ‰¹é‡ç”Ÿæˆè¡¨è¾¾å¼

        Args:
            batch_size: æ‰¹å¤§å°

        Returns:
            List[(tokens, state_ids, trajectory)]
        """
        batch_states = [[self.token_to_id['<BEG>']] for _ in range(batch_size)]
        batch_tokens = [['<BEG>'] for _ in range(batch_size)]
        batch_finished = [False] * batch_size
        batch_trajectories = [
            {
                'states': [],
                'actions': [],
                'types': [],
                'type_log_probs': [],
                'action_log_probs': [],
                'values': []
            }
            for _ in range(batch_size)
        ]

        for step in range(self.max_expr_len - 1):
            active_indices = [i for i in range(batch_size) if not batch_finished[i]]

            if not active_indices:
                break

            active_states = [batch_states[i] for i in active_indices]
            active_valid_info = [self._get_valid_actions(state) for state in active_states]

            with torch.no_grad():
                if self.use_amp:
                    with autocast(device_type='cuda'):
                        type_logits_batch, action_logits_batch, values_batch = \
                            self.actor_critic.forward_batch(active_states)
                else:
                    type_logits_batch, action_logits_batch, values_batch = \
                        self.actor_critic.forward_batch(active_states)

            for idx, i in enumerate(active_indices):
                valid_types, valid_actions_by_type = active_valid_info[idx]

                if len(valid_types) == 1 and valid_types[0] == 2:
                    batch_tokens[i].append('<SEP>')
                    batch_finished[i] = True
                    continue

                value = values_batch[idx]

                # Step 1: é‡‡æ ·åŠ¨ä½œç±»å‹
                type_logits = type_logits_batch[idx]
                type_mask = torch.full((3,), float('-inf'), device=self.device)
                type_mask[valid_types] = 0.0
                masked_type_logits = type_logits + type_mask

                type_probs = F.softmax(masked_type_logits, dim=-1)
                type_dist = Categorical(type_probs)
                action_type_tensor = type_dist.sample()
                action_type = action_type_tensor.item()
                type_log_prob = type_dist.log_prob(action_type_tensor).item()

                # Step 2: é‡‡æ ·å…·ä½“åŠ¨ä½œ
                valid_actions_for_type = valid_actions_by_type[action_type]
                action_logits = action_logits_batch[idx]

                action_mask = torch.full((len(self.vocab),), float('-inf'), device=self.device)
                action_mask[valid_actions_for_type] = 0.0
                masked_action_logits = action_logits + action_mask

                action_probs = F.softmax(masked_action_logits, dim=-1)
                action_dist = Categorical(action_probs)
                action_tensor = action_dist.sample()
                action = action_tensor.item()
                action_log_prob = action_dist.log_prob(action_tensor).item()

                # ä¿å­˜trajectory
                batch_trajectories[i]['states'].append(batch_states[i].copy())
                batch_trajectories[i]['types'].append(action_type)
                batch_trajectories[i]['actions'].append(action)
                batch_trajectories[i]['type_log_probs'].append(type_log_prob)
                batch_trajectories[i]['action_log_probs'].append(action_log_prob)
                batch_trajectories[i]['values'].append(value.item())

                token = self.id_to_token[action]
                batch_tokens[i].append(token)
                batch_states[i].append(action)

                if token == '<SEP>':
                    batch_finished[i] = True

        results = []
        for i in range(batch_size):
            results.append((batch_tokens[i], batch_states[i], batch_trajectories[i]))

        return results

    def _get_valid_actions(self, state: List[int]) -> Tuple[List[int], Dict[int, List[int]]]:
        """
        å±‚æ¬¡åŒ–åŠ¨ä½œé€‰æ‹© - ä¸¥æ ¼ä¿è¯RPNæ ˆå¹³è¡¡

        è¯­æ³•çº¦æŸå¼ºåŒ–ç­–ç•¥:
        1. æœ€å°é•¿åº¦: è‡³å°‘ <BEG> feature operator <SEP> (len>=4)
        2. å¼ºåˆ¶ç»“æŸ: stack==1 ä¸” len>=4 â†’ å¿…é¡»è¾“å‡º <SEP>
        3. ç¦æ­¢æ—©åœ: stack!=1 â†’ ç¦æ­¢è¾“å‡º <SEP>
        4. é˜²æ­¢æº¢å‡º: æ¥è¿‘max_lenæ—¶æå‰å¼ºåˆ¶ç»“æŸ
        5. æ“ä½œç¬¦çº¦æŸ: åªæœ‰æ ˆè¶³å¤Ÿå¤§æ—¶æ‰èƒ½ä½¿ç”¨å¯¹åº”arityçš„æ“ä½œç¬¦

        Returns:
            (valid_types, valid_actions_by_type)
        """
        current_len = len(state)
        MIN_VALID_LEN = 4  # <BEG> feature op <SEP>

        # ğŸ”¥ çº¦æŸ1: æ¥è¿‘æœ€å¤§é•¿åº¦æ—¶å¼ºåˆ¶ç»“æŸ
        # ç•™3ä¸ªtokençš„buffer: feature + operator + <SEP>
        if current_len >= self.max_expr_len - 3:
            # å¦‚æœå·²ç»è¾¾åˆ°æœ€å°é•¿åº¦ä¸”æ ˆå¹³è¡¡,å¼ºåˆ¶ç»“æŸ
            stack_size = self._calculate_stack_size(state)
            if stack_size == 1 and current_len >= MIN_VALID_LEN:
                return [2], {2: [self.token_to_id['<SEP>']]}
            # å¦åˆ™ä¹Ÿè¦å¼ºåˆ¶ç»“æŸ(å…œåº•,é¿å…æˆªæ–­)
            return [2], {2: [self.token_to_id['<SEP>']]}

        stack_size = self._calculate_stack_size(state)
        scale_stack = self._get_scale_stack(state)

        valid_types = []
        valid_actions_by_type = {}

        # ğŸ”¥ çº¦æŸ2: å½“stack==1ä¸”è¾¾åˆ°æœ€å°é•¿åº¦æ—¶,åªèƒ½ç»“æŸ
        if stack_size == 1 and current_len >= MIN_VALID_LEN:
            return [2], {2: [self.token_to_id['<SEP>']]}

        # ğŸ”¥ çº¦æŸ3: åªæœ‰æ ˆæœ‰æ•ˆ(stack>=0)æ—¶æ‰èƒ½æ·»åŠ feature/operator
        if stack_size >= 0:
            # Type 0: Features - ç¡®ä¿æ·»åŠ åæœ‰è¶³å¤Ÿç©ºé—´å®Œæˆè¡¨è¾¾å¼
            # æ·»åŠ featureåæœ€å°‘éœ€è¦: operator(1) + <SEP>(1) = 2ä¸ªtoken
            if current_len + 2 < self.max_expr_len:
                feature_actions = [self.token_to_id[f] for f in self.feature_names]
                if feature_actions:
                    valid_types.append(0)
                    valid_actions_by_type[0] = feature_actions

            # Type 1: Operators - ä¸¥æ ¼æ£€æŸ¥æ ˆå¤§å°å’Œæ•°é‡çº§
            operator_actions = []
            for op_name, op_info in self.operators.items():
                arity = op_info['arity']

                # ğŸ”¥ æ ¸å¿ƒçº¦æŸ: æ ˆå¿…é¡»è¶³å¤Ÿå¤§æ‰èƒ½åº”ç”¨è¯¥æ“ä½œç¬¦
                if stack_size >= arity:
                    # æ£€æŸ¥æ•°é‡çº§å…¼å®¹æ€§
                    if self._is_operator_scale_compatible(op_name, scale_stack):
                        # ğŸ”¥ é¢å¤–çº¦æŸ: åº”ç”¨æ“ä½œåæ ˆå¤§å°ä¼šå˜ä¸º stack - arity + 1
                        # å¿…é¡»ç¡®ä¿åº”ç”¨åè‡³å°‘æœ‰1ä¸ªå…ƒç´ (æœ€ç»ˆèƒ½ç»“æŸ)
                        new_stack_size = stack_size - arity + 1
                        if new_stack_size >= 1:
                            operator_actions.append(self.token_to_id[op_name])

            if operator_actions:
                valid_types.append(1)
                valid_actions_by_type[1] = operator_actions

        # Type 2: End - ğŸ”¥ çº¦æŸ4: åªæœ‰stack==1æ—¶æ‰èƒ½ç»“æŸ
        # æ³¨æ„: è¿™é‡Œä¸æ·»åŠ ,å› ä¸ºä¸Šé¢å·²ç»å¤„ç†äº†stack==1çš„æƒ…å†µ(å¼ºåˆ¶è¿”å›)
        # è¿™ä¸ªåˆ†æ”¯æ°¸è¿œä¸ä¼šæ‰§è¡Œ,ä½†ä¿ç•™ä½œä¸ºé€»è¾‘å®Œæ•´æ€§

        # ğŸ”¥ çº¦æŸ5: å¦‚æœæ²¡æœ‰æœ‰æ•ˆåŠ¨ä½œ,å¼ºåˆ¶ç»“æŸ(å…œåº•,é˜²æ­¢æ­»é”)
        if not valid_types:
            # è¿™ç§æƒ…å†µç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿ,ä½†ä½œä¸ºå®‰å…¨æªæ–½
            logger.warning(f"No valid actions at state len={current_len}, stack={stack_size}, forcing <SEP>")
            return [2], {2: [self.token_to_id['<SEP>']]}

        return valid_types, valid_actions_by_type

    def _calculate_stack_size(self, state: List[int]) -> int:
        """è®¡ç®—è¡¨è¾¾å¼æ ˆçš„å¤§å°"""
        if len(state) <= 1:
            return 0

        stack = 0
        for token_id in state[1:]:
            token = self.id_to_token[token_id]

            if token in self.feature_names:
                stack += 1
            elif token in self.operators:
                arity = self.operators[token]['arity']
                stack = stack - arity + 1
                if stack < 0:
                    return -1
            elif token == '<SEP>':
                break

        return stack

    def _get_scale_stack(self, state: List[int]) -> List[float]:
        """è·å–å½“å‰è¡¨è¾¾å¼çš„æ•°é‡çº§æ ˆ"""
        scale_stack = []

        for token_id in state[1:]:  # è·³è¿‡<BEG>
            token = self.id_to_token[token_id]

            if token in self.feature_names:
                scale = self.feature_scales.get(token, 1.0)
                scale_stack.append(scale)

            elif token in self.operators:
                op_info = self.operators[token]
                arity = op_info['arity']

                if len(scale_stack) < arity:
                    return []

                operand_scales = []
                for _ in range(arity):
                    operand_scales.append(scale_stack.pop())
                operand_scales.reverse()

                # è®¡ç®—ç»“æœæ•°é‡çº§
                result_scale = self._compute_result_scale(token, operand_scales)
                scale_stack.append(result_scale)

            elif token == '<SEP>':
                break

        return scale_stack

    def _compute_result_scale(self, op_name: str, operand_scales: List[float]) -> float:
        """è®¡ç®—æ“ä½œç»“æœçš„æ•°é‡çº§"""
        arity = len(operand_scales)

        if arity == 1:
            # å½’ä¸€åŒ–åˆ°[0,1]çš„ä¸€å…ƒç®—å­
            if op_name in ['sigmoid', 'rank', 'ts_rank10']:
                return 1.0
            # å½’ä¸€åŒ–åˆ°[-1,1]çš„ä¸€å…ƒç®—å­
            elif op_name in ['tanh', 'sign']:
                return 1.0
            # RSIç­‰å›ºå®šèŒƒå›´ç®—å­
            elif op_name in ['rsi14']:
                return 50.0
            # ä¿æŒæ•°é‡çº§çš„ç®—å­
            elif op_name in ['abs', 'delay1', 'delay3']:
                return operand_scales[0]
            # æ”¹å˜æ•°é‡çº§ä¸ºæ— é‡çº²çš„ç®—å­
            elif op_name in ['log', 'exp']:
                return 1.0
            # ç›¸å¯¹å˜åŒ–ç±»ç®—å­
            elif op_name in ['delta1', 'momentum5', 'roc10']:
                return operand_scales[0] * 0.1
            # å¹³æ»‘ç±»ç®—å­
            elif op_name in ['sma5', 'sma10', 'sma20', 'ema5', 'ema10',
                            'wma10', 'dema10', 'tema10']:
                return operand_scales[0]
            # æ³¢åŠ¨ç‡ç±»ç®—å­
            elif op_name in ['std10', 'std20', 'variance20', 'mad20']:
                return operand_scales[0]
            # åˆ†ä½æ•°
            elif op_name in ['quantile20', 'ts_min10', 'ts_max10']:
                return operand_scales[0]
            # å¹‚ã€å¹³æ–¹æ ¹
            elif op_name in ['pow', 'sqrt']:
                return operand_scales[0]
            # æ ‡å‡†åŒ–ç±»ç®—å­
            elif op_name in ['zscore20', 'scale']:
                return 1.0
            # æŠ€æœ¯æŒ‡æ ‡
            elif op_name in ['macd', 'bb_upper', 'bb_lower']:
                return operand_scales[0]
            # åŠ æƒç±»ç®—å­
            elif op_name in ['decay10']:
                return operand_scales[0]
            # è¿ä¹˜ç®—å­
            elif op_name in ['ts_prod10']:
                return 1.0
            else:
                return operand_scales[0]

        elif arity == 2:
            if op_name in ['add', 'sub', 'max', 'min']:
                return np.mean(operand_scales)
            elif op_name == 'mul':
                return operand_scales[0] * operand_scales[1]
            elif op_name == 'div':
                if operand_scales[1] > 1e-10:
                    return operand_scales[0] / operand_scales[1]
                else:
                    return operand_scales[0]
            elif op_name in ['corr20', 'covar']:
                return 1.0
            else:
                return np.mean(operand_scales)

        elif arity == 3:
            if op_name == 'condition':
                return np.mean([operand_scales[1], operand_scales[2]])
            else:
                return np.mean(operand_scales)

        else:
            return np.mean(operand_scales) if operand_scales else 1.0

    def _is_operator_scale_compatible(self, op_name: str, scale_stack: List[float]) -> bool:
        """æ£€æŸ¥æ“ä½œç¬¦æ˜¯å¦ä¸å½“å‰æ•°é‡çº§æ ˆå…¼å®¹"""
        if op_name not in self.operators:
            return False

        op_info = self.operators[op_name]
        arity = op_info['arity']
        scale_rule = op_info.get('scale_rule', 'any')
        scale_threshold = op_info.get('scale_threshold', None)

        if len(scale_stack) < arity:
            return False

        if scale_rule == 'any':
            return True

        if scale_rule == 'similar_only':
            if arity == 2:
                scale1 = scale_stack[-2]
                scale2 = scale_stack[-1]

                if scale1 == 0 or scale2 == 0:
                    return scale1 == scale2

                ratio = max(scale1, scale2) / min(scale1, scale2)
                threshold = scale_threshold if scale_threshold is not None else 100.0
                return ratio <= threshold
            return True

        return True

    def tokens_to_expression(self, tokens: List[str]) -> str:
        """å°†RPNæ ¼å¼çš„tokensè½¬æ¢ä¸ºå¯è¯»è¡¨è¾¾å¼"""
        if not tokens or tokens[0] != '<BEG>' or tokens[-1] != '<SEP>':
            return 'INVALID_EXPRESSION'

        stack: List[str] = []
        expr_tokens = tokens[1:-1]

        try:
            for token in expr_tokens:
                if token in self.feature_names:
                    stack.append(token)
                elif token in self.operators:
                    op_info = self.operators[token]
                    arity = op_info['arity']
                    if len(stack) < arity:
                        return 'INVALID_EXPRESSION'

                    args = [stack.pop() for _ in range(arity)]
                    args.reverse()
                    arg_str = ', '.join(args)
                    expr = f"{token}({arg_str})"
                    stack.append(expr)
                else:
                    return 'INVALID_EXPRESSION'

            if len(stack) != 1:
                return 'INVALID_EXPRESSION'

            return stack[0]
        except Exception as exc:
            logger.debug(f"Expression stringify error: {exc}")
            return 'INVALID_EXPRESSION'
