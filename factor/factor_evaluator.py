"""
å› å­è¯„ä¼°å™¨æ¨¡å—
è´Ÿè´£è®¡ç®—å› å­å€¼å’Œè¯„ä¼°è¡¨è¾¾å¼
"""

import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Optional
import time
import sys
from pathlib import Path

# æ·»åŠ utilsç›®å½•åˆ°è·¯å¾„
utils_path = Path(__file__).parent.parent / 'utils'
if str(utils_path) not in sys.path:
    sys.path.insert(0, str(utils_path))

try:
    from advanced_reward import AdvancedRewardCalculator, RewardConfig
    ADVANCED_REWARD_AVAILABLE = True
except ImportError:
    ADVANCED_REWARD_AVAILABLE = False
    logging.warning("AdvancedRewardCalculator not available, using simple reward")

logger = logging.getLogger(__name__)


class FactorEvaluator:
    """
    å› å­è¯„ä¼°å™¨ - è´Ÿè´£è®¡ç®—å› å­å€¼å’Œè¯„ä¼°è¡¨è¾¾å¼
    
    ä¿®æ”¹è¯´æ˜ (å®ç›˜é€‚é…):
    1. å¼•å…¥äº† Train/Val çŠ¶æ€åŒºåˆ†ï¼Œç¡®ä¿æ¸…æ´—éªŒè¯é›†æ•°æ®æ—¶ä½¿ç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡é‡ã€‚
    2. ä¿®å¤äº†æ•°æ®æ¸…æ´—ä¸­çš„æœªæ¥å‡½æ•° (Look-ahead Bias)ã€‚
    """

    def __init__(self,
                 operators: Dict,
                 feature_names: List[str],
                 combination_model,
                 train_data: pd.DataFrame,
                 val_data: pd.DataFrame,
                 train_target: pd.Series,
                 val_target: pd.Series):
        """
        åˆå§‹åŒ–å› å­è¯„ä¼°å™¨

        Args:
            operators: æ“ä½œç¬¦å­—å…¸
            feature_names: ç‰¹å¾åç§°åˆ—è¡¨
            combination_model: ç»„åˆæ¨¡å‹
            train_data: è®­ç»ƒæ•°æ®
            val_data: éªŒè¯æ•°æ®
            train_target: è®­ç»ƒç›®æ ‡
            val_target: éªŒè¯ç›®æ ‡
        """
        self.operators = operators
        self.feature_names = feature_names
        self.combination_model = combination_model
        self.train_data = train_data
        self.val_data = val_data
        self.train_target = train_target
        self.val_target = val_target

        # ğŸ”¥ æ–°å¢ï¼šç”¨äºç¼“å­˜å½“å‰æ­£åœ¨è¯„ä¼°çš„å› å­çš„è®­ç»ƒé›†ç»Ÿè®¡é‡
        # æ ¼å¼: {'median': float, 'lower': float, 'upper': float}
        self.current_factor_stats = None

        # ğŸ”¥ åˆå§‹åŒ–é«˜çº§å¥–åŠ±è®¡ç®—å™¨
        if ADVANCED_REWARD_AVAILABLE:
            # ä½¿ç”¨ç®€åŒ–é…ç½®ï¼šåªå¯ç”¨æƒ©ç½šé¡¹ï¼Œä¸ä½¿ç”¨å¢é‡Sharpeï¼ˆå› ä¸ºæˆ‘ä»¬å·²ç»åœ¨combinerä¸­è®¡ç®—ï¼‰
            reward_config = RewardConfig(
                use_incremental_sharpe=False,  # ä¸é‡å¤è®¡ç®—å¢é‡
                use_penalty=True,  # å¯ç”¨æƒ©ç½šé¡¹
                use_rolling_stability=False,  # æ•°æ®é‡å°æ—¶å…³é—­
                complexity_lambda=0.3,
                turnover_gamma=2.0,
                max_expr_length=30
            )
            self.reward_calculator = AdvancedRewardCalculator(reward_config)
            logger.info("âœ… AdvancedRewardCalculator enabled (penalty mode)")
        else:
            self.reward_calculator = None

    def evaluate_expression(self, tokens: List[str], trial_only: bool = False) -> Dict:
        """
        è¯„ä¼°è¡¨è¾¾å¼

        Args:
            tokens: tokenåˆ—è¡¨
            trial_only: æ˜¯å¦ä»…è¯•ç®—ä¸æäº¤ï¼ˆTrue=åªè®¡ç®—å¥–åŠ±ï¼ŒFalse=æ ¹æ®é˜ˆå€¼å†³å®šæ˜¯å¦æ·»åŠ ï¼‰

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        if len(tokens) < 3 or tokens[0] != '<BEG>' or tokens[-1] != '<SEP>':
            return {'valid': False, 'reason': 'invalid_format'}

        try:
            expr_tokens = tokens[1:-1]

            # ğŸ”¥ é‡ç½®ç»Ÿè®¡é‡ç¼“å­˜ï¼Œå¼€å§‹æ–°ä¸€è½®è¯„ä¼°
            self.current_factor_stats = None

            # 1. åœ¨è®­ç»ƒé›†ä¸Šè®¡ç®—å› å­å€¼ (Compute & Learn Stats)
            # æ³¨æ„ï¼šå¿…é¡»å…ˆç®—è®­ç»ƒé›†ï¼Œè¿™æ · _clean_series æ‰èƒ½è®¡ç®—å¹¶ä¿å­˜ç»Ÿè®¡é‡
            train_factor = self.compute_factor_values(expr_tokens, self.train_data, is_training=True)

            if train_factor is None:
                return {'valid': False, 'reason': 'train_computation_failed'}

            if self.current_factor_stats is None:
                return {'valid': False, 'reason': 'stats_computation_failed'}

            # 2. åœ¨éªŒè¯é›†ä¸Šè®¡ç®—å› å­å€¼ (Compute & Apply Stats)
            # è¿™é‡Œä¼šä½¿ç”¨ä¸Šä¸€æ­¥ç¼“å­˜çš„ç»Ÿè®¡é‡è¿›è¡Œæ¸…æ´—ï¼Œä¸¥ç¦ä½¿ç”¨éªŒè¯é›†è‡ªå·±çš„ç»Ÿè®¡é‡
            val_factor = self.compute_factor_values(expr_tokens, self.val_data, is_training=False)

            if val_factor is None:
                # å¦‚æœéªŒè¯é›†è®¡ç®—å¤±è´¥ï¼ˆä¾‹å¦‚æ•°æ®å¤ªçŸ­æ— æ³•è®¡ç®—SMAï¼‰ï¼Œè§†ä¸ºæ— æ•ˆ
                return {'valid': False, 'reason': 'val_computation_failed'}

            # ğŸ”¥ æ–°å¢: å¤šæ ·æ€§æ£€æŸ¥ - è®¡ç®—ä¸æ± ä¸­ç°æœ‰å› å­çš„ç›¸ä¼¼åº¦
            diversity_penalty = 0.0
            if len(self.combination_model.alpha_pool) > 0:
                similarity_score = self._calculate_expression_similarity(tokens)
                # ç›¸ä¼¼åº¦è¶Šé«˜,æƒ©ç½šè¶Šå¤§
                if similarity_score > 0.7:
                    # é«˜åº¦ç›¸ä¼¼,é‡åº¦æƒ©ç½š
                    diversity_penalty = -0.5 * similarity_score
                elif similarity_score > 0.5:
                    # ä¸­åº¦ç›¸ä¼¼,ä¸­åº¦æƒ©ç½š
                    diversity_penalty = -0.3 * similarity_score
                elif similarity_score > 0.3:
                    # è½»åº¦ç›¸ä¼¼,è½»åº¦æƒ©ç½š
                    diversity_penalty = -0.1 * similarity_score
                # å¦åˆ™ä¸æƒ©ç½š

            # 3. å…ˆè¯•ç®—ï¼šè®¡ç®—å¢é‡è´¡çŒ®ï¼ˆä¸ä¿®æ”¹æ± å­ï¼‰
            alpha_info = {
                'tokens': tokens,
                'timestamp': time.time(),
                # ä¿å­˜ç»Ÿè®¡é‡ï¼Œä»¥ä¾¿å°†æ¥å®ç›˜ç”Ÿæˆæ—¶å¤ç”¨
                'stats': self.current_factor_stats,
                # ä¿å­˜operatorså¼•ç”¨ï¼Œä¾›å›æµ‹ä½¿ç”¨
                'operators': self.operators
            }

            # ğŸ”¥ Trial Mode: è®¡ç®—å¢é‡Sharpe
            trial_result = self.combination_model.evaluate_new_factor(
                alpha_info, train_factor, val_factor
            )

            incremental_sharpe = trial_result.get('train_incremental_sharpe', 0.0)
            train_stats = trial_result.get('train_stats', {'sharpe': 0.0, 'composite_score': 0.0})
            val_stats = trial_result.get('val_stats', {'sharpe': 0.0, 'composite_score': 0.0})

            # 4. å†³ç­–ï¼šæ˜¯å¦çœŸæ­£æ·»åŠ åˆ°æ± å­
            # ğŸ”¥ è‡ªé€‚åº”é˜ˆå€¼ï¼šæ± å­è¶Šå°ï¼Œé˜ˆå€¼è¶Šä½
            base_threshold = getattr(self.combination_model.config, 'ic_threshold', 0.01)
            current_pool_size = len(self.combination_model.alpha_pool)

            # ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨å¢é‡Sharpeä½œä¸ºå†³ç­–æ ‡å‡†å’ŒPPOå­¦ä¹ ä¿¡å·
            # æ— è®ºæ± å­å¤§å°ï¼Œéƒ½ä½¿ç”¨ç»è¿‡linearä¼˜åŒ–åçš„"å¢é‡Sharpe"æ¥åˆ¤æ–­
            # åŸå› ï¼š
            # 1. å³ä½¿æ˜¯å•å› å­ï¼Œcombinerä¹Ÿä¼šç”¨Ridgeä¼˜åŒ–æƒé‡ï¼Œå¾—åˆ°çš„æ˜¯"ç»„åˆ"Sharpe
            # 2. å¢é‡Sharpe = æ–°ç»„åˆSharpe - æ—§ç»„åˆSharpeï¼Œæ‰æ˜¯çœŸæ­£çš„"è´¡çŒ®"
            # 3. å†³ç­–æ ‡å‡†å’ŒPPOå­¦ä¹ ç›®æ ‡å¿…é¡»ä¸€è‡´ï¼Œå¦åˆ™ç­–ç•¥ä¼šæ··ä¹±

            # æ ¹æ®æ± å­å¤§å°è°ƒæ•´é˜ˆå€¼ï¼ˆè€Œéæ”¹å˜è¯„ä»·æŒ‡æ ‡ï¼‰
            if current_pool_size < 3:
                # å‰3ä¸ªå› å­ï¼šå…è®¸è½»å¾®è´Ÿå€¼ï¼ˆå› ä¸ºæ ·æœ¬å°‘ï¼Œä¸ç¡®å®šæ€§å¤§ï¼‰
                ic_threshold = -0.03  # å…è®¸-3%çš„è´Ÿå¢é‡
            elif current_pool_size < 5:
                # ç¬¬4-5ä¸ªå› å­ï¼šè¦æ±‚å¾ˆå°çš„æ­£å¢é‡
                ic_threshold = 0.001  # 0.1%çš„å¢é‡å³å¯
            elif current_pool_size < 10:
                # ç¬¬6-10ä¸ªå› å­ï¼šè¦æ±‚ä¸­ç­‰å¢é‡
                ic_threshold = base_threshold * 0.3  # 0.3%çš„å¢é‡
            else:
                # 10ä¸ªå› å­åï¼šè¦æ±‚è¾ƒé«˜å¢é‡ï¼ˆæ± å­å·²ç»å¾ˆå¥½äº†ï¼Œæ–°å› å­å¿…é¡»å¸¦æ¥æ˜æ˜¾æ”¹è¿›ï¼‰
                ic_threshold = base_threshold * 0.6  # 0.6%çš„å¢é‡

            # ç»Ÿä¸€ä½¿ç”¨å¢é‡Sharpe
            decision_score = incremental_sharpe
            ppo_reward_signal = incremental_sharpe

            should_add = decision_score > ic_threshold and not trial_only
            actually_added = False

            # ğŸ”¥ è¯Šæ–­æ—¥å¿—ï¼šè®°å½•æ‹’ç»çš„åŸå› ï¼ˆæ˜¾å¼æ‰“å°ï¼‰
            if not trial_only and decision_score <= ic_threshold:
                logger.info(f"âŒ Factor REJECTED:")
                logger.info(f"   incremental_sharpe={decision_score:.6f} <= threshold={ic_threshold:.6f}")
                logger.info(f"   base_threshold={base_threshold:.6f}, pool_size={current_pool_size}")
                logger.info(f"   base_train_score={self.combination_model.base_train_score:.4f}")
                logger.info(f"   new_train_score={trial_result['train_stats']['sharpe']:.4f}")
                logger.info(f"   expression: {' '.join(tokens[:15])}...")

                # ğŸ”¥ é¢å¤–è¯Šæ–­ï¼šåˆ†æä¸ºä»€ä¹ˆè¢«æ‹’ç»
                if decision_score <= 0:
                    logger.info(f"   âš ï¸  Reason: New factor does NOT improve the combination (negative/zero increment)")
                elif self.combination_model.base_train_score > 2.0 and decision_score < 0.01:
                    logger.info(f"   âš ï¸  Reason: Base score is already high, hard to improve further")
                else:
                    logger.info(f"   âš ï¸  Reason: Improvement too small (below threshold)")

            if should_add:
                # ğŸ”¥ Commit Mode: çœŸæ­£æ·»åŠ åˆ°æ± å­
                old_pool_size = len(self.combination_model.alpha_pool)
                old_train_score = self.combination_model.base_train_score

                commit_result = self.combination_model.add_alpha_and_optimize(
                    alpha_info, train_factor, val_factor
                )
                current_pool_size = commit_result.get('pool_size', current_pool_size)
                train_score_after = commit_result.get('current_train_score', 0.0)
                val_score_after = commit_result.get('current_val_score', 0.0)
                actually_added = True

                # ğŸ”¥ è®°å½•æˆåŠŸæ·»åŠ ï¼ˆè¯¦ç»†ä¿¡æ¯ï¼‰
                logger.info(f"âœ… Factor ACCEPTED:")
                logger.info(f"   incremental_sharpe={decision_score:.6f} > threshold={ic_threshold:.6f}")
                logger.info(f"   Pool size: {old_pool_size} â†’ {current_pool_size}")
                logger.info(f"   Train score: {old_train_score:.4f} â†’ {train_score_after:.4f} (Î”={train_score_after - old_train_score:.4f})")
                logger.info(f"   Val score: {commit_result.get('current_val_score', 0.0):.4f}")
                logger.info(f"   Expression: {' '.join(tokens[:20])}...")

                # æ˜¾ç¤ºå½“å‰æ± å­ä¸­çš„å› å­æ•°é‡å’Œæƒé‡åˆ†å¸ƒ
                if self.combination_model.current_weights is not None:
                    weights = self.combination_model.current_weights
                    logger.info(f"   Weight stats: mean={np.mean(np.abs(weights)):.4f}, max={np.max(np.abs(weights)):.4f}, min={np.min(np.abs(weights)):.4f}")
            else:
                # ä¸æ·»åŠ ï¼Œä¿æŒåŸæœ‰åˆ†æ•°
                train_score_after = train_stats.get('sharpe', 0.0)
                val_score_after = val_stats.get('sharpe', 0.0)

            # 5. ğŸ”¥ åº”ç”¨é«˜çº§å¥–åŠ±è®¡ç®—ï¼ˆæƒ©ç½šé¡¹ + å¤šæ ·æ€§æƒ©ç½šï¼‰
            # ä½¿ç”¨ ppo_reward_signal è€Œé incremental_sharpeï¼Œç¡®ä¿PPOå­¦ä¹ åˆ°æ­£ç¡®çš„ä¿¡å·
            final_reward = ppo_reward_signal + diversity_penalty
            penalty_components = {'diversity_penalty': diversity_penalty}

            if self.reward_calculator is not None:
                # å‡†å¤‡old/newè¯„ä¼°æ•°æ®
                old_train_eval = {'sharpe': self.combination_model.base_train_score}
                old_val_eval = {'sharpe': self.combination_model.base_val_score}
                new_train_eval = train_stats
                new_val_eval = val_stats

                # è®¡ç®—æƒ©ç½šé¡¹ï¼ˆä¸åŒ…æ‹¬å¢é‡Sharpeï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»æœ‰äº†ï¼‰
                penalty_result = self.reward_calculator.calculate_reward(
                    new_train_eval=new_train_eval,
                    new_val_eval=new_val_eval,
                    old_train_eval=old_train_eval,
                    old_val_eval=old_val_eval,
                    alpha_info=alpha_info,
                    combination_series=None,  # æš‚ä¸ä½¿ç”¨æ¢æ‰‹ç‡æƒ©ç½š
                    evaluator=None
                )

                # åªå–æƒ©ç½šéƒ¨åˆ†ï¼ˆä¸åŒ…æ‹¬incremental_sharpeï¼‰
                penalty_components_extra = penalty_result.get('components', {})
                complexity_penalty = penalty_components_extra.get('complexity_penalty', 0.0)
                overfitting_penalty = penalty_components_extra.get('overfitting_penalty', 0.0)

                # æ›´æ–°penalty_components
                penalty_components.update(penalty_components_extra)

                # æœ€ç»ˆå¥–åŠ± = PPOå¥–åŠ±ä¿¡å· + å¤šæ ·æ€§æƒ©ç½š + å…¶ä»–æƒ©ç½šé¡¹
                final_reward = ppo_reward_signal + diversity_penalty + complexity_penalty + overfitting_penalty

                # logger.debug(f"Reward breakdown: ppo_signal={ppo_reward_signal:.4f}, "
                #            f"diversity={diversity_penalty:.4f}, complexity={complexity_penalty:.4f}, "
                #            f"overfitting={overfitting_penalty:.4f}, final={final_reward:.4f}")

            # 6. è¿”å›ç»“æœï¼ˆå¥–åŠ±æ˜¯PPO reward signal + æƒ©ç½šï¼‰
            return {
                'valid': True,
                'reward': final_reward,  # ğŸ”¥ PPOå­¦ä¹ ä¿¡å·ï¼ˆçœŸå®çš„å¢é‡Sharpe + æƒ©ç½šï¼‰
                'pool_size': current_pool_size,
                'added_to_pool': actually_added,  # æ˜¯å¦çœŸçš„è¢«æ·»åŠ ï¼ˆtrial_onlyæ—¶ä¸ºFalseï¼‰
                'qualifies': decision_score > ic_threshold,  # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨decision_scoreåˆ¤æ–­
                'incremental_sharpe': incremental_sharpe,  # ä¿æŒåŸå§‹å¢é‡Sharpeä¾›è®°å½•
                'ppo_reward_signal': ppo_reward_signal,  # ğŸ”¥ æ–°å¢ï¼šæ˜¾å¼è¿”å›PPOå­¦ä¹ çš„ä¿¡å·
                'penalty_components': penalty_components,
                'train_factor': train_factor,
                'val_factor': val_factor,
                'alpha_info': alpha_info,
                'train_eval': {
                    'sharpe': train_score_after,
                    'ic': ppo_reward_signal * 0.5,  # ğŸ”¥ ä½¿ç”¨ppo_reward_signal
                    'composite_score': ppo_reward_signal
                },
                'val_eval': {
                    'sharpe': val_score_after,
                    'ic': ppo_reward_signal * 0.5,
                    'composite_score': val_stats.get('composite_score', 0.0)
                },
                'composite_score': final_reward
            }

        except Exception as e:
            # logger.debug(f"Expression evaluation error: {e}")
            # import traceback
            # logger.debug(traceback.format_exc())
            return {'valid': False, 'reason': str(e)}

    def _calculate_expression_similarity(self, tokens: List[str]) -> float:
        """
        è®¡ç®—æ–°è¡¨è¾¾å¼ä¸æ± ä¸­ç°æœ‰è¡¨è¾¾å¼çš„æœ€å¤§ç›¸ä¼¼åº¦

        ç›¸ä¼¼åº¦è®¡ç®—ç­–ç•¥:
        1. Tokenåºåˆ—çš„Jaccardç›¸ä¼¼åº¦
        2. ç»“æ„ç›¸ä¼¼åº¦ (æ“ä½œç¬¦åºåˆ—)
        3. è¿”å›æœ€å¤§ç›¸ä¼¼åº¦åˆ†æ•°

        Args:
            tokens: æ–°è¡¨è¾¾å¼çš„tokenåˆ—è¡¨

        Returns:
            æœ€å¤§ç›¸ä¼¼åº¦åˆ†æ•° [0, 1]
        """
        if len(self.combination_model.alpha_pool) == 0:
            return 0.0

        new_tokens_set = set(tokens[1:-1])  # å»æ‰<BEG>å’Œ<SEP>
        new_operators = [t for t in tokens[1:-1] if t in self.operators]
        new_features = [t for t in tokens[1:-1] if t in self.feature_names]

        max_similarity = 0.0

        for alpha_info in self.combination_model.alpha_pool:
            existing_tokens = alpha_info['tokens']
            existing_tokens_set = set(existing_tokens[1:-1])
            existing_operators = [t for t in existing_tokens[1:-1] if t in self.operators]
            existing_features = [t for t in existing_tokens[1:-1] if t in self.feature_names]

            # 1. Tokené›†åˆçš„Jaccardç›¸ä¼¼åº¦
            if len(new_tokens_set) > 0 and len(existing_tokens_set) > 0:
                intersection = len(new_tokens_set & existing_tokens_set)
                union = len(new_tokens_set | existing_tokens_set)
                token_similarity = intersection / union if union > 0 else 0.0
            else:
                token_similarity = 0.0

            # 2. æ“ä½œç¬¦åºåˆ—ç›¸ä¼¼åº¦
            if len(new_operators) > 0 and len(existing_operators) > 0:
                common_ops = len(set(new_operators) & set(existing_operators))
                total_ops = max(len(new_operators), len(existing_operators))
                operator_similarity = common_ops / total_ops if total_ops > 0 else 0.0
            else:
                operator_similarity = 0.0

            # 3. ç‰¹å¾åºåˆ—ç›¸ä¼¼åº¦
            if len(new_features) > 0 and len(existing_features) > 0:
                common_features = len(set(new_features) & set(existing_features))
                total_features = max(len(new_features), len(existing_features))
                feature_similarity = common_features / total_features if total_features > 0 else 0.0
            else:
                feature_similarity = 0.0

            # ç»¼åˆç›¸ä¼¼åº¦ (åŠ æƒå¹³å‡)
            overall_similarity = (
                0.4 * token_similarity +
                0.4 * operator_similarity +
                0.2 * feature_similarity
            )

            max_similarity = max(max_similarity, overall_similarity)

        return max_similarity

    def compute_factor_values(self, expr_tokens: List[str], data: pd.DataFrame, is_training: bool = False) -> Optional[pd.Series]:
        """
        è®¡ç®—å› å­å€¼

        Args:
            expr_tokens: è¡¨è¾¾å¼tokenåˆ—è¡¨
            data: æ•°æ®DataFrame
            is_training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆå†³å®šæ˜¯è®¡ç®—ç»Ÿè®¡é‡è¿˜æ˜¯åº”ç”¨ç»Ÿè®¡é‡ï¼‰

        Returns:
            å› å­å€¼Seriesï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        try:
            stack = []

            for token in expr_tokens:
                if token in self.feature_names:
                    if token in data.columns:
                        stack.append(data[token].copy())
                    else:
                        return None

                elif token in self.operators:
                    op_info = self.operators[token]
                    if len(stack) < op_info['arity']:
                        return None

                    args = []
                    for _ in range(op_info['arity']):
                        args.append(stack.pop())
                    args.reverse()

                    # æ‰§è¡Œç®—å­è®¡ç®—
                    try:
                        result = op_info['func'](*args)
                    except Exception:
                        return None # ç®—å­æ‰§è¡Œå¤±è´¥ï¼ˆå¦‚é™¤é›¶ï¼‰

                    # ğŸ”¥ ä¸­é—´ç»“æœæ¸…æ´—ï¼šæ›´ä¿å®ˆçš„ç­–ç•¥ï¼Œé¿å…è¿‡åº¦å¡«å……ä¼ æ’­é”™è¯¯
                    result = result.replace([np.inf, -np.inf], np.nan)

                    # ğŸ”¥ ä¿®å¤ï¼šæ”¾å®½NaNå®¹å¿åº¦ 0.5 â†’ 0.7
                    # åŸå› ï¼štrain_computation_failed 11/16ï¼ŒNaNæ£€æŸ¥è¿‡äºä¸¥æ ¼å¯¼è‡´è®¡ç®—å¤±è´¥
                    # æ£€æŸ¥NaNæ¯”ä¾‹ï¼Œå¦‚æœè¿‡é«˜åˆ™è®¤ä¸ºè®¡ç®—å¤±è´¥
                    if len(result) > 0:
                        nan_ratio = result.isna().sum() / len(result)
                        if nan_ratio > 0.7:  # ä»0.5æé«˜åˆ°0.7
                            # NaNæ¯”ä¾‹è¶…è¿‡70%ï¼Œä¸­é—´æ­¥éª¤å¤±è´¥
                            return None

                    # åªåœ¨NaNæ¯”ä¾‹ä¸é«˜æ—¶æ‰å¡«å……
                    if result.isna().any():
                        result = result.ffill().fillna(0)

                    stack.append(result)

                else:
                    return None

            if len(stack) != 1:
                return None

            final_result = stack[0]
            
            # ğŸ”¥ æœ€ç»ˆç»“æœæ¸…æ´—ï¼ˆåŒ…å«å»æå€¼å’Œæ ‡å‡†åŒ–ï¼‰
            # è¿™é‡Œä¼ å…¥ is_training æ ‡å¿—ï¼Œå†³å®šæ˜¯ "Learn" è¿˜æ˜¯ "Apply" ç»Ÿè®¡é‡
            final_result = self._clean_series(final_result, is_training=is_training)

            if final_result is None:
                return None

            # æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿æ²¡æœ‰ NaNï¼ˆ_clean_series åº”è¯¥å·²ç»å¤„ç†äº†ï¼‰
            if final_result.isnull().any():
                final_result = final_result.fillna(0)

            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å˜åŒ– (é¿å…å¸¸æ•°å› å­)
            if final_result.std() < 1e-6:
                return None

            return final_result

        except Exception as e:
            # logger.debug(f"Factor computation error: {e}")
            return None

    def _clean_series(self, series: pd.Series, is_training: bool) -> Optional[pd.Series]:
        """
        æ¸…ç†åºåˆ—ï¼šå»æå€¼ + æ ‡å‡†åŒ– (Z-Score)
        """
        if series is None:
            return None

        # 1. åŸºç¡€æ¸…æ´—
        series = series.replace([np.inf, -np.inf], np.nan)
        # ğŸ”¥ ä¿®å¤ï¼šæ”¾å®½NaNå®¹å¿åº¦ 0.5 â†’ 0.7
        # åŸå› ï¼štrain_computation_failed 11/16ï¼ŒNaNæ£€æŸ¥è¿‡äºä¸¥æ ¼
        # æ£€æŸ¥ NaN æ¯”ä¾‹
        if series.isna().sum() / len(series) > 0.7:  # ä»0.5æé«˜åˆ°0.7
            return None
        series = series.ffill()

        # 2. è®¡ç®—/åº”ç”¨ç»Ÿè®¡é‡
        if is_training:
            try:
                # è®¡ç®—ç»Ÿè®¡é‡
                median = series.median()
                # è¿™é‡Œçš„ quantile èŒƒå›´å¯ä»¥é€‚å½“æ”¾å®½ï¼Œæ¯”å¦‚ 0.005 å’Œ 0.995
                lower = series.quantile(0.01)
                upper = series.quantile(0.99)
                
                # å…ˆå»æå€¼ï¼Œå†ç®—å‡å€¼æ–¹å·®ï¼Œè¿™æ ·æ›´ç¨³å¥
                clipped = series.clip(lower, upper)
                mean = clipped.mean()
                std = clipped.std()
                
                # ç¼“å­˜ç»Ÿè®¡é‡
                self.current_factor_stats = {
                    'median': float(median) if not pd.isna(median) else 0.0,
                    'lower': float(lower) if not pd.isna(lower) else -3.0,
                    'upper': float(upper) if not pd.isna(upper) else 3.0,
                    'mean': float(mean) if not pd.isna(mean) else 0.0,
                    'std': float(std) if not pd.isna(std) else 1.0,
                }
            except:
                return None
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç»Ÿè®¡é‡å¯ç”¨
        if self.current_factor_stats is None:
            if not is_training:
                # éªŒè¯é›†æ²¡æœ‰ç»Ÿè®¡é‡æ—¶çš„å›é€€ç­–ç•¥
                return self._clean_series(series, is_training=True)
            return None

        stats = self.current_factor_stats

        # 3. æ‰§è¡Œæ¸…æ´—æ“ä½œ
        # A. å»æå€¼ (Winsorization)
        series = series.clip(stats['lower'], stats['upper'])
        
        # B. å¡«å……ç¼ºå¤±å€¼ (ä½¿ç”¨ä¸­ä½æ•°)
        series = series.fillna(stats['median'])
        
        # C. ğŸ”¥ æ ‡å‡†åŒ– (Z-Score) - è¿™æ˜¯ä½ ä¹‹å‰ç¼ºå°‘çš„å…³é”®ä¸€æ­¥ï¼
        if stats['std'] > 1e-8:
            series = (series - stats['mean']) / stats['std']
        else:
            series = series - stats['mean']
            
        return series