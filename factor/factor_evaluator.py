"""
å› å­è¯„ä¼°å™¨æ¨¡å—
è´Ÿè´£è®¡ç®—å› å­å€¼å’Œè¯„ä¼°è¡¨è¾¾å¼
"""

import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Optional
import time
import sys
from pathlib import Path

# æ·»åŠ utilsç›®å½•åˆ°è·¯å¾„
utils_path = Path(__file__).parent.parent / 'utils'
if str(utils_path) not in sys.path:
    sys.path.insert(0, str(utils_path))

try:
    from advanced_reward import AdvancedRewardCalculator, RewardConfig
    ADVANCED_REWARD_AVAILABLE = True
except ImportError:
    ADVANCED_REWARD_AVAILABLE = False
    logging.warning("AdvancedRewardCalculator not available, using simple reward")

logger = logging.getLogger(__name__)


class FactorEvaluator:
    """
    å› å­è¯„ä¼°å™¨ - è´Ÿè´£è®¡ç®—å› å­å€¼å’Œè¯„ä¼°è¡¨è¾¾å¼
    
    ä¿®æ”¹è¯´æ˜ (å®ç›˜é€‚é…):
    1. å¼•å…¥äº† Train/Val çŠ¶æ€åŒºåˆ†ï¼Œç¡®ä¿æ¸…æ´—éªŒè¯é›†æ•°æ®æ—¶ä½¿ç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡é‡ã€‚
    2. ä¿®å¤äº†æ•°æ®æ¸…æ´—ä¸­çš„æœªæ¥å‡½æ•° (Look-ahead Bias)ã€‚
    """

    def __init__(self,
                 operators: Dict,
                 feature_names: List[str],
                 combination_model,
                 train_data: pd.DataFrame,
                 val_data: pd.DataFrame,
                 train_target: pd.Series,
                 val_target: pd.Series):
        """
        åˆå§‹åŒ–å› å­è¯„ä¼°å™¨

        Args:
            operators: æ“ä½œç¬¦å­—å…¸
            feature_names: ç‰¹å¾åç§°åˆ—è¡¨
            combination_model: ç»„åˆæ¨¡å‹
            train_data: è®­ç»ƒæ•°æ®
            val_data: éªŒè¯æ•°æ®
            train_target: è®­ç»ƒç›®æ ‡
            val_target: éªŒè¯ç›®æ ‡
        """
        self.operators = operators
        self.feature_names = feature_names
        self.combination_model = combination_model
        self.train_data = train_data
        self.val_data = val_data
        self.train_target = train_target
        self.val_target = val_target

        # ğŸ”¥ æ–°å¢ï¼šç”¨äºç¼“å­˜å½“å‰æ­£åœ¨è¯„ä¼°çš„å› å­çš„è®­ç»ƒé›†ç»Ÿè®¡é‡
        # æ ¼å¼: {'median': float, 'lower': float, 'upper': float}
        self.current_factor_stats = None

        # ğŸ”¥ åˆå§‹åŒ–é«˜çº§å¥–åŠ±è®¡ç®—å™¨
        if ADVANCED_REWARD_AVAILABLE:
            # ä½¿ç”¨ç®€åŒ–é…ç½®ï¼šåªå¯ç”¨æƒ©ç½šé¡¹ï¼Œä¸ä½¿ç”¨å¢é‡Sharpeï¼ˆå› ä¸ºæˆ‘ä»¬å·²ç»åœ¨combinerä¸­è®¡ç®—ï¼‰
            reward_config = RewardConfig(
                use_incremental_sharpe=False,  # ä¸é‡å¤è®¡ç®—å¢é‡
                use_penalty=True,  # å¯ç”¨æƒ©ç½šé¡¹
                use_rolling_stability=False,  # æ•°æ®é‡å°æ—¶å…³é—­
                complexity_lambda=0.3,
                turnover_gamma=2.0,
                max_expr_length=30
            )
            self.reward_calculator = AdvancedRewardCalculator(reward_config)
            logger.info("âœ… AdvancedRewardCalculator enabled (penalty mode)")
        else:
            self.reward_calculator = None

    def evaluate_expression(self, tokens: List[str], trial_only: bool = False) -> Dict:
        """
        è¯„ä¼°è¡¨è¾¾å¼

        Args:
            tokens: tokenåˆ—è¡¨
            trial_only: æ˜¯å¦ä»…è¯•ç®—ä¸æäº¤ï¼ˆTrue=åªè®¡ç®—å¥–åŠ±ï¼ŒFalse=æ ¹æ®é˜ˆå€¼å†³å®šæ˜¯å¦æ·»åŠ ï¼‰

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        if len(tokens) < 3 or tokens[0] != '<BEG>' or tokens[-1] != '<SEP>':
            return {'valid': False, 'reason': 'invalid_format'}

        try:
            expr_tokens = tokens[1:-1]
            
            # ğŸ”¥ é‡ç½®ç»Ÿè®¡é‡ç¼“å­˜ï¼Œå¼€å§‹æ–°ä¸€è½®è¯„ä¼°
            self.current_factor_stats = None

            # 1. åœ¨è®­ç»ƒé›†ä¸Šè®¡ç®—å› å­å€¼ (Compute & Learn Stats)
            # æ³¨æ„ï¼šå¿…é¡»å…ˆç®—è®­ç»ƒé›†ï¼Œè¿™æ · _clean_series æ‰èƒ½è®¡ç®—å¹¶ä¿å­˜ç»Ÿè®¡é‡
            train_factor = self.compute_factor_values(expr_tokens, self.train_data, is_training=True)
            
            if train_factor is None:
                return {'valid': False, 'reason': 'train_computation_failed'}
                
            if self.current_factor_stats is None:
                return {'valid': False, 'reason': 'stats_computation_failed'}

            # 2. åœ¨éªŒè¯é›†ä¸Šè®¡ç®—å› å­å€¼ (Compute & Apply Stats)
            # è¿™é‡Œä¼šä½¿ç”¨ä¸Šä¸€æ­¥ç¼“å­˜çš„ç»Ÿè®¡é‡è¿›è¡Œæ¸…æ´—ï¼Œä¸¥ç¦ä½¿ç”¨éªŒè¯é›†è‡ªå·±çš„ç»Ÿè®¡é‡
            val_factor = self.compute_factor_values(expr_tokens, self.val_data, is_training=False)
            
            if val_factor is None:
                # å¦‚æœéªŒè¯é›†è®¡ç®—å¤±è´¥ï¼ˆä¾‹å¦‚æ•°æ®å¤ªçŸ­æ— æ³•è®¡ç®—SMAï¼‰ï¼Œè§†ä¸ºæ— æ•ˆ
                return {'valid': False, 'reason': 'val_computation_failed'}

            # 3. å…ˆè¯•ç®—ï¼šè®¡ç®—å¢é‡è´¡çŒ®ï¼ˆä¸ä¿®æ”¹æ± å­ï¼‰
            alpha_info = {
                'tokens': tokens,
                'timestamp': time.time(),
                # ä¿å­˜ç»Ÿè®¡é‡ï¼Œä»¥ä¾¿å°†æ¥å®ç›˜ç”Ÿæˆæ—¶å¤ç”¨
                'stats': self.current_factor_stats,
                # ä¿å­˜operatorså¼•ç”¨ï¼Œä¾›å›æµ‹ä½¿ç”¨
                'operators': self.operators
            }

            # ğŸ”¥ Trial Mode: è®¡ç®—å¢é‡Sharpe
            trial_result = self.combination_model.evaluate_new_factor(
                alpha_info, train_factor, val_factor
            )

            incremental_sharpe = trial_result.get('train_incremental_sharpe', 0.0)
            train_stats = trial_result.get('train_stats', {'sharpe': 0.0, 'composite_score': 0.0})
            val_stats = trial_result.get('val_stats', {'sharpe': 0.0, 'composite_score': 0.0})

            # 4. å†³ç­–ï¼šæ˜¯å¦çœŸæ­£æ·»åŠ åˆ°æ± å­
            # ğŸ”¥ è‡ªé€‚åº”é˜ˆå€¼ï¼šæ± å­è¶Šå°ï¼Œé˜ˆå€¼è¶Šä½
            base_threshold = getattr(self.combination_model.config, 'ic_threshold', 0.01)
            current_pool_size = len(self.combination_model.alpha_pool)

            # ğŸ”¥ ä¿®å¤ï¼šå‰æœŸä½¿ç”¨0é˜ˆå€¼ï¼Œå…è®¸æ‰€æœ‰æœ‰æ•ˆå› å­è¿›å…¥
            if current_pool_size < 3:
                ic_threshold = 0.0  # å‰3ä¸ªå› å­ï¼šåªè¦å¢é‡>0å°±æ¥å—
            elif current_pool_size < 5:
                ic_threshold = base_threshold * 0.3  # ç¬¬4-5ä¸ªå› å­ç”¨0.3å€é˜ˆå€¼
            elif current_pool_size < 10:
                ic_threshold = base_threshold * 0.6  # ç¬¬6-10ä¸ªå› å­ç”¨0.6å€é˜ˆå€¼
            else:
                ic_threshold = base_threshold  # ä¹‹åç”¨æ­£å¸¸é˜ˆå€¼

            should_add = incremental_sharpe > ic_threshold and not trial_only
            actually_added = False

            # ğŸ”¥ è¯Šæ–­æ—¥å¿—ï¼šè®°å½•æ‹’ç»çš„åŸå› ï¼ˆæ˜¾å¼æ‰“å°ï¼‰
            if not trial_only and incremental_sharpe <= ic_threshold:
                logger.info(f"âŒ Factor REJECTED:")
                logger.info(f"   incremental_sharpe={incremental_sharpe:.6f} <= threshold={ic_threshold:.6f}")
                logger.info(f"   base_threshold={base_threshold:.6f}, pool_size={current_pool_size}")
                logger.info(f"   base_train_score={self.combination_model.base_train_score:.4f}")
                logger.info(f"   new_train_score={trial_result['train_stats']['sharpe']:.4f}")
                logger.info(f"   expression: {' '.join(tokens[:15])}...")

                # ğŸ”¥ é¢å¤–è¯Šæ–­ï¼šåˆ†æä¸ºä»€ä¹ˆå¢é‡ä½
                if incremental_sharpe <= 0:
                    logger.info(f"   âš ï¸  Reason: New factor does NOT improve the combination (negative/zero increment)")
                elif self.combination_model.base_train_score > 2.0 and incremental_sharpe < 0.01:
                    logger.info(f"   âš ï¸  Reason: Base score is already high, hard to improve further")
                else:
                    logger.info(f"   âš ï¸  Reason: Improvement too small (below threshold)")

            if should_add:
                # ğŸ”¥ Commit Mode: çœŸæ­£æ·»åŠ åˆ°æ± å­
                old_pool_size = len(self.combination_model.alpha_pool)
                old_train_score = self.combination_model.base_train_score

                commit_result = self.combination_model.add_alpha_and_optimize(
                    alpha_info, train_factor, val_factor
                )
                current_pool_size = commit_result.get('pool_size', current_pool_size)
                train_score_after = commit_result.get('current_train_score', 0.0)
                val_score_after = commit_result.get('current_val_score', 0.0)
                actually_added = True

                # ğŸ”¥ è®°å½•æˆåŠŸæ·»åŠ ï¼ˆè¯¦ç»†ä¿¡æ¯ï¼‰
                logger.info(f"âœ… Factor ACCEPTED:")
                logger.info(f"   incremental_sharpe={incremental_sharpe:.6f} > threshold={ic_threshold:.6f}")
                logger.info(f"   Pool size: {old_pool_size} â†’ {current_pool_size}")
                logger.info(f"   Train score: {old_train_score:.4f} â†’ {train_score_after:.4f} (Î”={train_score_after - old_train_score:.4f})")
                logger.info(f"   Val score: {commit_result.get('current_val_score', 0.0):.4f}")
                logger.info(f"   Expression: {' '.join(tokens[:20])}...")

                # æ˜¾ç¤ºå½“å‰æ± å­ä¸­çš„å› å­æ•°é‡å’Œæƒé‡åˆ†å¸ƒ
                if self.combination_model.current_weights is not None:
                    weights = self.combination_model.current_weights
                    logger.info(f"   Weight stats: mean={np.mean(np.abs(weights)):.4f}, max={np.max(np.abs(weights)):.4f}, min={np.min(np.abs(weights)):.4f}")
            else:
                # ä¸æ·»åŠ ï¼Œä¿æŒåŸæœ‰åˆ†æ•°
                train_score_after = train_stats.get('sharpe', 0.0)
                val_score_after = val_stats.get('sharpe', 0.0)

            # 5. ğŸ”¥ åº”ç”¨é«˜çº§å¥–åŠ±è®¡ç®—ï¼ˆæƒ©ç½šé¡¹ï¼‰
            final_reward = incremental_sharpe
            penalty_components = {}

            if self.reward_calculator is not None:
                # å‡†å¤‡old/newè¯„ä¼°æ•°æ®
                old_train_eval = {'sharpe': self.combination_model.base_train_score}
                old_val_eval = {'sharpe': self.combination_model.base_val_score}
                new_train_eval = train_stats
                new_val_eval = val_stats

                # è®¡ç®—æƒ©ç½šé¡¹ï¼ˆä¸åŒ…æ‹¬å¢é‡Sharpeï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»æœ‰äº†ï¼‰
                penalty_result = self.reward_calculator.calculate_reward(
                    new_train_eval=new_train_eval,
                    new_val_eval=new_val_eval,
                    old_train_eval=old_train_eval,
                    old_val_eval=old_val_eval,
                    alpha_info=alpha_info,
                    combination_series=None,  # æš‚ä¸ä½¿ç”¨æ¢æ‰‹ç‡æƒ©ç½š
                    evaluator=None
                )

                # åªå–æƒ©ç½šéƒ¨åˆ†ï¼ˆä¸åŒ…æ‹¬incremental_sharpeï¼‰
                penalty_components = penalty_result.get('components', {})
                complexity_penalty = penalty_components.get('complexity_penalty', 0.0)
                overfitting_penalty = penalty_components.get('overfitting_penalty', 0.0)

                # æœ€ç»ˆå¥–åŠ± = å¢é‡Sharpe + æƒ©ç½šé¡¹
                final_reward = incremental_sharpe + complexity_penalty + overfitting_penalty

                # logger.debug(f"Reward breakdown: incremental={incremental_sharpe:.4f}, "
                #            f"complexity={complexity_penalty:.4f}, overfitting={overfitting_penalty:.4f}, "
                #            f"final={final_reward:.4f}")

            # 6. è¿”å›ç»“æœï¼ˆå¥–åŠ±æ˜¯å¢é‡Sharpe + æƒ©ç½šï¼‰
            return {
                'valid': True,
                'reward': final_reward,  # ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šå¢é‡ + æƒ©ç½š
                'pool_size': current_pool_size,
                'added_to_pool': actually_added,  # æ˜¯å¦çœŸçš„è¢«æ·»åŠ ï¼ˆtrial_onlyæ—¶ä¸ºFalseï¼‰
                'qualifies': incremental_sharpe > ic_threshold,  # æ˜¯å¦è¾¾æ ‡
                'incremental_sharpe': incremental_sharpe,
                'penalty_components': penalty_components,
                'train_factor': train_factor,  # ğŸ”¥ æ–°å¢ï¼šè¿”å›å› å­æ•°æ®ä¾›åç»­æäº¤
                'val_factor': val_factor,
                'alpha_info': alpha_info,
                'train_eval': {
                    'sharpe': train_score_after,
                    'ic': incremental_sharpe * 0.5,  # ICå’Œå¢é‡Sharpeç›¸å…³
                    'composite_score': incremental_sharpe
                },
                'val_eval': {
                    'sharpe': val_score_after,
                    'ic': incremental_sharpe * 0.5,
                    'composite_score': val_stats.get('composite_score', 0.0)
                },
                'composite_score': final_reward  # ğŸ”¥ è¿™é‡Œä¹Ÿæ”¹ä¸ºæœ€ç»ˆå¥–åŠ±
            }

        except Exception as e:
            # logger.debug(f"Expression evaluation error: {e}")
            # import traceback
            # logger.debug(traceback.format_exc())
            return {'valid': False, 'reason': str(e)}

    def compute_factor_values(self, expr_tokens: List[str], data: pd.DataFrame, is_training: bool = False) -> Optional[pd.Series]:
        """
        è®¡ç®—å› å­å€¼

        Args:
            expr_tokens: è¡¨è¾¾å¼tokenåˆ—è¡¨
            data: æ•°æ®DataFrame
            is_training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆå†³å®šæ˜¯è®¡ç®—ç»Ÿè®¡é‡è¿˜æ˜¯åº”ç”¨ç»Ÿè®¡é‡ï¼‰

        Returns:
            å› å­å€¼Seriesï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        try:
            stack = []

            for token in expr_tokens:
                if token in self.feature_names:
                    if token in data.columns:
                        stack.append(data[token].copy())
                    else:
                        return None

                elif token in self.operators:
                    op_info = self.operators[token]
                    if len(stack) < op_info['arity']:
                        return None

                    args = []
                    for _ in range(op_info['arity']):
                        args.append(stack.pop())
                    args.reverse()

                    # æ‰§è¡Œç®—å­è®¡ç®—
                    try:
                        result = op_info['func'](*args)
                    except Exception:
                        return None # ç®—å­æ‰§è¡Œå¤±è´¥ï¼ˆå¦‚é™¤é›¶ï¼‰

                    # ğŸ”¥ ä¸­é—´ç»“æœæ¸…æ´—ï¼š
                    # ä¸ºäº†ä¿æŒè®¡ç®—é“¾çš„ç¨³å®šæ€§ï¼Œä¸­é—´æ­¥éª¤ä¹Ÿè¿›è¡Œè½»é‡çº§æ¸…æ´—
                    # ä½†å®Œå…¨çš„åˆ†å¸ƒå¯¹é½åªåœ¨æœ€åä¸€æ­¥è¿›è¡Œ
                    result = result.replace([np.inf, -np.inf], np.nan)
                    
                    # ç®€å•çš„ fillna é˜²æ­¢ NaN ä¼ æŸ“ï¼Œè¿™é‡Œç”¨ ffill ä¿æŒå› æœæ€§
                    result = result.ffill().fillna(0)
                    
                    stack.append(result)

                else:
                    return None

            if len(stack) != 1:
                return None

            final_result = stack[0]
            
            # ğŸ”¥ æœ€ç»ˆç»“æœæ¸…æ´—ï¼ˆåŒ…å«å»æå€¼å’Œæ ‡å‡†åŒ–ï¼‰
            # è¿™é‡Œä¼ å…¥ is_training æ ‡å¿—ï¼Œå†³å®šæ˜¯ "Learn" è¿˜æ˜¯ "Apply" ç»Ÿè®¡é‡
            final_result = self._clean_series(final_result, is_training=is_training)

            if final_result is None:
                return None

            # æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿æ²¡æœ‰ NaNï¼ˆ_clean_series åº”è¯¥å·²ç»å¤„ç†äº†ï¼‰
            if final_result.isnull().any():
                final_result = final_result.fillna(0)

            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å˜åŒ– (é¿å…å¸¸æ•°å› å­)
            if final_result.std() < 1e-6:
                return None

            return final_result

        except Exception as e:
            # logger.debug(f"Factor computation error: {e}")
            return None

    def _clean_series(self, series: pd.Series, is_training: bool) -> Optional[pd.Series]:
        """
        æ¸…ç†åºåˆ— - ä¸¥æ ¼é˜²æ­¢æœªæ¥å‡½æ•° (Strict No-Lookahead)
        
        Args:
            series: è¾“å…¥åºåˆ—
            is_training: True=è®¡ç®—å¹¶ä¿å­˜ç»Ÿè®¡é‡; False=ä½¿ç”¨å·²ä¿å­˜çš„ç»Ÿè®¡é‡
        """
        if series is None:
            return None

        # 1. æ›¿æ¢æ— ç©·å€¼
        series = series.replace([np.inf, -np.inf], np.nan)

        # 2. æ£€æŸ¥ NaN æ¯”ä¾‹ (å¦‚æœå¤ªå¤šç¼ºå¤±ï¼Œç›´æ¥ä¸¢å¼ƒ)
        # æ³¨æ„ï¼šåœ¨ Valid é›†ä¸­ï¼Œå¦‚æœç”±äº Lookback Buffer ä¸è¶³å¯¼è‡´å¼€å¤´æœ‰ NaNï¼Œ
        # è¿™é‡Œçš„é˜ˆå€¼éœ€è¦å®½å®¹ä¸€äº›ï¼Œæˆ–è€…åœ¨å¤–éƒ¨ä¿è¯ Buffer è¶³å¤Ÿã€‚
        nan_ratio = series.isna().sum() / len(series)
        if nan_ratio > 0.5:
            return None

        # 3. å› æœå¡«å…… (Causal Imputation)
        # ä¼˜å…ˆä½¿ç”¨å‰å‘å¡«å…… (ffill)ï¼Œè¿™æ„å‘³ç€ç”¨â€œæ˜¨å¤©â€çš„å€¼å¡«è¡¥â€œä»Šå¤©â€çš„ç©ºç¼º
        # ä¸¥ç¦ä½¿ç”¨ series.median() ç›´æ¥å¡«å……ï¼Œå› ä¸ºé‚£æ˜¯æœªæ¥çš„ç»Ÿè®¡é‡
        series = series.ffill()

        # 4. å»æå€¼å’Œå‰©ä½™ç¼ºå¤±å€¼å¡«å…… (Clip & Fill)
        if is_training:
            # === è®­ç»ƒæ¨¡å¼ï¼šå­¦ä¹ ç»Ÿè®¡é‡ ===
            try:
                median_val = series.median()
                # ä½¿ç”¨ 1% å’Œ 99% åˆ†ä½æ•°ç¡®å®šè¾¹ç•Œ
                q01 = series.quantile(0.01)
                q99 = series.quantile(0.99)
                
                # ç¼“å­˜ç»Ÿè®¡é‡
                self.current_factor_stats = {
                    'median': float(median_val) if not pd.isna(median_val) else 0.0,
                    'lower': float(q01) if not pd.isna(q01) else -10.0,
                    'upper': float(q99) if not pd.isna(q99) else 10.0
                }
            except Exception:
                return None # ç»Ÿè®¡é‡è®¡ç®—å¤±è´¥
                
            # åº”ç”¨æˆªæ–­
            series = series.clip(self.current_factor_stats['lower'], self.current_factor_stats['upper'])
            # å¡«å……å‰©ä½™çš„ NaN (é€šå¸¸æ˜¯åºåˆ—å¼€å¤´çš„)
            series = series.fillna(self.current_factor_stats['median'])

        else:
            # === éªŒè¯/å®ç›˜æ¨¡å¼ï¼šåº”ç”¨ç»Ÿè®¡é‡ ===
            if self.current_factor_stats is None:
                # è¿™æ˜¯ä¸€ä¸ªå¼‚å¸¸æƒ…å†µï¼šè¯•å›¾åœ¨æ²¡æœ‰è®­ç»ƒç»Ÿè®¡é‡çš„æƒ…å†µä¸‹è¯„ä¼°éªŒè¯é›†
                # å›é€€ç­–ç•¥ï¼šè¢«è¿«ä½¿ç”¨å½“å‰æ•°æ®çš„ç»Ÿè®¡é‡ï¼ˆä¼šæœ‰è½»å¾®æœªæ¥å‡½æ•°ï¼Œä½†æ€»æ¯”å´©æºƒå¥½ï¼‰
                # æ›´å¥½çš„åšæ³•æ˜¯è¿”å› None æˆ–æŠ¥é”™
                logger.warning("Evaluating validation data without training stats! Fallback to local stats.")
                return self._clean_series(series, is_training=True)
            
            # ä¸¥æ ¼ä½¿ç”¨è®­ç»ƒé›†çš„è¾¹ç•Œè¿›è¡Œæˆªæ–­
            series = series.clip(self.current_factor_stats['lower'], self.current_factor_stats['upper'])
            
            # ä¸¥æ ¼ä½¿ç”¨è®­ç»ƒé›†çš„ä¸­ä½æ•°å¡«å……å‰©ä½™ NaN
            series = series.fillna(self.current_factor_stats['median'])

        return series