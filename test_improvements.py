"""
æµ‹è¯•è¯­æ³•çº¦æŸå’Œå¤šæ ·æ€§æ”¹è¿›
"""
import sys
sys.path.insert(0, '/Users/duanjin/Desktop/å¼ºåŒ–å­¦ä¹ /PPO')

import numpy as np
import pandas as pd

def test_grammar_constraints():
    """æµ‹è¯•RPNæ ˆå¹³è¡¡çº¦æŸ - ç®€åŒ–ç‰ˆ"""
    print("=" * 60)
    print("æµ‹è¯• 1: è¯­æ³•çº¦æŸé€»è¾‘éªŒè¯")
    print("=" * 60)

    # æ¨¡æ‹ŸRPNæ ˆæ£€æŸ¥
    operators_arity = {
        'add': 2, 'sub': 2, 'mul': 2, 'div': 2,
        'sma10': 1, 'ema10': 1, 'corr20': 2, 'decay10': 1
    }
    feature_names = ['open', 'high', 'low', 'close', 'volume']

    def check_rpn_validity(tokens):
        """æ£€æŸ¥RPNè¡¨è¾¾å¼æ˜¯å¦æœ‰æ•ˆ"""
        if len(tokens) < 3 or tokens[0] != '<BEG>' or tokens[-1] != '<SEP>':
            return False, "invalid_format"

        stack = 0
        for token in tokens[1:-1]:
            if token in feature_names:
                stack += 1
            elif token in operators_arity:
                arity = operators_arity[token]
                if stack < arity:
                    return False, "insufficient_operands"
                stack = stack - arity + 1
            else:
                return False, "unknown_token"

        if stack != 1:
            return False, f"stack_imbalance (stack={stack})"

        return True, "valid"

    # æµ‹è¯•æ¡ˆä¾‹
    test_cases = [
        # (expression, expected_valid)
        (['<BEG>', 'high', 'sma10', '<SEP>'], True),  # ç®€å•æœ‰æ•ˆ
        (['<BEG>', 'high', 'low', 'sub', 'ema10', '<SEP>'], True),  # å¤åˆæœ‰æ•ˆ
        (['<BEG>', 'high', 'high', 'corr20', 'decay10', '<SEP>'], True),  # å®é™…æ¡ˆä¾‹
        (['<BEG>', 'high', '<SEP>'], True),  # å•ä¸ªfeatureä¹Ÿæ˜¯æœ‰æ•ˆçš„(æ ˆ=1)
        (['<BEG>', 'high', 'low', 'sub', '<SEP>'], True),  # æœ‰æ•ˆçš„äºŒå…ƒæ“ä½œ
        (['<BEG>', 'sma10', '<SEP>'], False),  # ç¼ºå°‘æ“ä½œæ•°
        (['<BEG>', 'high', 'low', '<SEP>'], False),  # æ ˆä¸å¹³è¡¡(æ ˆ=2)
    ]

    print("\næµ‹è¯•RPNæ ˆå¹³è¡¡æ£€æŸ¥:")
    passed = 0
    for i, (tokens, expected_valid) in enumerate(test_cases, 1):
        is_valid, reason = check_rpn_validity(tokens)
        status = "âœ…" if is_valid == expected_valid else "âŒ"
        print(f"{status} æ¡ˆä¾‹ {i}: {' '.join(tokens[1:-1])}")
        print(f"   ç»“æœ: {reason}, æœŸæœ›: {'valid' if expected_valid else 'invalid'}")

        if is_valid == expected_valid:
            passed += 1

    print(f"\né€šè¿‡: {passed}/{len(test_cases)}")
    assert passed == len(test_cases), "RPNæ ˆæ£€æŸ¥é€»è¾‘æœ‰è¯¯"
    print("âœ… æµ‹è¯•é€šè¿‡: RPNæ ˆå¹³è¡¡é€»è¾‘æ­£ç¡®!")


def test_diversity_similarity():
    """æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: ç›¸ä¼¼åº¦è®¡ç®—")
    print("=" * 60)

    feature_names = ['open', 'high', 'low', 'close', 'volume']
    operators = {'add', 'sub', 'mul', 'div', 'sma10', 'ema10', 'corr20', 'decay10'}

    def calculate_similarity(tokens1, tokens2):
        tokens1_set = set(tokens1[1:-1])
        tokens2_set = set(tokens2[1:-1])

        ops1 = [t for t in tokens1[1:-1] if t in operators]
        ops2 = [t for t in tokens2[1:-1] if t in operators]

        feats1 = [t for t in tokens1[1:-1] if t in feature_names]
        feats2 = [t for t in tokens2[1:-1] if t in feature_names]

        # Tokenç›¸ä¼¼åº¦
        if len(tokens1_set) > 0 and len(tokens2_set) > 0:
            intersection = len(tokens1_set & tokens2_set)
            union = len(tokens1_set | tokens2_set)
            token_sim = intersection / union if union > 0 else 0.0
        else:
            token_sim = 0.0

        # æ“ä½œç¬¦ç›¸ä¼¼åº¦
        if len(ops1) > 0 and len(ops2) > 0:
            common_ops = len(set(ops1) & set(ops2))
            total_ops = max(len(ops1), len(ops2))
            op_sim = common_ops / total_ops if total_ops > 0 else 0.0
        else:
            op_sim = 0.0

        # ç‰¹å¾ç›¸ä¼¼åº¦
        if len(feats1) > 0 and len(feats2) > 0:
            common_feats = len(set(feats1) & set(feats2))
            total_feats = max(len(feats1), len(feats2))
            feat_sim = common_feats / total_feats if total_feats > 0 else 0.0
        else:
            feat_sim = 0.0

        overall_sim = 0.4 * token_sim + 0.4 * op_sim + 0.2 * feat_sim
        return overall_sim

    # æµ‹è¯•æ¡ˆä¾‹
    test_cases = [
        # (expr1, expr2, expected_similarity_range)
        (
            ['<BEG>', 'high', 'high', 'corr20', 'ema10', '<SEP>'],
            ['<BEG>', 'high', 'high', 'corr20', 'decay10', '<SEP>'],
            (0.4, 0.7)  # ä¸­é«˜ç›¸ä¼¼åº¦ (åªæœ‰æœ€åä¸€ä¸ªæ“ä½œç¬¦ä¸åŒ)
        ),
        (
            ['<BEG>', 'high', 'low', 'sub', 'sma10', '<SEP>'],
            ['<BEG>', 'close', 'volume', 'mul', 'ema10', '<SEP>'],
            (0.0, 0.3)  # ä½ç›¸ä¼¼åº¦
        ),
        (
            ['<BEG>', 'close', 'sma10', '<SEP>'],
            ['<BEG>', 'close', 'sma10', '<SEP>'],
            (0.95, 1.0)  # å®Œå…¨ç›¸åŒ
        ),
    ]

    print("\næµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—:")
    for i, (expr1, expr2, expected_range) in enumerate(test_cases, 1):
        sim = calculate_similarity(expr1, expr2)
        print(f"\næ¡ˆä¾‹ {i}:")
        print(f"  è¡¨è¾¾å¼1: {' '.join(expr1[1:-1])}")
        print(f"  è¡¨è¾¾å¼2: {' '.join(expr2[1:-1])}")
        print(f"  ç›¸ä¼¼åº¦: {sim:.3f}")
        print(f"  æœŸæœ›èŒƒå›´: [{expected_range[0]:.2f}, {expected_range[1]:.2f}]")

        assert expected_range[0] <= sim <= expected_range[1], \
            f"ç›¸ä¼¼åº¦ {sim:.3f} ä¸åœ¨é¢„æœŸèŒƒå›´å†…"

    print("\nâœ… æµ‹è¯•é€šè¿‡: ç›¸ä¼¼åº¦è®¡ç®—å‡†ç¡®!")


def test_diversity_penalty():
    """æµ‹è¯•å¤šæ ·æ€§æƒ©ç½šæœºåˆ¶"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: å¤šæ ·æ€§æƒ©ç½šæœºåˆ¶")
    print("=" * 60)

    # æ¨¡æ‹Ÿä¸åŒç›¸ä¼¼åº¦ä¸‹çš„æƒ©ç½š
    test_similarities = [0.2, 0.4, 0.6, 0.8]

    print("\nç›¸ä¼¼åº¦ -> æƒ©ç½šæ˜ å°„:")
    for sim in test_similarities:
        if sim > 0.7:
            penalty = -0.5 * sim
        elif sim > 0.5:
            penalty = -0.3 * sim
        elif sim > 0.3:
            penalty = -0.1 * sim
        else:
            penalty = 0.0

        print(f"  ç›¸ä¼¼åº¦ {sim:.2f} -> æƒ©ç½š {penalty:.4f}")

    print("\nâœ… æµ‹è¯•é€šè¿‡: æƒ©ç½šæœºåˆ¶ç¬¦åˆé¢„æœŸ!")


if __name__ == '__main__':
    print("\n" + "ğŸš€" * 30)
    print("å¼€å§‹æµ‹è¯•è¯­æ³•çº¦æŸå’Œå¤šæ ·æ€§æ”¹è¿›")
    print("ğŸš€" * 30 + "\n")

    try:
        test_grammar_constraints()
        test_diversity_similarity()
        test_diversity_penalty()

        print("\n" + "=" * 60)
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        print("=" * 60)
        print("\næ”¹è¿›æ€»ç»“:")
        print("1. âœ… è¯­æ³•çº¦æŸå¼ºåŒ– - RPNæ ˆå¹³è¡¡ä¿è¯")
        print("2. âœ… ç›¸ä¼¼åº¦è®¡ç®— - å¤šç»´åº¦è¯„ä¼°")
        print("3. âœ… å¤šæ ·æ€§æƒ©ç½š - è‡ªé€‚åº”æƒ©ç½šæœºåˆ¶")
        print("\né¢„æœŸæ•ˆæœ:")
        print("  - invalid_format å¤±è´¥ç‡: ä» 80%+ é™è‡³ <10%")
        print("  - å› å­å¤šæ ·æ€§: é¿å…åŒè´¨åŒ–å› å­")
        print("  - æ•´ä½“æ€§èƒ½: æå‡æŒ–æ˜æ•ˆç‡å’Œè´¨é‡")

    except AssertionError as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ è¿è¡Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
