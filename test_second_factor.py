"""
æµ‹è¯•ç¬¬äºŒä¸ªã€ç¬¬ä¸‰ä¸ªå› å­å…¥æ± æ—¶çš„è®¡ç®—é€»è¾‘
"""
import sys
import os
import pandas as pd
import numpy as np
import logging

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)
sys.path.insert(0, os.path.join(current_dir, 'factor'))
sys.path.insert(0, os.path.join(current_dir, 'config'))
sys.path.insert(0, os.path.join(current_dir, 'utils'))

from config import TrainingConfig
from signals import SignalGenerator, PerformanceEvaluator
from evaluator import ICDiversityEvaluator
from combiner import ImprovedCombinationModel

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def generate_synthetic_data():
    """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®"""
    logger.info("ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®...")
    np.random.seed(42)
    n_bars = 3000

    # ç”Ÿæˆä»·æ ¼åºåˆ—ï¼ˆæ¨¡æ‹ŸBTCï¼‰
    returns = np.random.randn(n_bars) * 0.02 + 0.0001
    prices = 10000 * np.exp(np.cumsum(returns))

    # ç”ŸæˆOHLCVæ•°æ®
    data = pd.DataFrame({
        'open': prices * (1 + np.random.randn(n_bars) * 0.005),
        'high': prices * (1 + np.abs(np.random.randn(n_bars)) * 0.01),
        'low': prices * (1 - np.abs(np.random.randn(n_bars)) * 0.01),
        'close': prices,
        'volume': np.random.rand(n_bars) * 1000 + 500,
    })

    data['high'] = data[['open', 'close', 'high']].max(axis=1)
    data['low'] = data[['open', 'close', 'low']].min(axis=1)

    return data

def create_factor(data: pd.DataFrame, factor_type: str) -> pd.Series:
    """
    åˆ›å»ºä¸åŒç±»å‹çš„å› å­

    Args:
        data: ä»·æ ¼æ•°æ®
        factor_type: å› å­ç±»å‹ ('momentum_5', 'momentum_20', 'volatility', 'volume')
    """
    if factor_type == 'momentum_5':
        # 5æœŸåŠ¨é‡
        returns = data['close'].pct_change(5)
        mean = returns.rolling(100, min_periods=20).mean()
        std = returns.rolling(100, min_periods=20).std()
        factor = ((returns - mean) / (std + 1e-8)).fillna(0).clip(-3, 3)

    elif factor_type == 'momentum_20':
        # 20æœŸåŠ¨é‡ï¼ˆæ›´é•¿å‘¨æœŸï¼‰
        returns = data['close'].pct_change(20)
        mean = returns.rolling(100, min_periods=20).mean()
        std = returns.rolling(100, min_periods=20).std()
        factor = ((returns - mean) / (std + 1e-8)).fillna(0).clip(-3, 3)

    elif factor_type == 'volatility':
        # æ³¢åŠ¨ç‡å› å­
        returns = data['close'].pct_change()
        volatility = returns.rolling(20, min_periods=10).std()
        mean = volatility.rolling(100, min_periods=20).mean()
        std = volatility.rolling(100, min_periods=20).std()
        factor = ((volatility - mean) / (std + 1e-8)).fillna(0).clip(-3, 3)

    elif factor_type == 'volume':
        # æˆäº¤é‡å› å­
        volume_ma = data['volume'].rolling(20, min_periods=10).mean()
        volume_ratio = data['volume'] / (volume_ma + 1e-8)
        mean = volume_ratio.rolling(100, min_periods=20).mean()
        std = volume_ratio.rolling(100, min_periods=20).std()
        factor = ((volume_ratio - mean) / (std + 1e-8)).fillna(0).clip(-3, 3)

    else:
        raise ValueError(f"Unknown factor type: {factor_type}")

    return factor

def test_incremental_addition():
    """æµ‹è¯•å› å­é€ä¸ªæ·»åŠ çš„è¿‡ç¨‹"""
    logger.info("="*80)
    logger.info("æµ‹è¯•å› å­é€ä¸ªæ·»åŠ çš„å¢é‡è®¡ç®—é€»è¾‘")
    logger.info("="*80)

    # 1. å‡†å¤‡æ•°æ®
    data = generate_synthetic_data()
    config = TrainingConfig()

    train_size = int(len(data) * 0.6)
    train_data = data[:train_size].copy()
    val_data = data[train_size:].copy()

    # è®¡ç®—ç›®æ ‡å€¼
    train_data['target'] = train_data['close'].pct_change(config.prediction_horizon).shift(-config.prediction_horizon)
    val_data['target'] = val_data['close'].pct_change(config.prediction_horizon).shift(-config.prediction_horizon)

    train_data = train_data.dropna()
    val_data = val_data.dropna()

    train_target = train_data['target']
    val_target = val_data['target']

    logger.info(f"è®­ç»ƒé›†å¤§å°: {len(train_data)}, éªŒè¯é›†å¤§å°: {len(val_data)}")

    # 2. åˆ›å»ºè¯„ä¼°å™¨å’Œç»„åˆå™¨
    evaluator = ICDiversityEvaluator(config)
    combiner = ImprovedCombinationModel(config, max_alpha_count=15)
    combiner.set_targets(train_target, val_target)
    combiner.set_evaluator(evaluator)
    evaluator.set_combiner(combiner)

    # 3. å‡†å¤‡å¤šä¸ªå› å­
    factor_types = ['momentum_5', 'momentum_20', 'volatility', 'volume']
    factors = {}

    for ftype in factor_types:
        train_factor = create_factor(train_data, ftype)
        val_factor = create_factor(val_data, ftype)
        factors[ftype] = {
            'train': train_factor,
            'val': val_factor
        }
        logger.info(f"åˆ›å»ºå› å­ {ftype}: train_mean={train_factor.mean():.4f}, train_std={train_factor.std():.4f}")

    # 4. é€ä¸ªæ·»åŠ å› å­
    logger.info("\n" + "="*80)
    logger.info("å¼€å§‹é€ä¸ªæ·»åŠ å› å­")
    logger.info("="*80)

    results = []

    for i, ftype in enumerate(factor_types, 1):
        logger.info(f"\n{'='*80}")
        logger.info(f"ç¬¬ {i} ä¸ªå› å­: {ftype}")
        logger.info(f"å½“å‰æ± å­å¤§å°: {len(combiner.alpha_pool)}")
        logger.info(f"å½“å‰åŸºå‡†åˆ†æ•°: {combiner.base_train_score:.6f}")
        logger.info(f"{'='*80}")

        train_factor = factors[ftype]['train']
        val_factor = factors[ftype]['val']

        # 4.1 è¯•ç®—æ¨¡å¼ï¼šè®¡ç®—å¢é‡
        trial_result = combiner.evaluate_new_factor(
            alpha_info={'name': ftype},
            train_factor=train_factor,
            val_factor=val_factor
        )

        incremental_sharpe = trial_result.get('train_incremental_sharpe', 0.0)
        new_train_score = trial_result['train_stats'].get('sharpe', 0.0)

        # 4.2 åˆ¤æ–­é˜ˆå€¼
        pool_size = len(combiner.alpha_pool)
        base_threshold = config.ic_threshold

        if pool_size < 3:
            threshold = -0.03
            threshold_desc = "å‰3ä¸ªå› å­ï¼Œé˜ˆå€¼=-0.03"
        elif pool_size < 5:
            threshold = 0.001
            threshold_desc = "ç¬¬4-5ä¸ªå› å­ï¼Œé˜ˆå€¼=0.001"
        elif pool_size < 10:
            threshold = base_threshold * 0.3
            threshold_desc = f"ç¬¬6-10ä¸ªå› å­ï¼Œé˜ˆå€¼={threshold:.4f}"
        else:
            threshold = base_threshold * 0.6
            threshold_desc = f"10ä¸ªä»¥ä¸Šå› å­ï¼Œé˜ˆå€¼={threshold:.4f}"

        qualifies = incremental_sharpe > threshold

        logger.info(f"\nğŸ“Š è¯•ç®—ç»“æœ:")
        logger.info(f"  åŸºå‡†åˆ†æ•° (æ—§):      {combiner.base_train_score:.6f}")
        logger.info(f"  æ–°ç»„åˆåˆ†æ•°:         {new_train_score:.6f}")
        logger.info(f"  å¢é‡ Sharpe:        {incremental_sharpe:.6f}")
        logger.info(f"  å…¥æ± é˜ˆå€¼:           {threshold:.6f} ({threshold_desc})")
        logger.info(f"  æ˜¯å¦æ»¡è¶³æ¡ä»¶:       {'âœ… æ˜¯' if qualifies else 'âŒ å¦'}")

        # 4.3 å¦‚æœæ»¡è¶³æ¡ä»¶ï¼ŒçœŸæ­£æ·»åŠ 
        if qualifies:
            logger.info(f"\nğŸ’š å› å­ {ftype} æ»¡è¶³æ¡ä»¶ï¼Œæ·»åŠ åˆ°æ± å­...")

            commit_result = combiner.add_alpha_and_optimize(
                alpha_info={'name': ftype, 'type': ftype},
                train_factor=train_factor,
                val_factor=val_factor
            )

            new_pool_size = commit_result.get('pool_size', 0)
            new_base_score = commit_result.get('current_train_score', 0.0)
            actual_increment = commit_result.get('incremental_contribution', 0.0)

            logger.info(f"  âœ… æ·»åŠ æˆåŠŸ!")
            logger.info(f"  æ–°æ± å­å¤§å°:         {new_pool_size}")
            logger.info(f"  æ–°åŸºå‡†åˆ†æ•°:         {new_base_score:.6f}")
            logger.info(f"  å®é™…å¢é‡:           {actual_increment:.6f}")

            # æ˜¾ç¤ºæƒé‡
            if combiner.current_weights is not None:
                weights = combiner.current_weights
                logger.info(f"  å½“å‰æƒé‡:")
                for j, w in enumerate(weights):
                    factor_name = combiner.alpha_pool[j].get('name', f'alpha_{j}')
                    logger.info(f"    [{j}] {factor_name}: {w:.6f}")

            result_status = "âœ… å·²æ·»åŠ "
        else:
            logger.info(f"\nğŸ’” å› å­ {ftype} ä¸æ»¡è¶³æ¡ä»¶ï¼Œæ‹’ç»æ·»åŠ ")
            logger.info(f"  åŸå› : å¢é‡ {incremental_sharpe:.6f} <= é˜ˆå€¼ {threshold:.6f}")
            result_status = "âŒ è¢«æ‹’ç»"

        # è®°å½•ç»“æœ
        results.append({
            'order': i,
            'factor': ftype,
            'pool_size_before': pool_size,
            'base_score_before': combiner.base_train_score if not qualifies else commit_result.get('current_train_score', 0.0) - actual_increment if qualifies else combiner.base_train_score,
            'incremental_sharpe': incremental_sharpe,
            'threshold': threshold,
            'qualifies': qualifies,
            'status': result_status,
            'pool_size_after': len(combiner.alpha_pool),
            'base_score_after': combiner.base_train_score
        })

    # 5. æ€»ç»“
    logger.info("\n" + "="*80)
    logger.info("ğŸ“‹ æ€»ç»“")
    logger.info("="*80)

    results_df = pd.DataFrame(results)

    logger.info(f"\næœ€ç»ˆæ± å­å¤§å°: {len(combiner.alpha_pool)}")
    logger.info(f"æœ€ç»ˆåŸºå‡†åˆ†æ•°: {combiner.base_train_score:.6f}")

    logger.info(f"\nå„å› å­å°è¯•ç»“æœ:")
    for _, row in results_df.iterrows():
        logger.info(f"  [{row['order']}] {row['factor']:15s} | "
                   f"å¢é‡={row['incremental_sharpe']:7.4f} | "
                   f"é˜ˆå€¼={row['threshold']:7.4f} | "
                   f"{row['status']}")

    logger.info(f"\nå…¥æ± å› å­:")
    accepted = results_df[results_df['qualifies']]
    if len(accepted) > 0:
        for _, row in accepted.iterrows():
            logger.info(f"  [{row['order']}] {row['factor']}: å¢é‡={row['incremental_sharpe']:.6f}")
    else:
        logger.info("  æ— å› å­å…¥æ± ")

    logger.info(f"\nè¢«æ‹’ç»å› å­:")
    rejected = results_df[~results_df['qualifies']]
    if len(rejected) > 0:
        for _, row in rejected.iterrows():
            logger.info(f"  [{row['order']}] {row['factor']}: å¢é‡={row['incremental_sharpe']:.6f} < é˜ˆå€¼={row['threshold']:.6f}")
    else:
        logger.info("  æ‰€æœ‰å› å­éƒ½å…¥æ± äº†")

    return results_df

if __name__ == "__main__":
    try:
        results = test_incremental_addition()

        print("\n" + "="*80)
        print("âœ… æµ‹è¯•å®Œæˆ")
        print("="*80)

    except Exception as e:
        logger.error(f"æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        logger.error(traceback.format_exc())
