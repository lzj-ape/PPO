"""
å›æµ‹æ¨¡å— - å®Œæ•´çš„å› å­å›æµ‹å’Œåˆ†æ
åŒ…æ‹¬ï¼š
1. å•å› å­å›æµ‹ï¼ˆè‡ªåŠ¨è®¡ç®—ICï¼‰
2. å› å­ç»„åˆå›æµ‹ï¼ˆä»alpha_poolè®¡ç®—ï¼‰
3. æ‰‹ç»­è´¹ä¼˜åŒ–
4. è®­ç»ƒé›†vsæµ‹è¯•é›†å¯¹æ¯”
5. å› å­ç›¸å…³æ€§åˆ†æ
6. æ”¶ç›Šæ›²çº¿å¯¹æ¯”å›¾
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional
from scipy.stats import spearmanr
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
class Backtester:
    """é«˜çº§å›æµ‹å™¨ - å¸¦ICæ–¹å‘è°ƒæ•´å’Œæ‰‹ç»­è´¹ä¼˜åŒ–"""
    def __init__(self, 
                 prediction_horizon: int = 10,
                 bar_minutes: int = 15,
                 transaction_cost: float = 0.0005,
                 max_position: float = 0.1,
                 signal_threshold: float = 0.1):  # å…¼å®¹æ€§å‚æ•°ï¼Œæš‚æ—¶ä¿ç•™
        """
        å‚æ•°:
        ----
        max_position: æœ€å¤§æŒä»“æ¯”ä¾‹ï¼ˆé»˜è®¤0.1å³10%ï¼‰
        æ³¨æ„ï¼šä¿¡å·ç”Ÿæˆä½¿ç”¨æ»šåŠ¨åˆ†ä½æ•°æ–¹æ³•ï¼Œä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´
        """
        self.prediction_horizon = prediction_horizon
        self.bar_minutes = bar_minutes
        self.transaction_cost = transaction_cost
        self.max_position = max_position
        self.signal_threshold = signal_threshold  # å…¼å®¹æ€§ä¿ç•™ï¼Œæš‚ä¸ä½¿ç”¨
        
        self.bars_per_year = 365 * 24 * 60 / bar_minutes
        self.trades_per_year = self.bars_per_year / prediction_horizon
        
        print(f"ğŸ“Š å›æµ‹å™¨åˆå§‹åŒ–:")
        print(f"   é¢„æµ‹å‘¨æœŸ: {prediction_horizon} bars")
        print(f"   äº¤æ˜“æˆæœ¬: {transaction_cost*100:.3f}%")
        print(f"   æœ€å¤§æŒä»“: {max_position*100:.1f}%")
        print(f"   ä¿¡å·ç”Ÿæˆ: æ»šåŠ¨åˆ†ä½æ•°æ–¹æ³• (q_low=0.3, q_high=0.7)")
        print(f"   æŒä»“ç­–ç•¥: å§‹ç»ˆä¿æŒæŒä»“ï¼Œä¸­æ€§åŒºåŸŸ1/10ä»“ä½")
    
    def generate_signals(self, factor: pd.Series, lookback: int = 100) -> pd.Series:
        """
        ç”Ÿæˆäº¤æ˜“ä¿¡å· - å§‹ç»ˆä¿æŒæŒä»“

        ä½¿ç”¨æ»šåŠ¨åˆ†ä½æ•°æ–¹æ³•ï¼š
        1. è®¡ç®—æ»šåŠ¨åˆ†ä½æ•°é˜ˆå€¼ï¼ˆq_low=0.3, q_high=0.7ï¼‰
        2. ç”Ÿæˆè¿ç»­ä¿¡å·ï¼š
           - é«˜äºq_high: æ»¡ä»“åšå¤š (max_position)
           - ä½äºq_low: æ»¡ä»“åšç©º (-max_position)
           - ä¸­é—´åŒºåŸŸ: åŸºç¡€æŒä»“ (max_position / 10)ï¼Œå³å§‹ç»ˆä¿æŒ1/10ä»“ä½

        æ³¨æ„ï¼š
        - å§‹ç»ˆä¿æŒæŒä»“ï¼Œé¿å…ç©ºä»“æœŸ
        - ä¸­æ€§åŒºåŸŸä¿æŒå°ä»“ä½ï¼Œæ ¹æ®å› å­æ–¹å‘å†³å®šå¤šç©º
        """
        # ä½¿ç”¨æ»šåŠ¨åˆ†ä½æ•°
        q_low = 0.3
        q_high = 0.7

        # è®¡ç®—æ»šåŠ¨åˆ†ä½æ•°é˜ˆå€¼
        rolling_low = factor.rolling(window=lookback, min_periods=20).quantile(q_low)
        rolling_high = factor.rolling(window=lookback, min_periods=20).quantile(q_high)
        rolling_mid = factor.rolling(window=lookback, min_periods=20).median()

        # ç”Ÿæˆä¿¡å·ï¼šå§‹ç»ˆæœ‰æŒä»“
        signals = pd.Series(0.0, index=factor.index)

        # å¼ºä¿¡å·åŒºåŸŸï¼šæ»¡ä»“
        signals[factor > rolling_high] = self.max_position
        signals[factor < rolling_low] = -self.max_position

        # ä¸­æ€§åŒºåŸŸï¼šä¿æŒåŸºç¡€æŒä»“ï¼ˆ1/10ä»“ä½ï¼‰
        neutral_mask = (factor >= rolling_low) & (factor <= rolling_high)
        # æ ¹æ®å› å­ç›¸å¯¹ä¸­ä½æ•°çš„ä½ç½®å†³å®šå¤šç©ºæ–¹å‘
        # signals[neutral_mask & (factor >= rolling_mid)] = self.max_position / 10
        # signals[neutral_mask & (factor < rolling_mid)] = -self.max_position / 10
        return signals
    
    def backtest_single_factor(self,
                               factor: pd.Series,
                               returns: pd.Series,
                               lookback: int = 100,
                               name: str = "Factor",
                               factor_is_normalized: bool = False) -> Dict:
        """
        å•å› å­å›æµ‹ - ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´

        å…³é”®é€»è¾‘ï¼š
        1. ICç”¨åŸå§‹å› å­å€¼è®¡ç®—ï¼ˆæ›´çœŸå®åæ˜ é¢„æµ‹èƒ½åŠ›ï¼‰
        2. ä¿¡å·ç”Ÿæˆç”¨æ ‡å‡†åŒ–å› å­å€¼ï¼ˆç»Ÿä¸€å°ºåº¦ï¼‰

        å‚æ•°:
        ----
        factor: å› å­å€¼ï¼ˆåŸå§‹å€¼æˆ–æ ‡å‡†åŒ–å€¼ï¼‰
        returns: å®é™…æ”¶ç›Šç‡ï¼ˆtargetï¼‰
        lookback: å›çœ‹çª—å£
        name: å› å­åç§°
        factor_is_normalized: å› å­æ˜¯å¦å·²æ ‡å‡†åŒ–ï¼ˆé»˜è®¤Falseï¼Œå³åŸå§‹å€¼ï¼‰

        è¿”å›:
        ----
        åŒ…å«å„ç§æŒ‡æ ‡çš„å­—å…¸ï¼ŒåŒ…æ‹¬è‡ªåŠ¨è®¡ç®—çš„ICå’ŒSharpe
        """
        # å¯¹é½æ•°æ®
        aligned = pd.DataFrame({
            'factor': factor,
            'returns': returns
        }).dropna()
        if len(aligned) < lookback + self.prediction_horizon * 5:
            return {'error': 'Insufficient data', 'name': name}

        # â­ è®¡ç®—ICï¼šç”¨åŸå§‹å› å­å€¼
        ic = aligned['factor'].corr(aligned['returns'])

        # â­ ç”Ÿæˆä¿¡å·ï¼šå¦‚æœå› å­æœªæ ‡å‡†åŒ–ï¼Œå…ˆæ ‡å‡†åŒ–
        if not factor_is_normalized:
            # å¯¹å› å­è¿›è¡Œæ»šåŠ¨æ ‡å‡†åŒ–ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
            factor_for_signal = aligned['factor'].copy()
            rolling_mean = factor_for_signal.rolling(window=lookback, min_periods=20).mean()
            rolling_std = factor_for_signal.rolling(window=lookback, min_periods=20).std()
            factor_for_signal = (factor_for_signal - rolling_mean) / (rolling_std + 1e-8)
            factor_for_signal = factor_for_signal.fillna(0).clip(-3, 3)
        else:
            factor_for_signal = aligned['factor']

        # ç”Ÿæˆä¿¡å·ï¼ˆåŸºäºæ ‡å‡†åŒ–åçš„å› å­å€¼ï¼‰
        signals = self.generate_signals(factor_for_signal, lookback)
        
        # éé‡å é‡‡æ ·
        valid_start = lookback
        valid_indices = list(range(valid_start,len(aligned)))
        
        if len(valid_indices) < 10:
            return {'error': 'Insufficient trades', 'name': name}
        
        signals_sampled = signals.iloc[valid_indices]
        returns_sampled = aligned['returns'].iloc[valid_indices]
        
        # è®¡ç®—æ”¶ç›Š
        gross_returns = signals_sampled * returns_sampled
        
        # è®¡ç®—äº¤æ˜“æˆæœ¬ï¼ˆæ¢æ‰‹æˆæœ¬ï¼‰
        position_changes = signals_sampled.diff().abs().fillna(signals_sampled.abs())
        turnover = position_changes.sum() / len(position_changes)  # å¹³å‡æ¯æœŸæ¢æ‰‹
        transaction_costs = position_changes * self.transaction_cost
        
        # å‡€æ”¶ç›Š
        net_returns = gross_returns - transaction_costs
        
        # è®¡ç®—å„ç§æŒ‡æ ‡
        metrics = self._calculate_comprehensive_metrics(
            net_returns, gross_returns, signals_sampled, turnover
        )
        
        # æ·»åŠ é¢å¤–ä¿¡æ¯ï¼ˆåŒ…æ‹¬ICï¼‰
        metrics.update({
            'name': name,
            'ic': ic,  # ğŸ†• ICä½œä¸ºè¾“å‡ºç»“æœ
            'ic_direction': 'Long' if ic >= 0 else 'Short',
            'equity_curve': (1 + net_returns).cumprod(),
            'gross_equity': (1 + gross_returns).cumprod(),
            'signals': signals_sampled,
            'returns': net_returns,
            'gross_returns': gross_returns,
            'costs': transaction_costs,
            'turnover': turnover,
            'valid_indices': valid_indices,
        })
        
        return metrics
    
    def _calculate_comprehensive_metrics(self, 
                                        net_returns: pd.Series,
                                        gross_returns: pd.Series,
                                        signals: pd.Series,
                                        turnover: float) -> Dict:
        """è®¡ç®—å…¨é¢çš„å›æµ‹æŒ‡æ ‡"""
        if len(net_returns) < 5:
            return {}
        
        # åŸºç¡€ç»Ÿè®¡
        total_return = (1 + net_returns).prod() - 1
        gross_total_return = (1 + gross_returns).prod() - 1
        mean_return = net_returns.mean()
        std_return = net_returns.std()
        
        # å¹´åŒ–æ”¶ç›Š
        num_trades = len(net_returns)
        annual_return = (1 + total_return) ** (self.trades_per_year / num_trades) - 1
        
        # Sharpeæ¯”ç‡
        if std_return > 1e-8:
            sharpe = np.sqrt(self.trades_per_year) * mean_return / std_return
        else:
            sharpe = 0
        
        # æœ€å¤§å›æ’¤
        cum_returns = (1 + net_returns).cumprod()
        running_max = cum_returns.expanding().max()
        drawdown = (cum_returns - running_max) / running_max
        max_drawdown = abs(drawdown.min()) if len(drawdown) > 0 else 0
        
        # Calmaræ¯”ç‡
        if max_drawdown > 1e-6:
            calmar = annual_return / max_drawdown
        else:
            calmar = 0
        
        # èƒœç‡ - ç°åœ¨æ‰€æœ‰äº¤æ˜“éƒ½æœ‰æŒä»“
        win_rate = (net_returns > 0).sum() / len(net_returns) if len(net_returns) > 0 else 0
        num_active_trades = len(net_returns)

        # ç›ˆäºæ¯”
        winning_returns = net_returns[net_returns > 0]
        losing_returns = net_returns[net_returns < 0]
        
        if len(losing_returns) > 0 and losing_returns.mean() != 0:
            profit_loss_ratio = abs(winning_returns.mean() / losing_returns.mean())
        else:
            profit_loss_ratio = np.nan
        
        # ä¿¡æ¯æ¯”ç‡
        if std_return > 1e-8:
            ir = mean_return / std_return
        else:
            ir = 0
        
        # æ³¢åŠ¨ç‡ï¼ˆå¹´åŒ–ï¼‰
        volatility = std_return * np.sqrt(self.trades_per_year)
        
        # å¹³å‡æŒä»“
        avg_position = signals.abs().mean()
        
        # æ‰‹ç»­è´¹å æ¯”
        cost_ratio = ((gross_total_return - total_return) / (gross_total_return + 1e-8)) if gross_total_return > 0 else np.nan
        
        return {
            'total_return': total_return,
            'gross_total_return': gross_total_return,
            'annual_return': annual_return,
            'volatility': volatility,
            'sharpe_ratio': sharpe,
            'max_drawdown': max_drawdown,
            'calmar_ratio': calmar,
            'win_rate': win_rate,
            'profit_loss_ratio': profit_loss_ratio,
            'information_ratio': ir,
            'num_trades': num_trades,
            'num_active_trades': num_active_trades,
            'avg_position': avg_position,
            'cost_ratio': cost_ratio,
            'mean_return': mean_return,
            'std_return': std_return,
        }
    
    def backtest_factor_on_both_sets(self,
                                     train_factor: pd.Series,
                                     test_factor: pd.Series,
                                     train_returns: pd.Series,
                                     test_returns: pd.Series,
                                     name: str = "Factor") -> Dict:
        """
        åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šå›æµ‹åŒä¸€ä¸ªå› å­
        
        å‚æ•°:
        ----
        train_factor: è®­ç»ƒé›†å› å­å€¼
        test_factor: æµ‹è¯•é›†å› å­å€¼
        train_returns: è®­ç»ƒé›†æ”¶ç›Šç‡
        test_returns: æµ‹è¯•é›†æ”¶ç›Šç‡
        name: å› å­åç§°
        
        è¿”å›:
        ----
        {
            'train': train_metrics,  # åŒ…å«IC
            'test': test_metrics,    # åŒ…å«IC
            'name': name
        }
        """
        print(f"\nğŸ“Š å›æµ‹å› å­: {name}")
        
        # è®­ç»ƒé›†å›æµ‹ï¼ˆè‡ªåŠ¨è®¡ç®—ICï¼‰
        train_metrics = self.backtest_single_factor(
            train_factor, train_returns, name=f"{name}_train"
        )
        
        # æµ‹è¯•é›†å›æµ‹ï¼ˆè‡ªåŠ¨è®¡ç®—ICï¼‰
        test_metrics = self.backtest_single_factor(
            test_factor, test_returns, name=f"{name}_test"
        )
        
        # æ‰“å°ICä¿¡æ¯
        if 'error' not in train_metrics and 'error' not in test_metrics:
            train_ic = train_metrics.get('ic', 0)
            test_ic = test_metrics.get('ic', 0)
            print(f"   è®­ç»ƒé›† IC: {train_ic:.4f} ({'Long' if train_ic >= 0 else 'Short'})")
            print(f"   æµ‹è¯•é›† IC: {test_ic:.4f} ({'Long' if test_ic >= 0 else 'Short'})")
        
            print(f"   è®­ç»ƒé›† - æ”¶ç›Š: {train_metrics['total_return']*100:.2f}%, "
                  f"Sharpe: {train_metrics['sharpe_ratio']:.2f}, "
                  f"æ¢æ‰‹: {train_metrics['turnover']:.2f}")
            print(f"   æµ‹è¯•é›† - æ”¶ç›Š: {test_metrics['total_return']*100:.2f}%, "
                  f"Sharpe: {test_metrics['sharpe_ratio']:.2f}, "
                  f"æ¢æ‰‹: {test_metrics['turnover']:.2f}")
        
        return {
            'train': train_metrics,
            'test': test_metrics,
            'name': name
        }


class FactorAnalyzer:
    """å› å­åˆ†æå™¨ - å¯è§†åŒ–å’Œç›¸å…³æ€§åˆ†æ"""
    
    def __init__(self):
        self.results = []
    
    def add_backtest_result(self, result: Dict):
        """æ·»åŠ å›æµ‹ç»“æœ"""
        self.results.append(result)
    
    def plot_all_factors_comparison(self, save_path: str = 'factors_comparison.png'):
        """
        ç»˜åˆ¶æ‰€æœ‰å› å­çš„è®­ç»ƒé›†vsæµ‹è¯•é›†å¯¹æ¯”å›¾
        æ¯ä¸ªå› å­ä¸€ä¸ªå­å›¾ï¼Œæ˜¾ç¤ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„equity curve
        """
        valid_results = [r for r in self.results 
                        if 'error' not in r.get('train', {}) 
                        and 'error' not in r.get('test', {})]
        
        if not valid_results:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„å›æµ‹ç»“æœ")
            return
        
        n_factors = len(valid_results)
        n_cols = 3
        n_rows = (n_factors + n_cols - 1) // n_cols
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5*n_rows))
        axes = axes.flatten() if n_factors > 1 else [axes]
        
        for idx, result in enumerate(valid_results):
            ax = axes[idx]
            
            train_metrics = result['train']
            test_metrics = result['test']
            name = result['name']
            ic = result.get('ic', 0)
            
            # è®­ç»ƒé›†equity curve
            train_equity = train_metrics['equity_curve']
            ax.plot(train_equity.values, label='Train', linewidth=2, alpha=0.8)
            
            # æµ‹è¯•é›†equity curve
            test_equity = test_metrics['equity_curve']
            ax.plot(test_equity.values, label='Test', linewidth=2, alpha=0.8)
            
            # æ ‡é¢˜å’Œç»Ÿè®¡
            train_return = train_metrics['total_return'] * 100
            test_return = test_metrics['total_return'] * 100
            train_sharpe = train_metrics['sharpe_ratio']
            test_sharpe = test_metrics['sharpe_ratio']
            
            title = (f"{name}\n"
                    f"IC={ic:.3f} | Train: {train_return:.1f}% (SR={train_sharpe:.2f}) | "
                    f"Test: {test_return:.1f}% (SR={test_sharpe:.2f})")
            ax.set_title(title, fontsize=10, fontweight='bold')
            
            ax.axhline(y=1, color='gray', linestyle='--', alpha=0.3)
            ax.legend(loc='best')
            ax.grid(True, alpha=0.3)
            ax.set_xlabel('Time')
            ax.set_ylabel('Cumulative Return')
        
        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(n_factors, len(axes)):
            axes[idx].axis('off')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\nâœ… å› å­å¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}")
        plt.show()
    
    def plot_factors_correlation_heatmap(self, 
                                        train_factors: Dict[str, pd.Series],
                                        test_factors: Dict[str, pd.Series] = None,
                                        save_path: str = 'factors_correlation.png'):
        """
        ç»˜åˆ¶å› å­ç›¸å…³æ€§çƒ­åŠ›å›¾
        
        å‚æ•°:
        ----
        train_factors: {factor_name: factor_series} è®­ç»ƒé›†å› å­
        test_factors: {factor_name: factor_series} æµ‹è¯•é›†å› å­ï¼ˆå¯é€‰ï¼‰
        """
        # åˆ›å»ºè®­ç»ƒé›†å› å­çŸ©é˜µ
        train_df = pd.DataFrame(train_factors)
        train_df = train_df.dropna()
        
        # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
        train_corr = train_df.corr(method='spearman')
        
        # å¦‚æœæœ‰æµ‹è¯•é›†ï¼Œä¹Ÿè®¡ç®—
        if test_factors:
            test_df = pd.DataFrame(test_factors)
            test_df = test_df.dropna()
            test_corr = test_df.corr(method='spearman')
            
            # åˆ›å»º2ä¸ªå­å›¾
            fig, axes = plt.subplots(1, 2, figsize=(20, 8))
            
            # è®­ç»ƒé›†çƒ­åŠ›å›¾
            sns.heatmap(train_corr, annot=True, fmt='.2f', cmap='RdYlBu_r',
                       center=0, vmin=-1, vmax=1, square=True,
                       cbar_kws={'label': 'Spearman Correlation'},
                       ax=axes[0])
            axes[0].set_title('Training Set - Factor Correlation', 
                            fontsize=14, fontweight='bold', pad=20)
            
            # æµ‹è¯•é›†çƒ­åŠ›å›¾
            sns.heatmap(test_corr, annot=True, fmt='.2f', cmap='RdYlBu_r',
                       center=0, vmin=-1, vmax=1, square=True,
                       cbar_kws={'label': 'Spearman Correlation'},
                       ax=axes[1])
            axes[1].set_title('Test Set - Factor Correlation', 
                            fontsize=14, fontweight='bold', pad=20)
            
        else:
            # åªæœ‰è®­ç»ƒé›†
            fig, ax = plt.subplots(figsize=(12, 10))
            sns.heatmap(train_corr, annot=True, fmt='.2f', cmap='RdYlBu_r',
                       center=0, vmin=-1, vmax=1, square=True,
                       cbar_kws={'label': 'Spearman Correlation'},
                       ax=ax)
            ax.set_title('Factor Correlation Heatmap', 
                        fontsize=16, fontweight='bold', pad=20)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\nâœ… å› å­ç›¸å…³æ€§çƒ­åŠ›å›¾å·²ä¿å­˜: {save_path}")
        plt.show()
        
        # æ‰“å°é«˜ç›¸å…³æ€§çš„å› å­å¯¹
        print("\nğŸ” é«˜ç›¸å…³æ€§å› å­å¯¹ (|corr| > 0.7):")
        high_corr_pairs = []
        for i in range(len(train_corr.columns)):
            for j in range(i+1, len(train_corr.columns)):
                corr_val = train_corr.iloc[i, j]
                if abs(corr_val) > 0.7:
                    high_corr_pairs.append((
                        train_corr.columns[i],
                        train_corr.columns[j],
                        corr_val
                    ))
        
        if high_corr_pairs:
            for f1, f2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):
                print(f"   {f1} <-> {f2}: {corr:.3f}")
        else:
            print("   æœªå‘ç°é«˜ç›¸å…³æ€§å› å­å¯¹")
    
    def generate_summary_table(self) -> pd.DataFrame:
        """ç”Ÿæˆæ±‡æ€»è¡¨æ ¼"""
        valid_results = [r for r in self.results 
                        if 'error' not in r.get('train', {}) 
                        and 'error' not in r.get('test', {})]
        
        if not valid_results:
            return pd.DataFrame()
        
        summary_data = []
        for result in valid_results:
            train = result['train']
            test = result['test']
            
            summary_data.append({
                'Factor': result['name'],
                'IC': result.get('ic', 0),
                'Direction': 'Long' if result.get('ic', 0) >= 0 else 'Short',
                'Train_Return': train['total_return'],
                'Test_Return': test['total_return'],
                'Train_Sharpe': train['sharpe_ratio'],
                'Test_Sharpe': test['sharpe_ratio'],
                'Train_MaxDD': train['max_drawdown'],
                'Test_MaxDD': test['max_drawdown'],
                'Train_Turnover': train['turnover'],
                'Test_Turnover': test['turnover'],
                'Train_WinRate': train['win_rate'],
                'Test_WinRate': test['win_rate'],
                'Sharpe_Decay': train['sharpe_ratio'] - test['sharpe_ratio'],
            })
        
        df = pd.DataFrame(summary_data)
        
        # æ ¼å¼åŒ–
        df['Train_Return'] = df['Train_Return'].apply(lambda x: f"{x*100:.2f}%")
        df['Test_Return'] = df['Test_Return'].apply(lambda x: f"{x*100:.2f}%")
        df['IC'] = df['IC'].apply(lambda x: f"{x:.4f}")
        df['Train_Sharpe'] = df['Train_Sharpe'].apply(lambda x: f"{x:.2f}")
        df['Test_Sharpe'] = df['Test_Sharpe'].apply(lambda x: f"{x:.2f}")
        df['Train_MaxDD'] = df['Train_MaxDD'].apply(lambda x: f"{x*100:.2f}%")
        df['Test_MaxDD'] = df['Test_MaxDD'].apply(lambda x: f"{x*100:.2f}%")
        df['Train_Turnover'] = df['Train_Turnover'].apply(lambda x: f"{x:.2f}")
        df['Test_Turnover'] = df['Test_Turnover'].apply(lambda x: f"{x:.2f}")
        df['Sharpe_Decay'] = df['Sharpe_Decay'].apply(lambda x: f"{x:.2f}")
        
        return df
    
    def print_summary(self):
        """æ‰“å°æ±‡æ€»ä¿¡æ¯"""
        print("\n" + "="*100)
        print("ğŸ“Š å› å­å›æµ‹æ±‡æ€»")
        print("="*100)
        
        df = self.generate_summary_table()
        if len(df) > 0:
            print(df.to_string(index=False))
            
            # ä¿å­˜åˆ°CSV
            csv_path = 'backtest_summary.csv'
            df.to_csv(csv_path, index=False)
            print(f"\nâœ… æ±‡æ€»è¡¨æ ¼å·²ä¿å­˜: {csv_path}")
        else:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„å›æµ‹ç»“æœ")

def compute_factor_from_tokens(
    tokens: List[str],
    data: pd.DataFrame,
    operators: Dict,
    rolling_window: int = 100,  # æ»šåŠ¨çª—å£å¤§å°ï¼Œé»˜è®¤100ä¸ªå‘¨æœŸ
    normalize: bool = True  # æ˜¯å¦è¿›è¡Œæ»šåŠ¨æ ‡å‡†åŒ–
) -> Optional[pd.Series]:
    """
    æ ¹æ®tokensè¡¨è¾¾å¼è®¡ç®—å› å­å€¼
    å‚æ•°:
    ----
    tokens: list - è¡¨è¾¾å¼tokensï¼Œå¦‚ ['<BEG>', 'return', 'low', 'mul', '<SEP>']
    data: pd.DataFrame - åŸå§‹ç‰¹å¾æ•°æ®
    operators: dict - ç®—å­å­—å…¸
    rolling_window: int - æ»šåŠ¨æ ‡å‡†åŒ–çª—å£å¤§å°ï¼Œé»˜è®¤100
    normalize: bool - æ˜¯å¦è¿›è¡Œæ»šåŠ¨æ ‡å‡†åŒ–ï¼Œé»˜è®¤True
    è¿”å›:
    ----
    pd.Series - è®¡ç®—å¾—åˆ°çš„å› å­å€¼ï¼ˆæ ¹æ®normalizeå‚æ•°å†³å®šæ˜¯å¦æ ‡å‡†åŒ–ï¼‰
    """
    # ç§»é™¤<BEG>å’Œ<SEP>
    expr_tokens = tokens[1:-1]
    stack = []
    
    for token in expr_tokens:
        # å¦‚æœæ˜¯ç‰¹å¾åï¼ˆåˆ—åï¼‰
        if token in data.columns:
            stack.append(data[token].copy())
        # å¦‚æœæ˜¯ç®—å­
        elif token in operators:
            op_info = operators[token]
            arity = op_info['arity']
            func = op_info['func']
            
            # æ£€æŸ¥æ ˆæ˜¯å¦æœ‰è¶³å¤Ÿçš„æ“ä½œæ•°
            if len(stack) < arity:
                return None
            
            # å¼¹å‡ºæ“ä½œæ•°
            args = []
            for _ in range(arity):
                args.append(stack.pop())
            args.reverse()  # æ¢å¤æ­£ç¡®çš„é¡ºåº
            
            result = func(*args)
            result = result.replace([np.inf, -np.inf], np.nan)
            result = result.ffill().bfill().fillna(0)
            
            stack.append(result)
    
    if len(stack) != 1:
        return None

    factor_values = stack[0]

    # å¤„ç†å¼‚å¸¸å€¼
    factor_values = factor_values.replace([np.inf, -np.inf], np.nan)
    factor_values = factor_values.ffill().bfill().fillna(0)

    # æ ¹æ®normalizeå‚æ•°å†³å®šæ˜¯å¦æ ‡å‡†åŒ–
    if normalize:
        rolling_mean = factor_values.rolling(window=rolling_window, min_periods=20).mean()
        rolling_std = factor_values.rolling(window=rolling_window, min_periods=20).std()
        rolling_std = rolling_std.replace(0, np.nan)
        factor_values = (factor_values - rolling_mean) / (rolling_std + 1e-8)
        factor_values = factor_values.fillna(0)
        factor_values = factor_values.clip(-3, 3)  # ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼šclipåˆ°[-3,3]

    return factor_values

def compute_linear_combination(alpha_pool: List[Dict], weights: np.ndarray, data: pd.DataFrame,
                              rolling_window: int = 100) -> pd.Series:
    """
    è®¡ç®—å› å­çš„çº¿æ€§ç»„åˆï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰

    å…³é”®é€»è¾‘ï¼š
    1. å¯¹æ¯ä¸ªå•ç‹¬çš„å› å­è¿›è¡Œæ»šåŠ¨æ ‡å‡†åŒ–ï¼ˆnormalize=Trueï¼‰
    2. åŠ æƒç»„åˆæ ‡å‡†åŒ–åçš„å› å­
    3. ç»„åˆç»“æœä¸å†æ ‡å‡†åŒ–ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰

    å‚æ•°:
    ----
    alpha_pool: list - å› å­æ± 
    weights: np.ndarray - æƒé‡ï¼ˆé’ˆå¯¹æ ‡å‡†åŒ–åçš„å› å­ï¼‰
    data: pd.DataFrame - åŸå§‹æ•°æ®
    rolling_window: int - æ»šåŠ¨æ ‡å‡†åŒ–çª—å£å¤§å°ï¼Œé»˜è®¤100

    è¿”å›:
    ----
    pd.Series - çº¿æ€§ç»„åˆåçš„å› å­å€¼ï¼ˆæœªæ ‡å‡†åŒ–ï¼‰
    """
    if len(alpha_pool) == 0:
        return pd.Series(0, index=data.index)

    # è®¡ç®—æ‰€æœ‰å› å­å€¼ï¼ˆæ¯ä¸ªå› å­éƒ½æ ‡å‡†åŒ–ï¼‰
    factor_values_list = []
    valid_weights = []

    for i, alpha_info in enumerate(alpha_pool):
        tokens = alpha_info['tokens']
        operators = alpha_info['operators']

        # â­ å…³é”®ï¼šè®¡ç®—å› å­å€¼æ—¶è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆnormalize=Trueï¼‰
        factor_values = compute_factor_from_tokens(
            tokens, data, operators,
            rolling_window=rolling_window,
            normalize=True  # å¯¹å•ä¸ªå› å­æ ‡å‡†åŒ–
        )

        if factor_values is not None:
            factor_values_list.append(factor_values)
            valid_weights.append(weights[i])

    if len(factor_values_list) == 0:
        return pd.Series(0, index=data.index)

    # çº¿æ€§ç»„åˆï¼ˆä¸å†æ ‡å‡†åŒ–ï¼Œä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    combination = pd.Series(0, index=data.index)
    for weight, factor_vals in zip(valid_weights, factor_values_list):
        combination += weight * factor_vals

    return combination


def calculate_future_returns(close_prices: pd.Series, periods: int = 10) -> pd.Series:
    future_returns = close_prices.pct_change(periods).shift(-periods)
    return future_returns


def backtest_miner_with_plot(miner, data_split: str = 'test', top_n: int = 10,
                             save_path: str = 'factors_backtest_comparison.png') -> Dict:
    """
    å›æµ‹minerå¯¹è±¡ä¸­çš„æ‰€æœ‰å› å­å’Œç»„åˆï¼Œå¹¶ç”»å›¾å¯¹æ¯”
    
    å‚æ•°:
    ----
    miner: OptimizedSynergisticFactorMiner - è®­ç»ƒå¥½çš„æŒ–æ˜å™¨
    data_split: str - 'train', 'val', æˆ– 'test'
    top_n: int - å›æµ‹å‰Nä¸ªå› å­
    save_path: str - å›¾ç‰‡ä¿å­˜è·¯å¾„
    
    è¿”å›:
    ----
    dict - åŒ…å«individualå’Œcombinedå›æµ‹ç»“æœ
    """
    print(f"\n{'='*80}")
    print(f"ğŸ“Š å®Œæ•´å›æµ‹å¹¶ç”»å›¾ï¼š{data_split.upper()} æ•°æ®é›†")
    print(f"{'='*80}")
    
    # 1. è·å–æ•°æ®
    if data_split == 'train':
        data = miner.train_data
    elif data_split == 'val':
        data = miner.val_data
    else:
        data = miner.test_data
    print(f"æ•°æ®é‡: {len(data)} bars")
    # 2. è·å–å› å­æ± å’Œæƒé‡
    combiner = miner.combination_model
    alpha_pool = combiner.alpha_pool
    if combiner.combiner_type == 'linear':
        weights = np.array(combiner.weights)
    else:
        weights = np.ones(len(alpha_pool)) / len(alpha_pool)
    n_factors = min(top_n, len(alpha_pool))
    print(f"å› å­æ± å¤§å°: {len(alpha_pool)}, å›æµ‹å‰{n_factors}ä¸ª")
    
    # 3. è®¡ç®—ç›®æ ‡
    prediction_horizon = miner.config.prediction_horizon
    target_returns = data['close'].pct_change(prediction_horizon).shift(-prediction_horizon)
    
    # 4. åˆ›å»ºå›æµ‹å™¨
    backtester = Backtester(
        prediction_horizon=prediction_horizon,
        bar_minutes=miner.config.bar_minutes,
        transaction_cost=0.0005,
        max_position=0.1
    )
    
    # 5. å›æµ‹æ¯ä¸ªå•ç‹¬çš„å› å­
    print("\nå›æµ‹å•ä¸ªå› å­...")
    individual_results = {}
    
    for i in range(n_factors):
        alpha_info = alpha_pool[i]
        tokens = alpha_info['tokens']
        operators = alpha_info['operators']

        # â­ è®¡ç®—å› å­å€¼ï¼šä½¿ç”¨åŸå§‹å€¼ï¼ˆnormalize=Falseï¼‰ç”¨äºICè®¡ç®—
        factor_values = compute_factor_from_tokens(
            tokens, data, operators,
            rolling_window=100,
            normalize=False  # ä½¿ç”¨åŸå§‹å› å­å€¼
        )

        if factor_values is None:
            continue

        # å›æµ‹ï¼ˆå†…éƒ¨ä¼šæ ‡å‡†åŒ–ç”¨äºä¿¡å·ç”Ÿæˆï¼Œä½†ICç”¨åŸå§‹å€¼ï¼‰
        result = backtester.backtest_single_factor(
            factor=factor_values,
            returns=target_returns,
            lookback=100,
            name=f"Factor_{i}",
            factor_is_normalized=False  # æ˜ç¡®å‘ŠçŸ¥æ˜¯åŸå§‹å€¼
        )
        
        if 'error' not in result:
            individual_results[f"Factor_{i}"] = result
            tokens_short = '_'.join(tokens[1:min(3, len(tokens)-1)])
            print(f"  Factor {i} ({tokens_short}): IC={result['ic']:.3f}, "
                  f"Sharpe={result['sharpe_ratio']:.2f}, Return={result['total_return']*100:.1f}%")
    
    # 6. å›æµ‹ç»„åˆå› å­
    print("\nå›æµ‹å› å­ç»„åˆ...")
    # ç»„åˆå› å­ = æ ‡å‡†åŒ–å› å­çš„åŠ æƒå’Œï¼ˆç»“æœæœªæ ‡å‡†åŒ–ï¼Œä½†å°ºåº¦åˆç†ï¼‰
    combined_factor = compute_linear_combination(alpha_pool, weights, data, rolling_window=100)

    # å›æµ‹ç»„åˆå› å­
    # æ³¨æ„ï¼šç»„åˆå› å­ä¸æ˜¯æ ‡å‡†åŒ–çš„ï¼Œä½†å°ºåº¦åˆç†ï¼ˆè¾“å…¥æ˜¯[-3,3]èŒƒå›´çš„æ ‡å‡†åŒ–å› å­ï¼‰
    # ICè®¡ç®—ï¼šç”¨åŸå§‹ç»„åˆå€¼
    # ä¿¡å·ç”Ÿæˆï¼šå†…éƒ¨ä¼šæ ‡å‡†åŒ–
    combined_result = backtester.backtest_single_factor(
        factor=combined_factor,
        returns=target_returns,
        lookback=100,
        name="Combined",
        factor_is_normalized=False  # ç»„åˆç»“æœä¸æ˜¯æ ‡å‡†åŒ–çš„ï¼Œéœ€è¦åœ¨ä¿¡å·ç”Ÿæˆæ—¶æ ‡å‡†åŒ–
    )
    
    if 'error' not in combined_result:
        print(f"  ç»„åˆ: IC={combined_result['ic']:.3f}, "
              f"Sharpe={combined_result['sharpe_ratio']:.2f}, "
              f"Return={combined_result['total_return']*100:.1f}%")
    
    # 7. ç”»å›¾
    print(f"\nç»˜åˆ¶æ”¶ç›Šæ›²çº¿å¯¹æ¯”å›¾...")
    plot_backtest_comparison(individual_results, combined_result, data_split, save_path)
    
    # 8. æ‰“å°æ±‡æ€»è¡¨
    print_backtest_summary_table(individual_results, combined_result)
    
    return {
        'individual': individual_results,
        'combined': combined_result
    }


def plot_backtest_comparison(individual_results: Dict, combined_result: Dict,
                            data_split: str, save_path: str):
    """
    ç»˜åˆ¶æ‰€æœ‰å› å­å’Œç»„åˆçš„æ”¶ç›Šæ›²çº¿å¯¹æ¯”å›¾
    """
    # åˆ›å»ºå›¾å½¢
    fig, axes = plt.subplots(2, 2, figsize=(18, 12))
    fig.suptitle(f'Factor Backtest Comparison - {data_split.upper()} Set',
                fontsize=16, fontweight='bold')
    
    # 1. å·¦ä¸Šï¼šæ‰€æœ‰å› å­çš„æƒç›Šæ›²çº¿
    ax1 = axes[0, 0]
    
    # ç”»æ¯ä¸ªå› å­
    for name, result in individual_results.items():
        equity = result['equity_curve']
        ax1.plot(equity.values, label=name, alpha=0.6, linewidth=1)
    
    # ç”»ç»„åˆï¼ˆåŠ ç²—ï¼‰
    if combined_result and 'error' not in combined_result:
        combined_equity = combined_result['equity_curve']
        ax1.plot(combined_equity.values, label='Combined (Weighted)',
                color='red', linewidth=3, alpha=0.9)
    
    ax1.axhline(y=1, color='black', linestyle='--', alpha=0.3)
    ax1.set_title('Equity Curves: All Factors vs Combined', fontweight='bold')
    ax1.set_xlabel('Trading Periods')
    ax1.set_ylabel('Cumulative Return')
    ax1.legend(loc='upper left', fontsize=8, ncol=2)
    ax1.grid(True, alpha=0.3)
    
    # 2. å³ä¸Šï¼šåªæ˜¾ç¤ºTop5å› å­ + ç»„åˆ
    ax2 = axes[0, 1]
    
    # æŒ‰Sharpeæ’åºï¼Œå–Top5
    sorted_factors = sorted(individual_results.items(),
                          key=lambda x: x[1]['sharpe_ratio'],
                          reverse=True)[:5]
    
    for name, result in sorted_factors:
        equity = result['equity_curve']
        ax2.plot(equity.values, label=f"{name} (SR={result['sharpe_ratio']:.1f})",
                linewidth=2, alpha=0.8)
    
    # ç”»ç»„åˆ
    if combined_result and 'error' not in combined_result:
        combined_equity = combined_result['equity_curve']
        ax2.plot(combined_equity.values,
                label=f"Combined (SR={combined_result['sharpe_ratio']:.1f})",
                color='red', linewidth=3, alpha=0.9)
    
    ax2.axhline(y=1, color='black', linestyle='--', alpha=0.3)
    ax2.set_title('Top 5 Factors by Sharpe + Combined', fontweight='bold')
    ax2.set_xlabel('Trading Periods')
    ax2.set_ylabel('Cumulative Return')
    ax2.legend(loc='best', fontsize=9)
    ax2.grid(True, alpha=0.3)
    
    # 3. å·¦ä¸‹ï¼šSharpeæ¯”ç‡å¯¹æ¯”
    ax3 = axes[1, 0]
    
    names = list(individual_results.keys())
    sharpes = [individual_results[n]['sharpe_ratio'] for n in names]
    
    # æ·»åŠ ç»„åˆ
    if combined_result and 'error' not in combined_result:
        names.append('Combined')
        sharpes.append(combined_result['sharpe_ratio'])
    
    colors = ['steelblue'] * len(individual_results)
    if combined_result and 'error' not in combined_result:
        colors.append('red')
    
    bars = ax3.barh(range(len(names)), sharpes, color=colors, alpha=0.7)
    ax3.set_yticks(range(len(names)))
    ax3.set_yticklabels(names, fontsize=8)
    ax3.set_xlabel('Sharpe Ratio')
    ax3.set_title('Sharpe Ratio Comparison', fontweight='bold')
    ax3.axvline(x=0, color='black', linestyle='-', linewidth=0.5)
    ax3.grid(True, alpha=0.3, axis='x')
    
    # æ ‡æ³¨æ•°å€¼
    for i, (bar, val) in enumerate(zip(bars, sharpes)):
        ax3.text(val, i, f' {val:.2f}', va='center', fontsize=8)
    
    # 4. å³ä¸‹ï¼šæ€»æ”¶ç›Šå’ŒICå¯¹æ¯”
    ax4 = axes[1, 1]
    
    returns = [individual_results[n]['total_return'] * 100 for n in individual_results.keys()]
    ics = [individual_results[n]['ic'] for n in individual_results.keys()]
    
    # æ·»åŠ ç»„åˆ
    if combined_result and 'error' not in combined_result:
        returns.append(combined_result['total_return'] * 100)
        ics.append(combined_result['ic'])
    
    # æ•£ç‚¹å›¾
    scatter = ax4.scatter(ics[:-1] if len(ics) > len(individual_results) else ics,
                         returns[:-1] if len(returns) > len(individual_results) else returns,
                         s=100, alpha=0.6, c='steelblue', label='Individual Factors')
    
    # ç»„åˆç‚¹ï¼ˆçº¢è‰²åŠ å¤§ï¼‰
    if combined_result and 'error' not in combined_result:
        ax4.scatter([ics[-1]], [returns[-1]], s=300, alpha=0.9,
                   c='red', marker='*', label='Combined', edgecolors='darkred', linewidths=2)
    
    ax4.axhline(y=0, color='gray', linestyle='--', alpha=0.3)
    ax4.axvline(x=0, color='gray', linestyle='--', alpha=0.3)
    ax4.set_xlabel('IC')
    ax4.set_ylabel('Total Return (%)')
    ax4.set_title('Return vs IC', fontweight='bold')
    ax4.legend(loc='best')
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… å›¾ç‰‡å·²ä¿å­˜: {save_path}")
    plt.show()


def print_backtest_summary_table(individual_results: Dict, combined_result: Dict):
    """
    æ‰“å°å›æµ‹ç»“æœæ±‡æ€»è¡¨æ ¼
    """
    print(f"\n{'='*80}")
    print("ğŸ“Š å›æµ‹ç»“æœæ±‡æ€»è¡¨")
    print(f"{'='*80}")
    
    # è¡¨å¤´
    print(f"{'Factor':<15} {'IC':>8} {'Sharpe':>8} {'Return(%)':>12} {'MaxDD(%)':>12} {'WinRate(%)':>12}")
    print(f"{'-'*80}")
    
    # å•ä¸ªå› å­
    for name, result in individual_results.items():
        print(f"{name:<15} {result['ic']:>8.4f} {result['sharpe_ratio']:>8.2f} "
              f"{result['total_return']*100:>12.2f} {result['max_drawdown']*100:>12.2f} "
              f"{result['win_rate']*100:>12.1f}")
    
    # åˆ†éš”çº¿
    print(f"{'-'*80}")
    
    # ç»„åˆ
    if combined_result and 'error' not in combined_result:
        print(f"{'COMBINED':<15} {combined_result['ic']:>8.4f} "
              f"{combined_result['sharpe_ratio']:>8.2f} "
              f"{combined_result['total_return']*100:>12.2f} "
              f"{combined_result['max_drawdown']*100:>12.2f} "
              f"{combined_result['win_rate']*100:>12.1f}")
    
    print(f"{'='*80}\n")


class MinerBacktester:
    """
    Minerå›æµ‹å™¨ - ä¸€é”®å›æµ‹minerå¯¹è±¡

    ä½¿ç”¨æ–¹æ³•ï¼š
    --------
    # æ–¹æ³•1: ä»è®­ç»ƒå¥½çš„minerå¯¹è±¡å›æµ‹
    backtester = MinerBacktester(miner_linear)
    backtester.run('test')  # è‡ªåŠ¨å›æµ‹å¹¶ç”»å›¾

    # æ–¹æ³•2: ä»ä¿å­˜çš„JSONæ–‡ä»¶åŠ è½½æœ€ä½³å› å­ç»„åˆå›æµ‹
    backtester = MinerBacktester.from_best_solution('best_solutions.json', data, operators)
    backtester.run('test')
    """

    def __init__(self, miner):
        """
        å‚æ•°:
        ----
        miner: OptimizedSynergisticFactorMiner - è®­ç»ƒå¥½çš„æŒ–æ˜å™¨å¯¹è±¡
        """
        self.miner = miner
        self.results = {}
        print(f"âœ… å›æµ‹å™¨å·²åˆ›å»ºï¼Œå› å­æ± å¤§å°: {len(miner.combination_model.alpha_pool)}")
    
    def run(self, data_split: str = 'test', top_n: int = 10, 
            save_path: str = None, show_plot: bool = True):
        """
        è¿è¡Œå›æµ‹å¹¶ç”ŸæˆæŠ¥å‘Š
        
        å‚æ•°:
        ----
        data_split: 'train', 'val', æˆ– 'test'
        top_n: å›æµ‹å‰Nä¸ªå› å­
        save_path: å›¾ç‰‡ä¿å­˜è·¯å¾„ï¼ˆé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰
        show_plot: æ˜¯å¦æ˜¾ç¤ºå›¾è¡¨
        
        è¿”å›:
        ----
        dict - å›æµ‹ç»“æœ
        """
        if save_path is None:
            save_path = f'{data_split}_factors_backtest.png'
        
        # è°ƒç”¨å›æµ‹å‡½æ•°
        results = backtest_miner_with_plot(
            self.miner, 
            data_split=data_split, 
            top_n=top_n,
            save_path=save_path
        )
        
        self.results[data_split] = results
        return results
    
    def run_all(self, top_n: int = 10):
        """
        åœ¨è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†ä¸Šéƒ½è¿è¡Œå›æµ‹
        """
        print(f"\n{'='*80}")
        print("ğŸ“Š åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šè¿è¡Œå›æµ‹")
        print(f"{'='*80}\n")
        
        # è®­ç»ƒé›†
        self.run('train', top_n=top_n, save_path='train_backtest.png')
        
        # éªŒè¯é›†
        self.run('val', top_n=top_n, save_path='val_backtest.png')
        
        # æµ‹è¯•é›†
        self.run('test', top_n=top_n, save_path='test_backtest.png')
        
        # æ‰“å°å¯¹æ¯”
        self._print_comparison()
        
        return self.results
    
    def _print_comparison(self):
        """æ‰“å°ä¸‰æ•°æ®é›†å¯¹æ¯”"""
        print(f"\n{'='*80}")
        print("ğŸ“Š ä¸‰æ•°æ®é›†ç»„åˆå› å­å¯¹æ¯”")
        print(f"{'='*80}")
        print(f"{'æŒ‡æ ‡':<20} {'è®­ç»ƒé›†':<15} {'éªŒè¯é›†':<15} {'æµ‹è¯•é›†':<15}")
        print(f"{'-'*80}")
        
        metrics_to_show = [
            ('IC', 'ic', '{:.4f}'),
            ('Sharpeæ¯”ç‡', 'sharpe_ratio', '{:.2f}'),
            ('æ€»æ”¶ç›Š(%)', 'total_return', '{:.2f}', 100),
            ('å¹´åŒ–æ”¶ç›Š(%)', 'annual_return', '{:.2f}', 100),
            ('æœ€å¤§å›æ’¤(%)', 'max_drawdown', '{:.2f}', 100),
            ('èƒœç‡(%)', 'win_rate', '{:.2f}', 100),
        ]
        
        for metric_display, metric_key, fmt, *scale in metrics_to_show:
            multiplier = scale[0] if scale else 1
            
            train_val = self.results.get('train', {}).get('combined', {}).get(metric_key, 0) * multiplier
            val_val = self.results.get('val', {}).get('combined', {}).get(metric_key, 0) * multiplier
            test_val = self.results.get('test', {}).get('combined', {}).get(metric_key, 0) * multiplier
            
            print(f"{metric_display:<20} {fmt.format(train_val):<15} {fmt.format(val_val):<15} {fmt.format(test_val):<15}")
        
        print(f"{'='*80}\n")
    
    def get_combined_result(self, data_split: str = 'test'):
        """è·å–ç»„åˆå› å­çš„å›æµ‹ç»“æœ"""
        if data_split not in self.results:
            print(f"âŒ {data_split} æ•°æ®é›†å°šæœªå›æµ‹ï¼Œè¯·å…ˆè¿è¡Œ run('{data_split}')")
            return None
        return self.results[data_split]['combined']

    def get_individual_results(self, data_split: str = 'test'):
        """è·å–å•ä¸ªå› å­çš„å›æµ‹ç»“æœ"""
        if data_split not in self.results:
            print(f"âŒ {data_split} æ•°æ®é›†å°šæœªå›æµ‹ï¼Œè¯·å…ˆè¿è¡Œ run('{data_split}')")
            return None
        return self.results[data_split]['individual']

    @classmethod
    def from_best_solution(cls, json_path: str, data: pd.DataFrame,
                          operators: Dict, config=None,
                          snapshot_index: int = -1):
        """
        ä»ä¿å­˜çš„best_solutions.jsonæ–‡ä»¶åŠ è½½æœ€ä½³å› å­ç»„åˆå¹¶åˆ›å»ºå›æµ‹å™¨

        å‚æ•°:
        ----
        json_path: str - best_solutions.jsonæ–‡ä»¶è·¯å¾„
        data: pd.DataFrame - å®Œæ•´æ•°æ®ï¼ˆåŒ…å«train/val/testï¼‰
        operators: Dict - ç®—å­å­—å…¸ï¼ˆéœ€è¦ä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
        config: TrainingConfig - é…ç½®å¯¹è±¡ï¼ˆå¯é€‰ï¼Œç”¨äºæ•°æ®åˆ†å‰²ï¼‰
        snapshot_index: int - ä½¿ç”¨ç¬¬å‡ ä¸ªå¿«ç…§ï¼ˆé»˜è®¤-1è¡¨ç¤ºæœ€åä¸€ä¸ªï¼Œå³æœ€ä½³ï¼‰

        è¿”å›:
        ----
        MinerBacktester - å›æµ‹å™¨å¯¹è±¡

        ç¤ºä¾‹:
        ----
        >>> # åŠ è½½æ•°æ®å’Œç®—å­
        >>> data = pd.read_csv('your_data.csv')
        >>> from operators import TimeSeriesOperators
        >>> ts_ops = TimeSeriesOperators()
        >>> operators = {...}  # æ„å»ºç®—å­å­—å…¸
        >>>
        >>> # ä»JSONåŠ è½½å¹¶å›æµ‹
        >>> backtester = MinerBacktester.from_best_solution(
        ...     'best_solutions.json', data, operators
        ... )
        >>> results = backtester.run('test')
        """
        import json
        from pathlib import Path
        from config import TrainingConfig

        # è¯»å–JSONæ–‡ä»¶
        json_file = Path(json_path)
        if not json_file.exists():
            raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {json_path}")

        with json_file.open('r', encoding='utf-8') as f:
            data_loaded = json.load(f)

        snapshots = data_loaded.get('snapshots', [])
        if not snapshots:
            raise ValueError(f"âŒ JSONæ–‡ä»¶ä¸­æ²¡æœ‰å¿«ç…§æ•°æ®")

        # é€‰æ‹©å¿«ç…§
        if snapshot_index < 0:
            snapshot_index = len(snapshots) + snapshot_index

        if snapshot_index < 0 or snapshot_index >= len(snapshots):
            raise IndexError(f"âŒ å¿«ç…§ç´¢å¼• {snapshot_index} è¶…å‡ºèŒƒå›´ [0, {len(snapshots)-1}]")

        snapshot = snapshots[snapshot_index]

        print(f"")
        print(f"{'='*70}")
        print(f"ğŸ“‚ ä»JSONåŠ è½½æœ€ä½³å› å­ç»„åˆ")
        print(f"{'='*70}")
        print(f"æ–‡ä»¶: {json_path}")
        print(f"å¿«ç…§: #{snapshot_index} / {len(snapshots)}")
        print(f"è®­ç»ƒæ—¶é—´: Iteration {snapshot['iteration']}")
        print(f"éªŒè¯åˆ†æ•°: {snapshot['val_score']:.4f}")
        print(f"å› å­æ•°é‡: {snapshot['pool_size']}")
        print(f"{'='*70}")

        # åˆ›å»ºä¸€ä¸ªä¼ªminerå¯¹è±¡ï¼ˆåªåŒ…å«å¿…è¦çš„å±æ€§ï¼‰
        class PseudoMiner:
            """ä¼ªminerå¯¹è±¡ - ç”¨äºä»JSONåŠ è½½å› å­æ± """
            def __init__(self, data, operators, snapshot, config):
                self.config = config or TrainingConfig()

                # æ•°æ®åˆ†å‰²
                train_size = int(len(data) * self.config.train_ratio)
                val_size = int(len(data) * self.config.val_ratio)

                self.train_data = data.iloc[:train_size].copy()
                self.val_data = data.iloc[train_size:train_size+val_size].copy()
                self.test_data = data.iloc[train_size+val_size:].copy()

                # å¡«å……ç¼ºå¤±å€¼
                self.train_data = self.train_data.ffill().bfill().fillna(0)
                self.val_data = self.val_data.ffill().bfill().fillna(0)
                self.test_data = self.test_data.ffill().bfill().fillna(0)

                # åˆ›å»ºä¼ªç»„åˆæ¨¡å‹
                class PseudoCombinationModel:
                    def __init__(self, snapshot, operators):
                        self.alpha_pool = []
                        self.weights = []
                        self.combiner_type = 'linear'  # å‡è®¾æ˜¯linear

                        # ä»snapshotåŠ è½½å› å­æ± 
                        for factor_info in snapshot['factors']:
                            alpha_info = {
                                'tokens': factor_info['tokens'],
                                'timestamp': factor_info.get('timestamp', 0),
                                'operators': operators
                            }
                            self.alpha_pool.append(alpha_info)

                            # åŠ è½½æƒé‡
                            weight = factor_info.get('weight')
                            if weight is not None:
                                self.weights.append(weight)

                        print(f"âœ… åŠ è½½äº† {len(self.alpha_pool)} ä¸ªå› å­")
                        if self.weights:
                            print(f"âœ… åŠ è½½äº† {len(self.weights)} ä¸ªæƒé‡")
                            print(f"   æƒé‡èŒƒå›´: [{min(self.weights):.4f}, {max(self.weights):.4f}]")

                self.combination_model = PseudoCombinationModel(snapshot, operators)

        pseudo_miner = PseudoMiner(data, operators, snapshot, config)

        # åˆ›å»ºå›æµ‹å™¨
        backtester = cls(pseudo_miner)
        print(f"âœ… å›æµ‹å™¨åˆ›å»ºæˆåŠŸ\n")

        return backtester


def load_and_backtest_from_json(json_path: str, data: pd.DataFrame,
                                operators: Dict, config=None,
                                data_split: str = 'test',
                                snapshot_index: int = -1,
                                save_path: str = None,
                                top_n: int = 10) -> Dict:
    backtester = MinerBacktester.from_best_solution(
        json_path, data, operators, config, snapshot_index
    )

    if save_path is None:
        save_path = f'{data_split}_backtest_from_json.png'

    results = backtester.run(data_split=data_split, save_path=save_path, top_n=top_n)

    return results


def demonstrate_backtest_workflow():
    """æ¼”ç¤ºå®Œæ•´çš„å›æµ‹æµç¨‹"""
    print("="*80)
    print("ğŸš€ å› å­å›æµ‹æ¼”ç¤º")
    print("="*80)
    
    # 1. åˆ›å»ºç¤ºä¾‹æ•°æ®
    print("\nğŸ“Š Step 1: åˆ›å»ºç¤ºä¾‹æ•°æ®")
    n_bars = 2000
    returns = np.random.randn(n_bars) * 0.015
    prices = 10000 * np.exp(np.cumsum(returns))
    
    data = pd.DataFrame({
        'close': prices,
        'returns': np.concatenate([[0], np.diff(np.log(prices))])
    })
    
    # åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    split_idx = int(len(data) * 0.7)
    train_data = data.iloc[:split_idx].copy()
    test_data = data.iloc[split_idx:].copy()
    
    print(f"   è®­ç»ƒé›†: {len(train_data)} bars")
    print(f"   æµ‹è¯•é›†: {len(test_data)} bars")
    
    # 2. åˆ›å»ºç¤ºä¾‹å› å­
    print("\nğŸ”§ Step 2: åˆ›å»ºç¤ºä¾‹å› å­")
    
    # å› å­1: åŠ¨é‡å› å­
    train_data['factor1'] = train_data['close'].pct_change(5)
    test_data['factor1'] = test_data['close'].pct_change(5)
    
    # å› å­2: åè½¬å› å­
    train_data['factor2'] = -train_data['close'].pct_change(20)
    test_data['factor2'] = -test_data['close'].pct_change(20)
    
    # å› å­3: æ³¢åŠ¨ç‡å› å­
    train_data['factor3'] = train_data['returns'].rolling(10).std()
    test_data['factor3'] = test_data['returns'].rolling(10).std()
    
    print("   å› å­å·²åˆ›å»ºï¼ŒICå°†åœ¨å›æµ‹æ—¶è‡ªåŠ¨è®¡ç®—")
    
    # 3. åˆ›å»ºå›æµ‹å™¨å’Œåˆ†æå™¨
    print("\nğŸ“ˆ Step 3: å›æµ‹")
    backtester = Backtester(
        prediction_horizon=10,
        transaction_cost=0.0005,
        signal_threshold=0.1  # é™ä½æ¢æ‰‹
    )
    
    analyzer = FactorAnalyzer()
    
    # å›æµ‹æ¯ä¸ªå› å­ï¼ˆä¸éœ€è¦é¢„å…ˆè®¡ç®—ICï¼‰
    factors_info = [
        ('Factor1_Momentum', 'factor1'),
        ('Factor2_Reversal', 'factor2'),
        ('Factor3_Volatility', 'factor3'),
    ]
    
    for name, col in factors_info:
        result = backtester.backtest_factor_on_both_sets(
            train_data[col],
            test_data[col],
            train_data['returns'],
            test_data['returns'],
            name=name
        )
        analyzer.add_backtest_result(result)
    
    # 4. å¯è§†åŒ–
    print("\nğŸ“Š Step 4: ç”Ÿæˆå¯è§†åŒ–")
    
    # ç»˜åˆ¶å¯¹æ¯”å›¾
    analyzer.plot_all_factors_comparison('demo_factors_comparison.png')
    
    # ç»˜åˆ¶ç›¸å…³æ€§çƒ­åŠ›å›¾
    train_factors = {
        'Factor1': train_data['factor1'],
        'Factor2': train_data['factor2'],
        'Factor3': train_data['factor3'],
    }
    
    test_factors = {
        'Factor1': test_data['factor1'],
        'Factor2': test_data['factor2'],
        'Factor3': test_data['factor3'],
    }
    
    analyzer.plot_factors_correlation_heatmap(
        train_factors, test_factors, 'demo_factors_correlation.png'
    )
    
    # 5. æ‰“å°æ±‡æ€»
    analyzer.print_summary()
    
    print("\n" + "="*80)
    print("âœ… æ¼”ç¤ºå®Œæˆï¼")
    print("="*80)


if __name__ == "__main__":
    demonstrate_backtest_workflow()