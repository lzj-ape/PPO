{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ÁéØÂ¢ÉËÆæÁΩÆÂÆåÊàêÔºåÊ®°ÂùóÂØºÂÖ•ÊàêÂäüÔºÅ\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Ëß£ÂÜ≥Ë∑ØÂæÑÈóÆÈ¢òÔºöÂ∞ÜÂΩìÂâçÁõÆÂΩïÂèäÂ≠êÁõÆÂΩïÂä†ÂÖ• Python ÊêúÁ¥¢Ë∑ØÂæÑ\n",
    "# ---------------------------------------------------------\n",
    "# Ëé∑ÂèñÂΩìÂâç notebook ÊâÄÂú®ÁöÑÁªùÂØπË∑ØÂæÑ\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Â∞ÜÊ†πÁõÆÂΩïÂèäÂÖ∂Â≠êÊ®°ÂùóÁõÆÂΩïÂä†ÂÖ• sys.path\n",
    "# ËøôÊ†∑‰Ω†Â∞±ÂèØ‰ª•Áõ¥Êé• import config, PPO, factor Á≠â\n",
    "sys.path.append(current_dir)\n",
    "sys.path.append(os.path.join(current_dir, 'config'))\n",
    "sys.path.append(os.path.join(current_dir, 'factor'))\n",
    "sys.path.append(os.path.join(current_dir, 'model'))\n",
    "sys.path.append(os.path.join(current_dir, 'utils'))\n",
    "sys.path.append(os.path.join(current_dir, 'backtest'))\n",
    "sys.path.append(os.path.join(current_dir, 'PPO'))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. ÂØºÂÖ•‰Ω†ÁöÑÈ°πÁõÆÊ®°Âùó\n",
    "# ---------------------------------------------------------\n",
    "from config import TrainingConfig\n",
    "from miner_new import OptimizedSynergisticFactorMiner\n",
    "from backtest import MinerBacktester\n",
    "from utils import setup_logging\n",
    "\n",
    "# ËÆæÁΩÆÊó•ÂøóÊòæÁ§∫\n",
    "logger = setup_logging(level=logging.INFO)\n",
    "\n",
    "print(\"‚úÖ ÁéØÂ¢ÉËÆæÁΩÆÂÆåÊàêÔºåÊ®°ÂùóÂØºÂÖ•ÊàêÂäüÔºÅ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Ê≠£Âú®ÁîüÊàê/Âä†ËΩΩÊï∞ÊçÆ...\n",
      "‚úÖ Êï∞ÊçÆÈ™åËØÅÈÄöËøá\n",
      "‚úÖ Êï∞ÊçÆÂä†ËΩΩÂÆåÊàêÔºåÂÖ± 50000 Êù° K Á∫ø\n",
      "\n",
      "Êï∞ÊçÆÈ¢ÑËßà:\n",
      "                            open         high          low        close  \\\n",
      "2024-06-04 18:45:00  8027.740253  8027.795383  8021.697114  8024.287790   \n",
      "2024-06-04 19:00:00  8020.396092  8037.271120  8013.764292  8019.504172   \n",
      "2024-06-04 19:15:00  8019.618024  8022.761720  8018.977301  8022.585450   \n",
      "2024-06-04 19:30:00  8024.479648  8027.465507  8020.015944  8020.702462   \n",
      "2024-06-04 19:45:00  8020.969934  8023.650405  8016.480666  8021.491986   \n",
      "\n",
      "                          volume  \n",
      "2024-06-04 18:45:00  5556.001022  \n",
      "2024-06-04 19:00:00  8782.726206  \n",
      "2024-06-04 19:15:00  5189.070753  \n",
      "2024-06-04 19:30:00  5095.042530  \n",
      "2024-06-04 19:45:00  9896.427236  \n",
      "\n",
      "Êï∞ÊçÆÁªüËÆ°:\n",
      "               open          high           low         close        volume\n",
      "count  5.000000e+04  5.000000e+04  5.000000e+04  5.000000e+04  50000.000000\n",
      "mean   1.740843e+06  1.743351e+06  1.738321e+06  1.740840e+06   7646.531166\n",
      "std    2.488327e+06  2.491426e+06  2.485206e+06  2.488313e+06   1448.065923\n",
      "min    7.949484e+03  7.967645e+03  7.930040e+03  7.949240e+03   5002.930696\n",
      "25%    3.046711e+04  3.051789e+04  3.042579e+04  3.047470e+04   6396.156946\n",
      "50%    2.959088e+05  2.965751e+05  2.954060e+05  2.959506e+05   7657.996187\n",
      "75%    2.836677e+06  2.842155e+06  2.831961e+06  2.836329e+06   8897.918707\n",
      "max    9.178571e+06  9.197883e+06  9.163480e+06  9.174533e+06  10403.227859\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# ÂáÜÂ§áÊï∞ÊçÆÔºö‰ΩøÁî®ÊîπËøõÁöÑÊï∞ÊçÆÁîüÊàêÂô®\n",
    "# Â¶ÇÊûú‰Ω†ÊúâÁúüÂÆûÊï∞ÊçÆÔºåËØ∑ÊõøÊç¢‰∏∫ pd.read_csv('your_data.csv')\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "from data_generator import generate_realistic_market_data, validate_data\n",
    "\n",
    "# 1. Âä†ËΩΩÊï∞ÊçÆ\n",
    "print(\"‚è≥ Ê≠£Âú®ÁîüÊàê/Âä†ËΩΩÊï∞ÊçÆ...\")\n",
    "\n",
    "# üî• ‰ΩøÁî®ÊîπËøõÁöÑÊï∞ÊçÆÁîüÊàêÂô® - ÈÅøÂÖçÊú™Êù•ÂáΩÊï∞ÔºåÊõ¥Ë¥¥ËøëÁúüÂÆûÂ∏ÇÂú∫\n",
    "data = generate_realistic_market_data(\n",
    "    n_rows=50000,\n",
    "    start_date='2023-01-01',\n",
    "    freq='15min',\n",
    "    seed=42,\n",
    "    initial_price=10000.0\n",
    ")\n",
    "\n",
    "# Â¶ÇÊûú‰ΩøÁî®ÁúüÂÆûÊï∞ÊçÆÔºåÂèñÊ∂à‰∏ãÈù¢ËøôË°åÁöÑÊ≥®ÈáäÔºö\n",
    "# data = pd.read_csv('your_data.csv', parse_dates=['timestamp'], index_col='timestamp')\n",
    "\n",
    "# 2. È™åËØÅÊï∞ÊçÆË¥®Èáè\n",
    "validation = validate_data(data)\n",
    "if validation['valid']:\n",
    "    print(\"‚úÖ Êï∞ÊçÆÈ™åËØÅÈÄöËøá\")\n",
    "else:\n",
    "    print(\"‚ùå Êï∞ÊçÆÈ™åËØÅÂ§±Ë¥•:\")\n",
    "    for issue in validation['issues']:\n",
    "        print(f\"  - {issue}\")\n",
    "\n",
    "print(f\"‚úÖ Êï∞ÊçÆÂä†ËΩΩÂÆåÊàêÔºåÂÖ± {len(data)} Êù° K Á∫ø\")\n",
    "print(\"\\nÊï∞ÊçÆÈ¢ÑËßà:\")\n",
    "print(data.tail())\n",
    "print(\"\\nÊï∞ÊçÆÁªüËÆ°:\")\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è ÈÖçÁΩÆÂ∑≤Âä†ËΩΩ: linear Ê®°Âºè\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# ÈÖçÁΩÆËÆ≠ÁªÉÂèÇÊï∞\n",
    "# ---------------------------------------------------------\n",
    "config = TrainingConfig()\n",
    "\n",
    "# --- Ê†πÊçÆ Notebook ËøêË°åÁéØÂ¢ÉËøõË°åÂæÆË∞É ---\n",
    "config.ppo_epochs = 4           # PPO ÊØèÊ¨°Êõ¥Êñ∞ÁöÑËΩÆÊï∞\n",
    "config.batch_size = 64          # ÊâπÊ¨°Â§ßÂ∞è\n",
    "config.lr_actor = 3e-4         # Â≠¶‰π†Áéá\n",
    "config.combiner_type = 'linear' # 'linear' (Á∫øÊÄßÁªÑÂêà) Êàñ 'lstm' (Á•ûÁªèÁΩëÁªúÁªÑÂêà)\n",
    "\n",
    "# ÊåñÊéòÂèÇÊï∞\n",
    "BATCH_SIZE = 16                 # ÊØèÊ¨°ÁîüÊàêÁöÑÂÖ¨ÂºèÊï∞Èáè\n",
    "TRAIN_INTERVAL = 100             # ÊØèÂ§öÂ∞ëÊ≠•ËÆ≠ÁªÉ‰∏ÄÊ¨° PPO\n",
    "\n",
    "print(f\"‚öôÔ∏è ÈÖçÁΩÆÂ∑≤Âä†ËΩΩ: {config.combiner_type} Ê®°Âºè\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-21 10:30:09,563 - INFO - Calculating target variable: future_return\n",
      "2025-11-21 10:30:09,565 - INFO - Target calculated: 10-period forward percentage returns\n",
      "2025-11-21 10:30:09,566 - INFO -   Mean: 0.000062\n",
      "2025-11-21 10:30:09,566 - INFO -   Std: 0.014640\n",
      "2025-11-21 10:30:09,571 - INFO - Data split with Purging Gap=10:\n",
      "2025-11-21 10:30:09,571 - INFO -   Train: 30000 bars\n",
      "2025-11-21 10:30:09,572 - INFO -   Val: 10000 bars\n",
      "2025-11-21 10:30:09,572 - INFO -   Test: 9980 bars\n",
      "2025-11-21 10:30:10,344 - INFO - Synergy Evaluator initialized (Target: Incremental Sharpe)\n",
      "2025-11-21 10:30:10,345 - INFO - Advanced Reward Calculator initialized:\n",
      "2025-11-21 10:30:10,345 - INFO -   - Incremental Sharpe: False\n",
      "2025-11-21 10:30:10,345 - INFO -   - Penalty: True\n",
      "2025-11-21 10:30:10,345 - INFO -   - Rolling Stability: False\n",
      "2025-11-21 10:30:10,345 - INFO - ‚úÖ AdvancedRewardCalculator enabled (penalty mode)\n",
      "2025-11-21 10:30:10,345 - INFO - FactorMinerCore initialized:\n",
      "2025-11-21 10:30:10,346 - INFO -   Device: cpu\n",
      "2025-11-21 10:30:10,346 - INFO -   Combiner type: linear\n",
      "2025-11-21 10:30:10,346 - INFO -   Features: 5\n",
      "2025-11-21 10:30:10,346 - INFO -   Vocab size: 55\n",
      "2025-11-21 10:30:10,346 - INFO - OptimizedSynergisticFactorMiner initialized (using refactored modules)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ ÊåñÊéòÂô®ÂàùÂßãÂåñÂÆåÊàêÔºåÂáÜÂ§áÂºÄÂßãÊåñÊéò...\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# ÂàùÂßãÂåñÂõ†Â≠êÊåñÊéòÂô®\n",
    "# ---------------------------------------------------------\n",
    "miner = OptimizedSynergisticFactorMiner(\n",
    "    data=data,\n",
    "    target_col='future_return', # ÁõÆÊ†áÂàóÂêç\n",
    "    config=config,\n",
    "    max_factors=100,             # Âõ†Â≠êÊ±†ÊúÄÂ§ß‰øùÁïôÊï∞Èáè\n",
    "    max_expr_len=50             # Âõ†Â≠êÂÖ¨ÂºèÊúÄÂ§ßÈïøÂ∫¶\n",
    ")\n",
    "print(\"ü§ñ ÊåñÊéòÂô®ÂàùÂßãÂåñÂÆåÊàêÔºåÂáÜÂ§áÂºÄÂßãÊåñÊéò...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-21 10:30:10,351 - INFO - Starting factor mining:\n",
      "2025-11-21 10:30:10,351 - INFO -   Iterations: 1000\n",
      "2025-11-21 10:30:10,351 - INFO -   Batch size: 16\n",
      "2025-11-21 10:30:10,352 - INFO -   Early stop patience: 1000\n",
      "2025-11-21 10:30:10,606 - INFO - üîÑ PPO Update #1 at iteration 0\n",
      "2025-11-21 10:30:10,606 - INFO -   Buffer size: 597\n",
      "2025-11-21 10:30:12,620 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:30:12,622 - INFO -     Policy Loss: 0.130083\n",
      "2025-11-21 10:30:12,622 - INFO -     Value Loss: 0.245832\n",
      "2025-11-21 10:30:12,622 - INFO -     Entropy Loss: -0.069476\n",
      "2025-11-21 10:30:12,622 - INFO -     Advantage - Mean: 0.2315, Std: 0.7181\n",
      "2025-11-21 10:30:12,623 - INFO -     Value - Mean: -0.3613, Std: 0.7767\n",
      "2025-11-21 10:30:12,623 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:30:12,624 - INFO - ‚ú® New best VAL score: 3.1841 at iteration 0\n",
      "2025-11-21 10:30:13,739 - INFO - üîÑ PPO Update #2 at iteration 3\n",
      "2025-11-21 10:30:13,749 - INFO -   Buffer size: 2071\n",
      "2025-11-21 10:30:20,535 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:30:20,539 - INFO -     Policy Loss: 0.086797\n",
      "2025-11-21 10:30:20,539 - INFO -     Value Loss: 0.038258\n",
      "2025-11-21 10:30:20,539 - INFO -     Entropy Loss: -0.068828\n",
      "2025-11-21 10:30:20,540 - INFO -     Advantage - Mean: -0.0286, Std: 0.3003\n",
      "2025-11-21 10:30:20,540 - INFO -     Value - Mean: 0.0518, Std: 0.3010\n",
      "2025-11-21 10:30:20,540 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:30:20,794 - INFO - Iteration 5/1000\n",
      "2025-11-21 10:30:20,800 - INFO -   Avg Reward: -1.1499\n",
      "2025-11-21 10:30:20,801 - INFO -   Best VAL: 3.1841\n",
      "2025-11-21 10:30:20,802 - INFO -   Pool Size: 1\n",
      "2025-11-21 10:30:20,802 - INFO -   Current Best Factor Combination:\n",
      "2025-11-21 10:30:20,803 - INFO -     Train Score: 4.8132\n",
      "2025-11-21 10:30:20,804 - INFO -     Val Score: 3.1841\n",
      "2025-11-21 10:30:20,805 - INFO -     Top 3 Factors by Weight:\n",
      "2025-11-21 10:30:20,806 - INFO -       #1: weight=0.0000, incremental_contribution=4.8132\n",
      "2025-11-21 10:30:20,806 - INFO -           expression: <BEG> close <SEP>\n",
      "2025-11-21 10:30:21,467 - INFO - üîÑ PPO Update #3 at iteration 6\n",
      "2025-11-21 10:30:21,468 - INFO -   Buffer size: 2258\n",
      "2025-11-21 10:30:28,777 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:30:28,777 - INFO -     Policy Loss: 0.045051\n",
      "2025-11-21 10:30:28,777 - INFO -     Value Loss: 0.008341\n",
      "2025-11-21 10:30:28,778 - INFO -     Entropy Loss: -0.068414\n",
      "2025-11-21 10:30:28,778 - INFO -     Advantage - Mean: -0.0257, Std: 0.1192\n",
      "2025-11-21 10:30:28,778 - INFO -     Value - Mean: 0.0152, Std: 0.1180\n",
      "2025-11-21 10:30:28,778 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:30:29,516 - INFO - üîÑ PPO Update #4 at iteration 9\n",
      "2025-11-21 10:30:29,516 - INFO -   Buffer size: 2352\n",
      "2025-11-21 10:30:37,138 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:30:37,139 - INFO -     Policy Loss: 0.027582\n",
      "2025-11-21 10:30:37,140 - INFO -     Value Loss: 0.003733\n",
      "2025-11-21 10:30:37,140 - INFO -     Entropy Loss: -0.067888\n",
      "2025-11-21 10:30:37,140 - INFO -     Advantage - Mean: -0.0234, Std: 0.0709\n",
      "2025-11-21 10:30:37,140 - INFO -     Value - Mean: -0.0077, Std: 0.0693\n",
      "2025-11-21 10:30:37,140 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:30:37,141 - INFO - Iteration 10/1000\n",
      "2025-11-21 10:30:37,141 - INFO -   Avg Reward: -0.3595\n",
      "2025-11-21 10:30:37,141 - INFO -   Best VAL: 3.1841\n",
      "2025-11-21 10:30:37,142 - INFO -   Pool Size: 1\n",
      "2025-11-21 10:30:37,142 - INFO -   Current Best Factor Combination:\n",
      "2025-11-21 10:30:37,142 - INFO -     Train Score: 4.8132\n",
      "2025-11-21 10:30:37,142 - INFO -     Val Score: 3.1841\n",
      "2025-11-21 10:30:37,143 - INFO -     Top 3 Factors by Weight:\n",
      "2025-11-21 10:30:37,143 - INFO -       #1: weight=0.0000, incremental_contribution=4.8132\n",
      "2025-11-21 10:30:37,143 - INFO -           expression: <BEG> close <SEP>\n",
      "2025-11-21 10:30:37,870 - INFO - üîÑ PPO Update #5 at iteration 12\n",
      "2025-11-21 10:30:37,871 - INFO -   Buffer size: 2352\n",
      "2025-11-21 10:30:45,116 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:30:45,116 - INFO -     Policy Loss: 0.020774\n",
      "2025-11-21 10:30:45,116 - INFO -     Value Loss: 0.002355\n",
      "2025-11-21 10:30:45,116 - INFO -     Entropy Loss: -0.069923\n",
      "2025-11-21 10:30:45,117 - INFO -     Advantage - Mean: -0.0126, Std: 0.0530\n",
      "2025-11-21 10:30:45,117 - INFO -     Value - Mean: -0.0294, Std: 0.0508\n",
      "2025-11-21 10:30:45,117 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:30:45,764 - INFO - Iteration 15/1000\n",
      "2025-11-21 10:30:45,765 - INFO -   Avg Reward: -0.8882\n",
      "2025-11-21 10:30:45,765 - INFO -   Best VAL: 3.1841\n",
      "2025-11-21 10:30:45,767 - INFO -   Pool Size: 1\n",
      "2025-11-21 10:30:45,767 - INFO -   Current Best Factor Combination:\n",
      "2025-11-21 10:30:45,768 - INFO -     Train Score: 4.8132\n",
      "2025-11-21 10:30:45,769 - INFO -     Val Score: 3.1841\n",
      "2025-11-21 10:30:45,770 - INFO -     Top 3 Factors by Weight:\n",
      "2025-11-21 10:30:45,770 - INFO -       #1: weight=0.0000, incremental_contribution=4.8132\n",
      "2025-11-21 10:30:45,771 - INFO -           expression: <BEG> close <SEP>\n",
      "2025-11-21 10:30:46,173 - INFO - üîÑ PPO Update #6 at iteration 15\n",
      "2025-11-21 10:30:46,173 - INFO -   Buffer size: 2072\n",
      "2025-11-21 10:30:52,353 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:30:52,353 - INFO -     Policy Loss: 0.012128\n",
      "2025-11-21 10:30:52,354 - INFO -     Value Loss: 0.004832\n",
      "2025-11-21 10:30:52,354 - INFO -     Entropy Loss: -0.071166\n",
      "2025-11-21 10:30:52,354 - INFO -     Advantage - Mean: -0.0062, Std: 0.0743\n",
      "2025-11-21 10:30:52,354 - INFO -     Value - Mean: -0.0504, Std: 0.0420\n",
      "2025-11-21 10:30:52,354 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:30:53,119 - INFO - üîÑ PPO Update #7 at iteration 18\n",
      "2025-11-21 10:30:53,120 - INFO -   Buffer size: 2305\n",
      "2025-11-21 10:31:00,136 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:31:00,136 - INFO -     Policy Loss: 0.020505\n",
      "2025-11-21 10:31:00,137 - INFO -     Value Loss: 0.001946\n",
      "2025-11-21 10:31:00,137 - INFO -     Entropy Loss: -0.071260\n",
      "2025-11-21 10:31:00,137 - INFO -     Advantage - Mean: 0.0074, Std: 0.0455\n",
      "2025-11-21 10:31:00,137 - INFO -     Value - Mean: -0.0676, Std: 0.0401\n",
      "2025-11-21 10:31:00,137 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:31:00,378 - INFO - Iteration 20/1000\n",
      "2025-11-21 10:31:00,379 - INFO -   Avg Reward: -0.6246\n",
      "2025-11-21 10:31:00,379 - INFO -   Best VAL: 3.1841\n",
      "2025-11-21 10:31:00,379 - INFO -   Pool Size: 1\n",
      "2025-11-21 10:31:00,379 - INFO -   Current Best Factor Combination:\n",
      "2025-11-21 10:31:00,379 - INFO -     Train Score: 4.8132\n",
      "2025-11-21 10:31:00,380 - INFO -     Val Score: 3.1841\n",
      "2025-11-21 10:31:00,380 - INFO -     Top 3 Factors by Weight:\n",
      "2025-11-21 10:31:00,380 - INFO -       #1: weight=0.0000, incremental_contribution=4.8132\n",
      "2025-11-21 10:31:00,380 - INFO -           expression: <BEG> close <SEP>\n",
      "2025-11-21 10:31:01,024 - INFO - üîÑ PPO Update #8 at iteration 21\n",
      "2025-11-21 10:31:01,025 - INFO -   Buffer size: 2305\n",
      "2025-11-21 10:31:07,872 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:31:07,873 - INFO -     Policy Loss: 0.009163\n",
      "2025-11-21 10:31:07,873 - INFO -     Value Loss: 0.001662\n",
      "2025-11-21 10:31:07,873 - INFO -     Entropy Loss: -0.072677\n",
      "2025-11-21 10:31:07,873 - INFO -     Advantage - Mean: -0.0087, Std: 0.0403\n",
      "2025-11-21 10:31:07,874 - INFO -     Value - Mean: -0.0321, Std: 0.0356\n",
      "2025-11-21 10:31:07,874 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:31:08,601 - INFO - üîÑ PPO Update #9 at iteration 24\n",
      "2025-11-21 10:31:08,603 - INFO -   Buffer size: 2305\n",
      "2025-11-21 10:31:15,636 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:31:15,636 - INFO -     Policy Loss: 0.012847\n",
      "2025-11-21 10:31:15,636 - INFO -     Value Loss: 0.001608\n",
      "2025-11-21 10:31:15,637 - INFO -     Entropy Loss: -0.074966\n",
      "2025-11-21 10:31:15,637 - INFO -     Advantage - Mean: 0.0054, Std: 0.0392\n",
      "2025-11-21 10:31:15,637 - INFO -     Value - Mean: -0.0643, Std: 0.0326\n",
      "2025-11-21 10:31:15,637 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:31:15,637 - INFO - Iteration 25/1000\n",
      "2025-11-21 10:31:15,638 - INFO -   Avg Reward: -0.3554\n",
      "2025-11-21 10:31:15,638 - INFO -   Best VAL: 3.1841\n",
      "2025-11-21 10:31:15,638 - INFO -   Pool Size: 1\n",
      "2025-11-21 10:31:15,638 - INFO -   Current Best Factor Combination:\n",
      "2025-11-21 10:31:15,639 - INFO -     Train Score: 4.8132\n",
      "2025-11-21 10:31:15,639 - INFO -     Val Score: 3.1841\n",
      "2025-11-21 10:31:15,639 - INFO -     Top 3 Factors by Weight:\n",
      "2025-11-21 10:31:15,639 - INFO -       #1: weight=0.0000, incremental_contribution=4.8132\n",
      "2025-11-21 10:31:15,639 - INFO -           expression: <BEG> close <SEP>\n",
      "2025-11-21 10:31:16,426 - INFO - üîÑ PPO Update #10 at iteration 27\n",
      "2025-11-21 10:31:16,426 - INFO -   Buffer size: 2352\n",
      "2025-11-21 10:31:23,449 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:31:23,450 - INFO -     Policy Loss: 0.010806\n",
      "2025-11-21 10:31:23,450 - INFO -     Value Loss: 0.000822\n",
      "2025-11-21 10:31:23,450 - INFO -     Entropy Loss: -0.075743\n",
      "2025-11-21 10:31:23,450 - INFO -     Advantage - Mean: -0.0040, Std: 0.0293\n",
      "2025-11-21 10:31:23,451 - INFO -     Value - Mean: -0.0424, Std: 0.0290\n",
      "2025-11-21 10:31:23,451 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:31:23,932 - INFO - Iteration 30/1000\n",
      "2025-11-21 10:31:23,932 - INFO -   Avg Reward: -0.2237\n",
      "2025-11-21 10:31:23,933 - INFO -   Best VAL: 3.1841\n",
      "2025-11-21 10:31:23,933 - INFO -   Pool Size: 1\n",
      "2025-11-21 10:31:23,933 - INFO -   Current Best Factor Combination:\n",
      "2025-11-21 10:31:23,933 - INFO -     Train Score: 4.8132\n",
      "2025-11-21 10:31:23,933 - INFO -     Val Score: 3.1841\n",
      "2025-11-21 10:31:23,934 - INFO -     Top 3 Factors by Weight:\n",
      "2025-11-21 10:31:23,934 - INFO -       #1: weight=0.0000, incremental_contribution=4.8132\n",
      "2025-11-21 10:31:23,934 - INFO -           expression: <BEG> close <SEP>\n",
      "2025-11-21 10:31:24,187 - INFO - üîÑ PPO Update #11 at iteration 30\n",
      "2025-11-21 10:31:24,187 - INFO -   Buffer size: 2307\n",
      "2025-11-21 10:31:31,226 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:31:31,226 - INFO -     Policy Loss: 0.010040\n",
      "2025-11-21 10:31:31,227 - INFO -     Value Loss: 0.001423\n",
      "2025-11-21 10:31:31,227 - INFO -     Entropy Loss: -0.076000\n",
      "2025-11-21 10:31:31,227 - INFO -     Advantage - Mean: -0.0055, Std: 0.0375\n",
      "2025-11-21 10:31:31,227 - INFO -     Value - Mean: -0.0438, Std: 0.0244\n",
      "2025-11-21 10:31:31,227 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:31:31,980 - INFO - üîÑ PPO Update #12 at iteration 33\n",
      "2025-11-21 10:31:31,981 - INFO -   Buffer size: 2352\n",
      "2025-11-21 10:31:39,030 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:31:39,031 - INFO -     Policy Loss: 0.005620\n",
      "2025-11-21 10:31:39,031 - INFO -     Value Loss: 0.000608\n",
      "2025-11-21 10:31:39,031 - INFO -     Entropy Loss: -0.076504\n",
      "2025-11-21 10:31:39,031 - INFO -     Advantage - Mean: -0.0055, Std: 0.0261\n",
      "2025-11-21 10:31:39,031 - INFO -     Value - Mean: -0.0375, Std: 0.0277\n",
      "2025-11-21 10:31:39,031 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:31:39,276 - INFO - Iteration 35/1000\n",
      "2025-11-21 10:31:39,276 - INFO -   Avg Reward: -0.2317\n",
      "2025-11-21 10:31:39,277 - INFO -   Best VAL: 3.1841\n",
      "2025-11-21 10:31:39,277 - INFO -   Pool Size: 1\n",
      "2025-11-21 10:31:39,277 - INFO -   Current Best Factor Combination:\n",
      "2025-11-21 10:31:39,277 - INFO -     Train Score: 4.8132\n",
      "2025-11-21 10:31:39,278 - INFO -     Val Score: 3.1841\n",
      "2025-11-21 10:31:39,278 - INFO -     Top 3 Factors by Weight:\n",
      "2025-11-21 10:31:39,278 - INFO -       #1: weight=0.0000, incremental_contribution=4.8132\n",
      "2025-11-21 10:31:39,278 - INFO -           expression: <BEG> close <SEP>\n",
      "2025-11-21 10:31:39,758 - INFO - üîÑ PPO Update #13 at iteration 36\n",
      "2025-11-21 10:31:39,758 - INFO -   Buffer size: 2352\n",
      "2025-11-21 10:31:46,799 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:31:46,799 - INFO -     Policy Loss: 0.006850\n",
      "2025-11-21 10:31:46,799 - INFO -     Value Loss: 0.000585\n",
      "2025-11-21 10:31:46,799 - INFO -     Entropy Loss: -0.077074\n",
      "2025-11-21 10:31:46,799 - INFO -     Advantage - Mean: -0.0020, Std: 0.0249\n",
      "2025-11-21 10:31:46,800 - INFO -     Value - Mean: -0.0487, Std: 0.0225\n",
      "2025-11-21 10:31:46,800 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:31:47,691 - INFO - üîÑ PPO Update #14 at iteration 39\n",
      "2025-11-21 10:31:47,691 - INFO -   Buffer size: 2267\n",
      "2025-11-21 10:31:54,518 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:31:54,518 - INFO -     Policy Loss: 0.006735\n",
      "2025-11-21 10:31:54,518 - INFO -     Value Loss: 0.002092\n",
      "2025-11-21 10:31:54,518 - INFO -     Entropy Loss: -0.077397\n",
      "2025-11-21 10:31:54,519 - INFO -     Advantage - Mean: -0.0090, Std: 0.0454\n",
      "2025-11-21 10:31:54,519 - INFO -     Value - Mean: -0.0372, Std: 0.0233\n",
      "2025-11-21 10:31:54,519 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:31:54,519 - INFO - Iteration 40/1000\n",
      "2025-11-21 10:31:54,520 - INFO -   Avg Reward: -0.3143\n",
      "2025-11-21 10:31:54,520 - INFO -   Best VAL: 3.1841\n",
      "2025-11-21 10:31:54,520 - INFO -   Pool Size: 1\n",
      "2025-11-21 10:31:54,520 - INFO -   Current Best Factor Combination:\n",
      "2025-11-21 10:31:54,521 - INFO -     Train Score: 4.8132\n",
      "2025-11-21 10:31:54,521 - INFO -     Val Score: 3.1841\n",
      "2025-11-21 10:31:54,521 - INFO -     Top 3 Factors by Weight:\n",
      "2025-11-21 10:31:54,521 - INFO -       #1: weight=0.0000, incremental_contribution=4.8132\n",
      "2025-11-21 10:31:54,521 - INFO -           expression: <BEG> close <SEP>\n",
      "2025-11-21 10:31:55,240 - INFO - üîÑ PPO Update #15 at iteration 42\n",
      "2025-11-21 10:31:55,240 - INFO -   Buffer size: 2352\n",
      "2025-11-21 10:32:02,451 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:32:02,451 - INFO -     Policy Loss: 0.001504\n",
      "2025-11-21 10:32:02,451 - INFO -     Value Loss: 0.000433\n",
      "2025-11-21 10:32:02,452 - INFO -     Entropy Loss: -0.077303\n",
      "2025-11-21 10:32:02,452 - INFO -     Advantage - Mean: -0.0044, Std: 0.0217\n",
      "2025-11-21 10:32:02,452 - INFO -     Value - Mean: -0.0400, Std: 0.0244\n",
      "2025-11-21 10:32:02,452 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:32:02,933 - INFO - Iteration 45/1000\n",
      "2025-11-21 10:32:02,933 - INFO -   Avg Reward: -0.1846\n",
      "2025-11-21 10:32:02,933 - INFO -   Best VAL: 3.1841\n",
      "2025-11-21 10:32:02,934 - INFO -   Pool Size: 1\n",
      "2025-11-21 10:32:02,934 - INFO -   Current Best Factor Combination:\n",
      "2025-11-21 10:32:02,934 - INFO -     Train Score: 4.8132\n",
      "2025-11-21 10:32:02,934 - INFO -     Val Score: 3.1841\n",
      "2025-11-21 10:32:02,934 - INFO -     Top 3 Factors by Weight:\n",
      "2025-11-21 10:32:02,935 - INFO -       #1: weight=0.0000, incremental_contribution=4.8132\n",
      "2025-11-21 10:32:02,935 - INFO -           expression: <BEG> close <SEP>\n",
      "2025-11-21 10:32:03,176 - INFO - üîÑ PPO Update #16 at iteration 45\n",
      "2025-11-21 10:32:03,176 - INFO -   Buffer size: 2352\n",
      "2025-11-21 10:32:10,148 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:32:10,148 - INFO -     Policy Loss: 0.008510\n",
      "2025-11-21 10:32:10,148 - INFO -     Value Loss: 0.000436\n",
      "2025-11-21 10:32:10,148 - INFO -     Entropy Loss: -0.077631\n",
      "2025-11-21 10:32:10,148 - INFO -     Advantage - Mean: -0.0032, Std: 0.0210\n",
      "2025-11-21 10:32:10,149 - INFO -     Value - Mean: -0.0443, Std: 0.0223\n",
      "2025-11-21 10:32:10,149 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:32:10,856 - INFO - üîÑ PPO Update #17 at iteration 48\n",
      "2025-11-21 10:32:10,857 - INFO -   Buffer size: 2258\n",
      "2025-11-21 10:32:17,798 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:32:17,798 - INFO -     Policy Loss: 0.003113\n",
      "2025-11-21 10:32:17,798 - INFO -     Value Loss: 0.001363\n",
      "2025-11-21 10:32:17,799 - INFO -     Entropy Loss: -0.077510\n",
      "2025-11-21 10:32:17,799 - INFO -     Advantage - Mean: -0.0042, Std: 0.0360\n",
      "2025-11-21 10:32:17,799 - INFO -     Value - Mean: -0.0442, Std: 0.0216\n",
      "2025-11-21 10:32:17,799 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:32:18,044 - INFO - Iteration 50/1000\n",
      "2025-11-21 10:32:18,044 - INFO -   Avg Reward: -0.3594\n",
      "2025-11-21 10:32:18,044 - INFO -   Best VAL: 3.1841\n",
      "2025-11-21 10:32:18,044 - INFO -   Pool Size: 1\n",
      "2025-11-21 10:32:18,044 - INFO -   Current Best Factor Combination:\n",
      "2025-11-21 10:32:18,045 - INFO -     Train Score: 4.8132\n",
      "2025-11-21 10:32:18,045 - INFO -     Val Score: 3.1841\n",
      "2025-11-21 10:32:18,045 - INFO -     Top 3 Factors by Weight:\n",
      "2025-11-21 10:32:18,045 - INFO -       #1: weight=0.0000, incremental_contribution=4.8132\n",
      "2025-11-21 10:32:18,045 - INFO -           expression: <BEG> close <SEP>\n",
      "2025-11-21 10:32:18,518 - INFO - üîÑ PPO Update #18 at iteration 51\n",
      "2025-11-21 10:32:18,519 - INFO -   Buffer size: 2352\n",
      "2025-11-21 10:32:25,517 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:32:25,517 - INFO -     Policy Loss: 0.004288\n",
      "2025-11-21 10:32:25,517 - INFO -     Value Loss: 0.000365\n",
      "2025-11-21 10:32:25,517 - INFO -     Entropy Loss: -0.077514\n",
      "2025-11-21 10:32:25,517 - INFO -     Advantage - Mean: -0.0027, Std: 0.0204\n",
      "2025-11-21 10:32:25,518 - INFO -     Value - Mean: -0.0445, Std: 0.0223\n",
      "2025-11-21 10:32:25,518 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:32:26,241 - INFO - üîÑ PPO Update #19 at iteration 54\n",
      "2025-11-21 10:32:26,242 - INFO -   Buffer size: 2305\n",
      "2025-11-21 10:32:33,319 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:32:33,319 - INFO -     Policy Loss: 0.005779\n",
      "2025-11-21 10:32:33,319 - INFO -     Value Loss: 0.000822\n",
      "2025-11-21 10:32:33,319 - INFO -     Entropy Loss: -0.077968\n",
      "2025-11-21 10:32:33,319 - INFO -     Advantage - Mean: -0.0015, Std: 0.0294\n",
      "2025-11-21 10:32:33,320 - INFO -     Value - Mean: -0.0506, Std: 0.0189\n",
      "2025-11-21 10:32:33,320 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:32:33,320 - INFO - Iteration 55/1000\n",
      "2025-11-21 10:32:33,320 - INFO -   Avg Reward: -0.2297\n",
      "2025-11-21 10:32:33,321 - INFO -   Best VAL: 3.1841\n",
      "2025-11-21 10:32:33,321 - INFO -   Pool Size: 1\n",
      "2025-11-21 10:32:33,321 - INFO -   Current Best Factor Combination:\n",
      "2025-11-21 10:32:33,321 - INFO -     Train Score: 4.8132\n",
      "2025-11-21 10:32:33,321 - INFO -     Val Score: 3.1841\n",
      "2025-11-21 10:32:33,322 - INFO -     Top 3 Factors by Weight:\n",
      "2025-11-21 10:32:33,322 - INFO -       #1: weight=0.0000, incremental_contribution=4.8132\n",
      "2025-11-21 10:32:33,322 - INFO -           expression: <BEG> close <SEP>\n",
      "2025-11-21 10:32:34,105 - INFO - üîÑ PPO Update #20 at iteration 57\n",
      "2025-11-21 10:32:34,106 - INFO -   Buffer size: 2352\n",
      "2025-11-21 10:32:41,222 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:32:41,223 - INFO -     Policy Loss: 0.000173\n",
      "2025-11-21 10:32:41,224 - INFO -     Value Loss: 0.000310\n",
      "2025-11-21 10:32:41,224 - INFO -     Entropy Loss: -0.078164\n",
      "2025-11-21 10:32:41,224 - INFO -     Advantage - Mean: 0.0032, Std: 0.0182\n",
      "2025-11-21 10:32:41,224 - INFO -     Value - Mean: -0.0574, Std: 0.0217\n",
      "2025-11-21 10:32:41,224 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:32:41,712 - INFO - Iteration 60/1000\n",
      "2025-11-21 10:32:41,713 - INFO -   Avg Reward: -0.2297\n",
      "2025-11-21 10:32:41,713 - INFO -   Best VAL: 3.1841\n",
      "2025-11-21 10:32:41,713 - INFO -   Pool Size: 1\n",
      "2025-11-21 10:32:41,713 - INFO -   Current Best Factor Combination:\n",
      "2025-11-21 10:32:41,713 - INFO -     Train Score: 4.8132\n",
      "2025-11-21 10:32:41,714 - INFO -     Val Score: 3.1841\n",
      "2025-11-21 10:32:41,714 - INFO -     Top 3 Factors by Weight:\n",
      "2025-11-21 10:32:41,714 - INFO -       #1: weight=0.0000, incremental_contribution=4.8132\n",
      "2025-11-21 10:32:41,714 - INFO -           expression: <BEG> close <SEP>\n",
      "2025-11-21 10:32:41,960 - INFO - üîÑ PPO Update #21 at iteration 60\n",
      "2025-11-21 10:32:41,961 - INFO -   Buffer size: 2352\n",
      "2025-11-21 10:32:49,005 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:32:49,005 - INFO -     Policy Loss: 0.001335\n",
      "2025-11-21 10:32:49,005 - INFO -     Value Loss: 0.000281\n",
      "2025-11-21 10:32:49,005 - INFO -     Entropy Loss: -0.078385\n",
      "2025-11-21 10:32:49,006 - INFO -     Advantage - Mean: 0.0017, Std: 0.0162\n",
      "2025-11-21 10:32:49,006 - INFO -     Value - Mean: -0.0532, Std: 0.0243\n",
      "2025-11-21 10:32:49,006 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:32:49,916 - INFO - üîÑ PPO Update #22 at iteration 63\n",
      "2025-11-21 10:32:49,919 - INFO -   Buffer size: 2315\n",
      "2025-11-21 10:32:56,838 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:32:56,838 - INFO -     Policy Loss: -0.004928\n",
      "2025-11-21 10:32:56,839 - INFO -     Value Loss: 0.001354\n",
      "2025-11-21 10:32:56,839 - INFO -     Entropy Loss: -0.078622\n",
      "2025-11-21 10:32:56,839 - INFO -     Advantage - Mean: -0.0024, Std: 0.0369\n",
      "2025-11-21 10:32:56,839 - INFO -     Value - Mean: -0.0491, Std: 0.0233\n",
      "2025-11-21 10:32:56,839 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:32:57,086 - INFO - Iteration 65/1000\n",
      "2025-11-21 10:32:57,087 - INFO -   Avg Reward: -0.2093\n",
      "2025-11-21 10:32:57,087 - INFO -   Best VAL: 3.1841\n",
      "2025-11-21 10:32:57,087 - INFO -   Pool Size: 1\n",
      "2025-11-21 10:32:57,087 - INFO -   Current Best Factor Combination:\n",
      "2025-11-21 10:32:57,087 - INFO -     Train Score: 4.8132\n",
      "2025-11-21 10:32:57,088 - INFO -     Val Score: 3.1841\n",
      "2025-11-21 10:32:57,088 - INFO -     Top 3 Factors by Weight:\n",
      "2025-11-21 10:32:57,088 - INFO -       #1: weight=0.0000, incremental_contribution=4.8132\n",
      "2025-11-21 10:32:57,088 - INFO -           expression: <BEG> close <SEP>\n",
      "2025-11-21 10:32:57,569 - INFO - üîÑ PPO Update #23 at iteration 66\n",
      "2025-11-21 10:32:57,569 - INFO -   Buffer size: 2352\n",
      "2025-11-21 10:33:04,565 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:33:04,566 - INFO -     Policy Loss: 0.002731\n",
      "2025-11-21 10:33:04,566 - INFO -     Value Loss: 0.000259\n",
      "2025-11-21 10:33:04,566 - INFO -     Entropy Loss: -0.078382\n",
      "2025-11-21 10:33:04,566 - INFO -     Advantage - Mean: 0.0025, Std: 0.0166\n",
      "2025-11-21 10:33:04,567 - INFO -     Value - Mean: -0.0550, Std: 0.0224\n",
      "2025-11-21 10:33:04,567 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:33:05,287 - INFO - üîÑ PPO Update #24 at iteration 69\n",
      "2025-11-21 10:33:05,287 - INFO -   Buffer size: 2352\n",
      "2025-11-21 10:33:12,417 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:33:12,418 - INFO -     Policy Loss: 0.002647\n",
      "2025-11-21 10:33:12,418 - INFO -     Value Loss: 0.000224\n",
      "2025-11-21 10:33:12,418 - INFO -     Entropy Loss: -0.078250\n",
      "2025-11-21 10:33:12,418 - INFO -     Advantage - Mean: -0.0017, Std: 0.0147\n",
      "2025-11-21 10:33:12,418 - INFO -     Value - Mean: -0.0466, Std: 0.0202\n",
      "2025-11-21 10:33:12,419 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:33:12,419 - INFO - Iteration 70/1000\n",
      "2025-11-21 10:33:12,419 - INFO -   Avg Reward: -0.1000\n",
      "2025-11-21 10:33:12,419 - INFO -   Best VAL: 3.1841\n",
      "2025-11-21 10:33:12,419 - INFO -   Pool Size: 1\n",
      "2025-11-21 10:33:12,419 - INFO -   Current Best Factor Combination:\n",
      "2025-11-21 10:33:12,420 - INFO -     Train Score: 4.8132\n",
      "2025-11-21 10:33:12,420 - INFO -     Val Score: 3.1841\n",
      "2025-11-21 10:33:12,420 - INFO -     Top 3 Factors by Weight:\n",
      "2025-11-21 10:33:12,420 - INFO -       #1: weight=0.0000, incremental_contribution=4.8132\n",
      "2025-11-21 10:33:12,421 - INFO -           expression: <BEG> close <SEP>\n",
      "2025-11-21 10:33:13,135 - INFO - üîÑ PPO Update #25 at iteration 72\n",
      "2025-11-21 10:33:13,136 - INFO -   Buffer size: 2352\n",
      "2025-11-21 10:33:20,165 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:33:20,166 - INFO -     Policy Loss: -0.000821\n",
      "2025-11-21 10:33:20,166 - INFO -     Value Loss: 0.000215\n",
      "2025-11-21 10:33:20,167 - INFO -     Entropy Loss: -0.078037\n",
      "2025-11-21 10:33:20,167 - INFO -     Advantage - Mean: -0.0039, Std: 0.0137\n",
      "2025-11-21 10:33:20,167 - INFO -     Value - Mean: -0.0421, Std: 0.0189\n",
      "2025-11-21 10:33:20,167 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:33:20,829 - INFO - Iteration 75/1000\n",
      "2025-11-21 10:33:20,830 - INFO -   Avg Reward: -0.1290\n",
      "2025-11-21 10:33:20,830 - INFO -   Best VAL: 3.1841\n",
      "2025-11-21 10:33:20,830 - INFO -   Pool Size: 1\n",
      "2025-11-21 10:33:20,830 - INFO -   Current Best Factor Combination:\n",
      "2025-11-21 10:33:20,830 - INFO -     Train Score: 4.8132\n",
      "2025-11-21 10:33:20,831 - INFO -     Val Score: 3.1841\n",
      "2025-11-21 10:33:20,831 - INFO -     Top 3 Factors by Weight:\n",
      "2025-11-21 10:33:20,831 - INFO -       #1: weight=0.0000, incremental_contribution=4.8132\n",
      "2025-11-21 10:33:20,831 - INFO -           expression: <BEG> close <SEP>\n",
      "2025-11-21 10:33:21,073 - INFO - üîÑ PPO Update #26 at iteration 75\n",
      "2025-11-21 10:33:21,074 - INFO -   Buffer size: 2305\n",
      "2025-11-21 10:33:27,979 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:33:27,979 - INFO -     Policy Loss: -0.003288\n",
      "2025-11-21 10:33:27,979 - INFO -     Value Loss: 0.000710\n",
      "2025-11-21 10:33:27,980 - INFO -     Entropy Loss: -0.078241\n",
      "2025-11-21 10:33:27,980 - INFO -     Advantage - Mean: -0.0031, Std: 0.0265\n",
      "2025-11-21 10:33:27,980 - INFO -     Value - Mean: -0.0455, Std: 0.0191\n",
      "2025-11-21 10:33:27,980 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:33:28,854 - INFO - üîÑ PPO Update #27 at iteration 78\n",
      "2025-11-21 10:33:28,855 - INFO -   Buffer size: 2310\n",
      "2025-11-21 10:33:35,863 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:33:35,863 - INFO -     Policy Loss: 0.002017\n",
      "2025-11-21 10:33:35,863 - INFO -     Value Loss: 0.001132\n",
      "2025-11-21 10:33:35,863 - INFO -     Entropy Loss: -0.077725\n",
      "2025-11-21 10:33:35,864 - INFO -     Advantage - Mean: 0.0015, Std: 0.0341\n",
      "2025-11-21 10:33:35,864 - INFO -     Value - Mean: -0.0590, Std: 0.0163\n",
      "2025-11-21 10:33:35,864 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:33:36,109 - INFO - Iteration 80/1000\n",
      "2025-11-21 10:33:36,110 - INFO -   Avg Reward: -0.1619\n",
      "2025-11-21 10:33:36,110 - INFO -   Best VAL: 3.1841\n",
      "2025-11-21 10:33:36,110 - INFO -   Pool Size: 1\n",
      "2025-11-21 10:33:36,110 - INFO -   Current Best Factor Combination:\n",
      "2025-11-21 10:33:36,110 - INFO -     Train Score: 4.8132\n",
      "2025-11-21 10:33:36,110 - INFO -     Val Score: 3.1841\n",
      "2025-11-21 10:33:36,111 - INFO -     Top 3 Factors by Weight:\n",
      "2025-11-21 10:33:36,111 - INFO -       #1: weight=0.0000, incremental_contribution=4.8132\n",
      "2025-11-21 10:33:36,111 - INFO -           expression: <BEG> close <SEP>\n",
      "2025-11-21 10:33:36,589 - INFO - üîÑ PPO Update #28 at iteration 81\n",
      "2025-11-21 10:33:36,589 - INFO -   Buffer size: 2352\n",
      "2025-11-21 10:33:43,556 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:33:43,556 - INFO -     Policy Loss: -0.002995\n",
      "2025-11-21 10:33:43,556 - INFO -     Value Loss: 0.000225\n",
      "2025-11-21 10:33:43,557 - INFO -     Entropy Loss: -0.077428\n",
      "2025-11-21 10:33:43,557 - INFO -     Advantage - Mean: 0.0008, Std: 0.0152\n",
      "2025-11-21 10:33:43,557 - INFO -     Value - Mean: -0.0511, Std: 0.0232\n",
      "2025-11-21 10:33:43,557 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:33:44,340 - INFO - üîÑ PPO Update #29 at iteration 84\n",
      "2025-11-21 10:33:44,341 - INFO -   Buffer size: 2352\n",
      "2025-11-21 10:33:51,344 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:33:51,344 - INFO -     Policy Loss: -0.003444\n",
      "2025-11-21 10:33:51,344 - INFO -     Value Loss: 0.000227\n",
      "2025-11-21 10:33:51,344 - INFO -     Entropy Loss: -0.077557\n",
      "2025-11-21 10:33:51,345 - INFO -     Advantage - Mean: 0.0016, Std: 0.0159\n",
      "2025-11-21 10:33:51,345 - INFO -     Value - Mean: -0.0541, Std: 0.0211\n",
      "2025-11-21 10:33:51,345 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:33:51,345 - INFO - Iteration 85/1000\n",
      "2025-11-21 10:33:51,345 - INFO -   Avg Reward: -0.1000\n",
      "2025-11-21 10:33:51,346 - INFO -   Best VAL: 3.1841\n",
      "2025-11-21 10:33:51,346 - INFO -   Pool Size: 1\n",
      "2025-11-21 10:33:51,346 - INFO -   Current Best Factor Combination:\n",
      "2025-11-21 10:33:51,346 - INFO -     Train Score: 4.8132\n",
      "2025-11-21 10:33:51,346 - INFO -     Val Score: 3.1841\n",
      "2025-11-21 10:33:51,347 - INFO -     Top 3 Factors by Weight:\n",
      "2025-11-21 10:33:51,347 - INFO -       #1: weight=0.0000, incremental_contribution=4.8132\n",
      "2025-11-21 10:33:51,347 - INFO -           expression: <BEG> close <SEP>\n",
      "2025-11-21 10:33:52,070 - INFO - üîÑ PPO Update #30 at iteration 87\n",
      "2025-11-21 10:33:52,071 - INFO -   Buffer size: 2352\n",
      "2025-11-21 10:33:59,039 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:33:59,039 - INFO -     Policy Loss: -0.000053\n",
      "2025-11-21 10:33:59,039 - INFO -     Value Loss: 0.000197\n",
      "2025-11-21 10:33:59,040 - INFO -     Entropy Loss: -0.077596\n",
      "2025-11-21 10:33:59,040 - INFO -     Advantage - Mean: 0.0004, Std: 0.0145\n",
      "2025-11-21 10:33:59,040 - INFO -     Value - Mean: -0.0524, Std: 0.0181\n",
      "2025-11-21 10:33:59,040 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:33:59,525 - INFO - Iteration 90/1000\n",
      "2025-11-21 10:33:59,525 - INFO -   Avg Reward: -0.1000\n",
      "2025-11-21 10:33:59,526 - INFO -   Best VAL: 3.1841\n",
      "2025-11-21 10:33:59,526 - INFO -   Pool Size: 1\n",
      "2025-11-21 10:33:59,526 - INFO -   Current Best Factor Combination:\n",
      "2025-11-21 10:33:59,526 - INFO -     Train Score: 4.8132\n",
      "2025-11-21 10:33:59,526 - INFO -     Val Score: 3.1841\n",
      "2025-11-21 10:33:59,526 - INFO -     Top 3 Factors by Weight:\n",
      "2025-11-21 10:33:59,527 - INFO -       #1: weight=0.0000, incremental_contribution=4.8132\n",
      "2025-11-21 10:33:59,527 - INFO -           expression: <BEG> close <SEP>\n",
      "2025-11-21 10:33:59,824 - INFO - üîÑ PPO Update #31 at iteration 90\n",
      "2025-11-21 10:33:59,825 - INFO -   Buffer size: 2352\n",
      "2025-11-21 10:34:07,001 - INFO -   PPO Training Stats:\n",
      "2025-11-21 10:34:07,002 - INFO -     Policy Loss: -0.005807\n",
      "2025-11-21 10:34:07,002 - INFO -     Value Loss: 0.000158\n",
      "2025-11-21 10:34:07,003 - INFO -     Entropy Loss: -0.078323\n",
      "2025-11-21 10:34:07,003 - INFO -     Advantage - Mean: -0.0003, Std: 0.0131\n",
      "2025-11-21 10:34:07,003 - INFO -     Value - Mean: -0.0487, Std: 0.0218\n",
      "2025-11-21 10:34:07,004 - INFO -     Learning Rate: 0.000300\n",
      "2025-11-21 10:34:07,792 - INFO - üîÑ PPO Update #32 at iteration 93\n",
      "2025-11-21 10:34:07,792 - INFO -   Buffer size: 2352\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      5\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 7\u001b[0m results \u001b[38;5;241m=\u001b[39m miner\u001b[38;5;241m.\u001b[39mmine_factors(\n\u001b[1;32m      8\u001b[0m     n_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m      9\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     10\u001b[0m     train_interval\u001b[38;5;241m=\u001b[39mTRAIN_INTERVAL,\n\u001b[1;32m     11\u001b[0m     print_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,           \u001b[38;5;66;03m# ÊØè5Ê¨°ÊâìÂç∞‰∏ÄÊ¨°ËøõÂ∫¶\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     early_stop_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m      \u001b[38;5;66;03m# Êó©ÂÅúËÄêÂøÉÂÄº\u001b[39;00m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müéâ ÊåñÊéòÂÆåÊàêÔºÅËÄóÊó∂: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Áßí\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Âº∫ÂåñÂ≠¶‰π†/PPO/PPO/miner_new.py:103\u001b[0m, in \u001b[0;36mOptimizedSynergisticFactorMiner.mine_factors\u001b[0;34m(self, n_iterations, batch_size, train_interval, print_interval, early_stop_patience, min_delta)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmine_factors\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     83\u001b[0m                 n_iterations: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m     84\u001b[0m                 batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 early_stop_patience: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     88\u001b[0m                 min_delta: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-4\u001b[39m):\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    ‰∏ªÊåñÊéòÂæ™ÁéØ\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m        Âõ†Â≠êÊ±†ÂàóË°®\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcore\u001b[38;5;241m.\u001b[39mmine_factors(\n\u001b[1;32m    104\u001b[0m         n_iterations\u001b[38;5;241m=\u001b[39mn_iterations,\n\u001b[1;32m    105\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    106\u001b[0m         train_interval\u001b[38;5;241m=\u001b[39mtrain_interval,\n\u001b[1;32m    107\u001b[0m         print_interval\u001b[38;5;241m=\u001b[39mprint_interval,\n\u001b[1;32m    108\u001b[0m         early_stop_patience\u001b[38;5;241m=\u001b[39mearly_stop_patience,\n\u001b[1;32m    109\u001b[0m         min_delta\u001b[38;5;241m=\u001b[39mmin_delta\n\u001b[1;32m    110\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Âº∫ÂåñÂ≠¶‰π†/PPO/PPO/miner_core.py:727\u001b[0m, in \u001b[0;36mFactorMinerCore.mine_factors\u001b[0;34m(self, n_iterations, batch_size, train_interval, print_interval, early_stop_patience, min_delta)\u001b[0m\n\u001b[1;32m    724\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîÑ PPO Update #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mppo_update_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at iteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    725\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Buffer size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mppo_buffer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 727\u001b[0m train_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mppo_trainer\u001b[38;5;241m.\u001b[39mtrain_ppo_step(\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr_generator\u001b[38;5;241m.\u001b[39m_get_valid_actions\n\u001b[1;32m    729\u001b[0m )\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_stats:\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# üî• ÊâìÂç∞PPOËÆ≠ÁªÉËØ¶ÁªÜ‰ø°ÊÅØ\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  PPO Training Stats:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Âº∫ÂåñÂ≠¶‰π†/PPO/PPO/ppo_trainer.py:131\u001b[0m, in \u001b[0;36mPPOTrainer.train_ppo_step\u001b[0;34m(self, get_valid_actions_fn)\u001b[0m\n\u001b[1;32m    127\u001b[0m         type_logits_batch, action_logits_batch, values_batch \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m    128\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_critic\u001b[38;5;241m.\u001b[39mforward_batch(batch_states)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     type_logits_batch, action_logits_batch, values_batch \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m--> 131\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_critic\u001b[38;5;241m.\u001b[39mforward_batch(batch_states)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action_logits_batch\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Âº∫ÂåñÂ≠¶‰π†/PPO/PPO/networks.py:283\u001b[0m, in \u001b[0;36mActorCriticNetwork.forward_batch\u001b[0;34m(self, states_list)\u001b[0m\n\u001b[1;32m    280\u001b[0m     padded_states[i, :seq_len] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    281\u001b[0m     padding_mask[i, :seq_len] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(padded_states, padding_mask)\n",
      "File \u001b[0;32m~/Desktop/Âº∫ÂåñÂ≠¶‰π†/PPO/PPO/networks.py:260\u001b[0m, in \u001b[0;36mActorCriticNetwork.forward\u001b[0;34m(self, x, padding_mask)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 260\u001b[0m     features, sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor(x, padding_mask)\n\u001b[1;32m    261\u001b[0m     type_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_head(features)\n\u001b[1;32m    262\u001b[0m     action_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_head(features)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/Âº∫ÂåñÂ≠¶‰π†/PPO/PPO/networks.py:143\u001b[0m, in \u001b[0;36mLSTMFeatureExtractor.forward\u001b[0;34m(self, x, padding_mask)\u001b[0m\n\u001b[1;32m    140\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    142\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[0;32m--> 143\u001b[0m lstm_output, (h_n, c_n) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(embedded)\n\u001b[1;32m    144\u001b[0m lstm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(lstm_output)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1124\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[1;32m   1125\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1126\u001b[0m         hx,\n\u001b[1;32m   1127\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m   1130\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout,\n\u001b[1;32m   1131\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first,\n\u001b[1;32m   1134\u001b[0m     )\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1138\u001b[0m         batch_sizes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m   1146\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# ÂºÄÂßãËøêË°åÊåñÊéòÂæ™ÁéØ\n",
    "# ---------------------------------------------------------\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "results = miner.mine_factors(\n",
    "    n_iterations=1000,\n",
    "    batch_size=16,\n",
    "    train_interval=TRAIN_INTERVAL,\n",
    "    print_interval=5,           # ÊØè5Ê¨°ÊâìÂç∞‰∏ÄÊ¨°ËøõÂ∫¶\n",
    "    early_stop_patience=1000      # Êó©ÂÅúËÄêÂøÉÂÄº\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nüéâ ÊåñÊéòÂÆåÊàêÔºÅËÄóÊó∂: {end_time - start_time:.2f} Áßí\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# ÁªòÂà∂ËÆ≠ÁªÉÂéÜÂè≤Êõ≤Á∫ø\n",
    "# ---------------------------------------------------------\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"üìä Ê≠£Âú®ÁªòÂà∂ËÆ≠ÁªÉÂéÜÂè≤...\")\n",
    "miner.plot_training_history()\n",
    "\n",
    "# ÂàÜÊûêÊÄßËÉΩË°∞ÈÄÄÊÉÖÂÜµ\n",
    "miner.analyze_performance_degradation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# ÂØπÊúÄ‰Ω≥Âõ†Â≠êÁªÑÂêàËøõË°åÂõûÊµã\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# ÂàõÂª∫ÂõûÊµãÂô®\n",
    "backtester = MinerBacktester(miner)\n",
    "\n",
    "# Âú®ÊµãËØïÈõÜ‰∏äËøêË°åÂõûÊµãÂπ∂ÁîªÂõæ\n",
    "print(\"\\nüìà ÂºÄÂßãÊµãËØïÈõÜÂõûÊµã...\")\n",
    "test_results = backtester.run(\n",
    "    data_split='test', \n",
    "    top_n=5, \n",
    "    save_path='test_backtest_result.png'\n",
    ")\n",
    "\n",
    "# ÊâìÂç∞ËØ¶ÁªÜÁöÑÂçïÂõ†Â≠êË°®Áé∞\n",
    "print(\"\\nüìã ÊúÄ‰Ω≥ÂçïÂõ†Â≠êË°®Áé∞ (Top 5):\")\n",
    "individual_res = backtester.get_individual_results('test')\n",
    "if individual_res:\n",
    "    sorted_factors = sorted(individual_res.items(), key=lambda x: x[1]['sharpe_ratio'], reverse=True)[:5]\n",
    "    for name, metrics in sorted_factors:\n",
    "        print(f\"  {name}: Sharpe={metrics['sharpe_ratio']:.2f}, IC={metrics['ic']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
