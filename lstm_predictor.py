"""
LSTMå› å­é¢„æµ‹å™¨æ¨¡å— - åœ¨PPOè®­ç»ƒç»“æŸåå¯¹æœ€ä½³å› å­ç»„åˆè¿›è¡Œè®­ç»ƒ
åŠŸèƒ½ï¼š
1. æ¥æ”¶PPOæŒ–æ˜çš„æœ€ä½³å› å­ç»„åˆ
2. ä½¿ç”¨LSTMå­¦ä¹ å› å­ç»„åˆçš„æ—¶åºæ¨¡å¼
3. ç”Ÿæˆæœ€ç»ˆçš„é¢„æµ‹ä¿¡å·å’Œäº¤æ˜“ç­–ç•¥

å®ç›˜é€‚é…é‡ç‚¹ï¼š
- æ¶ˆé™¤æ ‡å‡†åŒ–è¿‡ç¨‹ä¸­çš„æœªæ¥å‡½æ•° (Stateful Normalization)
- æ‰¹é‡æ¨ç†åŠ é€Ÿ (Batch Inference)
- æ··åˆæŸå¤±å‡½æ•° (MSE + IC)
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Tuple, Optional, Union
import logging
from pathlib import Path
import json
import matplotlib.pyplot as plt
from scipy.stats import spearmanr

# ç¡®ä¿ä» networks å¯¼å…¥ LSTMFactorCombiner
# å¦‚æœ combiner ä¸­æ²¡æœ‰å¯¼å‡º compute_factor_from_tokensï¼Œéœ€è¦ç¡®ä¿è¯¥è¾…åŠ©å‡½æ•°å¯ç”¨
try:
    from networks import LSTMFactorCombiner
except ImportError:
    # ç®€å•çš„ fallback å®šä¹‰ï¼Œé˜²æ­¢å¯¼å…¥æŠ¥é”™ï¼ˆå®é™…ä½¿ç”¨æ—¶åº”ç¡®ä¿ networks.py å­˜åœ¨ï¼‰
    class LSTMFactorCombiner(nn.Module):
        def __init__(self, n_factors, hidden_dim, lstm_layers, dropout):
            super().__init__()
            self.lstm = nn.LSTM(n_factors, hidden_dim, lstm_layers, batch_first=True, dropout=dropout)
            self.head = nn.Linear(hidden_dim, 1)
        def forward(self, x):
            out, _ = self.lstm(x)
            return self.head(out)

from config import TrainingConfig

logger = logging.getLogger(__name__)


# ==========================================
# è¾…åŠ©å·¥å…·ç±»
# ==========================================

class ICLoss(nn.Module):
    """IC æŸå¤±å‡½æ•° (Pearson Correlation Loss) - ç”¨äºæœ€å¤§åŒ–é¢„æµ‹å€¼ä¸ç›®æ ‡çš„ç›¸å…³æ€§"""
    def __init__(self):
        super().__init__()

    def forward(self, preds, targets):
        # å½’ä¸€åŒ– (Batch å†…)
        preds_mean = preds.mean()
        targets_mean = targets.mean()
        
        preds_centered = preds - preds_mean
        targets_centered = targets - targets_mean
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        numerator = torch.sum(preds_centered * targets_centered)
        denominator = torch.sqrt(torch.sum(preds_centered ** 2)) * torch.sqrt(torch.sum(targets_centered ** 2))
        
        pearson_corr = numerator / (denominator + 1e-8)
        
        # æˆ‘ä»¬å¸Œæœ›ç›¸å…³æ€§æœ€å¤§åŒ–(æ¥è¿‘1)ï¼Œæ‰€ä»¥ Loss = 1 - Correlation
        return 1.0 - pearson_corr


class FactorSequenceDataset(Dataset):
    """å› å­åºåˆ—æ•°æ®é›† - æ”¯æŒè®­ç»ƒå’Œæ¨ç†æ¨¡å¼"""

    def __init__(self, factor_values: np.ndarray, targets: Optional[np.ndarray] = None,
                 sequence_length: int = 20):
        """
        Args:
            factor_values: [T, n_factors] å› å­çŸ©é˜µ
            targets: [T] ç›®æ ‡æ”¶ç›Šç‡ (å¯é€‰)
            sequence_length: åºåˆ—çª—å£é•¿åº¦
        """
        self.factor_values = torch.FloatTensor(factor_values)
        self.targets = torch.FloatTensor(targets) if targets is not None else None
        self.sequence_length = sequence_length
        
        # æœ‰æ•ˆæ ·æœ¬æ•° = æ€»é•¿åº¦ - çª—å£é•¿åº¦ + 1
        self.n_samples = len(factor_values) - sequence_length + 1

    def __len__(self):
        return max(0, self.n_samples)

    def __getitem__(self, idx):
        # è¾“å…¥: ä» idx åˆ° idx+seq_len çš„çª—å£
        x = self.factor_values[idx : idx + self.sequence_length]
        
        if self.targets is not None:
            # ç›®æ ‡: åºåˆ—æœ€åä¸€ä¸ªæ—¶é—´æ­¥å¯¹åº”çš„æ”¶ç›Š (å‡è®¾ targets å·²ç»æ˜¯é¢„å…ˆå¯¹é½å¥½çš„æœªæ¥æ”¶ç›Š)
            y = self.targets[idx + self.sequence_length - 1]
            return x, y
        else:
            return x


# ==========================================
# ä¸»ç±»ï¼šLSTMFactorPredictor
# ==========================================

class LSTMFactorPredictor:
    """LSTMå› å­é¢„æµ‹å™¨ - å…·å¤‡å®ç›˜èƒ½åŠ›çš„ç‹¬ç«‹é¢„æµ‹æ¨¡å—"""

    def __init__(self, config: TrainingConfig = None):
        self.config = config or TrainingConfig()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        self.model = None
        self.n_factors = None
        
        # ğŸ”¥ å…³é”®ï¼šä¿å­˜è®­ç»ƒé›†çš„ç»Ÿè®¡é‡ï¼Œç”¨äºæ ‡å‡†åŒ–éªŒè¯é›†å’Œå®ç›˜æ•°æ®ï¼Œé˜²æ­¢æœªæ¥å‡½æ•°
        self.factor_stats = {}  # {'mean': Series, 'std': Series}
        
        self.training_history = {
            'train_loss': [], 'val_loss': [],
            'train_ic': [], 'val_ic': [],
            'train_sharpe': [], 'val_sharpe': [],
        }

        logger.info(f"ğŸ“Š LSTM Predictor initialized on {self.device}")

    def prepare_factor_matrix(self, alpha_pool: List[Dict], data: pd.DataFrame,
                              operators: Dict, fit_scaler: bool = False) -> pd.DataFrame:
        """
        å‡†å¤‡å› å­çŸ©é˜µå¹¶è¿›è¡Œæ ‡å‡†åŒ–
        
        Args:
            alpha_pool: å› å­æ± ä¿¡æ¯
            data: åŸå§‹è¡Œæƒ…æ•°æ®
            operators: ç®—å­å­—å…¸
            fit_scaler: True=è®¡ç®—å¹¶ä¿å­˜ç»Ÿè®¡é‡(è®­ç»ƒé›†); False=å¤ç”¨ç»Ÿè®¡é‡(éªŒè¯/æµ‹è¯•/å®ç›˜)
        
        Returns:
            æ ‡å‡†åŒ–åçš„å› å­ DataFrame
        """
        # å¯¼å…¥å› å­è®¡ç®—å‡½æ•°
        from factor_computation import compute_factor_from_tokens

        factor_dict = {}

        # 1. è®¡ç®—åŸå§‹å› å­å€¼
        for i, alpha_info in enumerate(alpha_pool):
            tokens = alpha_info['tokens']
            try:
                # è°ƒç”¨ combiner ä¸­çš„è®¡ç®—é€»è¾‘
                factor = compute_factor_from_tokens(tokens, data, operators)
                
                if factor is None:
                    logger.warning(f"Factor {i} computation returned None, using zeros")
                    factor_dict[f'factor_{i}'] = pd.Series(0.0, index=data.index)
                else:
                    factor_dict[f'factor_{i}'] = factor
            except Exception as e:
                logger.warning(f"Factor {i} computation failed: {e}, using zeros")
                factor_dict[f'factor_{i}'] = pd.Series(0.0, index=data.index)

        factor_matrix = pd.DataFrame(factor_dict)
        
        # 2. åŸºç¡€æ¸…æ´—ï¼šå¤„ç† Inf å’Œ NaN (å› æœå¡«å……)
        factor_matrix = factor_matrix.replace([np.inf, -np.inf], np.nan)
        factor_matrix = factor_matrix.ffill().fillna(0.0)

        # 3. æ ‡å‡†åŒ– (Strict No-Lookahead Bias)
        if fit_scaler:
            # è®­ç»ƒæ¨¡å¼ï¼šè®¡ç®—å¹¶ä¿å­˜ç»Ÿè®¡é‡
            self.factor_stats['mean'] = factor_matrix.mean()
            self.factor_stats['std'] = factor_matrix.std() + 1e-8
            logger.info("âœ… Computed and saved factor normalization stats from Training Data")
        
        # æ£€æŸ¥ç»Ÿè®¡é‡æ˜¯å¦å­˜åœ¨
        if not self.factor_stats:
            # å¦‚æœæ²¡æœ‰ç»Ÿè®¡é‡ï¼ˆä¾‹å¦‚ç›´æ¥é¢„æµ‹è€ŒæœªåŠ è½½æ¨¡å‹ï¼‰ï¼Œå‘å‡ºè­¦å‘Šå¹¶ä½¿ç”¨å½“å‰æ•°æ®ï¼ˆæœ‰é£é™©ï¼‰
            logger.warning("âš ï¸ Normalization stats not found! Using current batch stats (RISKY for Val/Test!)")
            current_mean = factor_matrix.mean()
            current_std = factor_matrix.std() + 1e-8
        else:
            # ä½¿ç”¨ä¿å­˜çš„ç»Ÿè®¡é‡
            current_mean = self.factor_stats['mean']
            current_std = self.factor_stats['std']

        # åº”ç”¨æ ‡å‡†åŒ– (Z-Score)
        # æ³¨æ„ï¼šå¯¹é½åˆ—åï¼Œé˜²æ­¢ alpha_pool å˜åŒ–å¯¼è‡´ key error
        try:
            factor_matrix = (factor_matrix - current_mean) / current_std
        except Exception as e:
            logger.error(f"Normalization alignment error: {e}. Using raw values.")
            
        # æˆªæ–­æå€¼ (Clip outliers)
        factor_matrix = factor_matrix.clip(-5, 5)

        logger.info(f"âœ… Prepared factor matrix: {factor_matrix.shape}")
        return factor_matrix

    def build_model(self, n_factors: int):
        """æ„å»ºLSTMæ¨¡å‹ç½‘ç»œ"""
        self.n_factors = n_factors

        self.model = LSTMFactorCombiner(
            n_factors=n_factors,
            hidden_dim=self.config.combiner_hidden_dim,
            lstm_layers=self.config.combiner_lstm_layers,
            dropout=self.config.combiner_dropout
        ).to(self.device)

        logger.info(f"ğŸ—ï¸ Built LSTM model: {n_factors} factors -> {self.config.combiner_hidden_dim}D hidden")
        logger.info(f"   Parameters: {sum(p.numel() for p in self.model.parameters()):,}")

    def train(self, train_factors: pd.DataFrame, train_targets: pd.Series,
              val_factors: pd.DataFrame, val_targets: pd.Series,
              epochs: int = 100, batch_size: int = 64, sequence_length: int = 20,
              early_stop_patience: int = 15) -> Dict:
        """
        è®­ç»ƒ LSTM æ¨¡å‹
        """
        if self.model is None:
            self.build_model(train_factors.shape[1])

        # 1. æ„å»ºæ•°æ®é›†
        train_dataset = FactorSequenceDataset(train_factors.values, train_targets.values, sequence_length)
        val_dataset = FactorSequenceDataset(val_factors.values, val_targets.values, sequence_length)

        # è®­ç»ƒé›†æ‰“ä¹±ï¼ŒéªŒè¯é›†ä¸æ‰“ä¹±
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

        # 2. ä¼˜åŒ–å™¨ä¸è°ƒåº¦å™¨
        optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config.combiner_lr,
            weight_decay=self.config.combiner_weight_decay
        )
        
        # å­¦ä¹ ç‡è¡°å‡
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=5, verbose=True
        )

        # 3. æŸå¤±å‡½æ•°ï¼šMSE + IC Loss
        mse_criterion = nn.MSELoss()
        ic_criterion = ICLoss()

        logger.info(f"ğŸš€ Starting LSTM training | Train: {len(train_dataset)} samples | Val: {len(val_dataset)} samples")

        best_val_score = -float('inf')  # ä¼˜å…ˆä¼˜åŒ– IC
        best_val_loss = float('inf')
        no_improve_count = 0
        best_model_state = None

        # 4. è®­ç»ƒå¾ªç¯
        for epoch in range(epochs):
            # --- Training ---
            self.model.train()
            train_losses = []
            train_preds_all = []
            train_targets_all = []

            for x_batch, y_batch in train_loader:
                x_batch = x_batch.to(self.device)
                y_batch = y_batch.to(self.device)

                optimizer.zero_grad()

                # å‰å‘ä¼ æ’­: å–åºåˆ—æœ€åä¸€æ­¥çš„è¾“å‡º [batch, 1]
                predictions = self.model(x_batch)[:, -1]
                
                # æ··åˆ Loss: 0.5 MSE + 0.5 IC Loss
                # MSE ä¿è¯æ•°å€¼ç¨³å®šæ€§ï¼ŒIC Loss ä¿è¯æ’åºèƒ½åŠ›
                loss = mse_criterion(predictions, y_batch) + 0.5 * ic_criterion(predictions, y_batch)

                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()

                train_losses.append(loss.item())
                train_preds_all.append(predictions.detach().cpu().numpy())
                train_targets_all.append(y_batch.detach().cpu().numpy())

            # --- Validation ---
            self.model.eval()
            val_losses = []
            val_preds_all = []
            val_targets_all = []

            with torch.no_grad():
                for x_batch, y_batch in val_loader:
                    x_batch = x_batch.to(self.device)
                    y_batch = y_batch.to(self.device)

                    predictions = self.model(x_batch)[:, -1]
                    
                    loss = mse_criterion(predictions, y_batch) + 0.5 * ic_criterion(predictions, y_batch)
                    val_losses.append(loss.item())
                    
                    val_preds_all.append(predictions.cpu().numpy())
                    val_targets_all.append(y_batch.cpu().numpy())

            # --- Metrics Calculation ---
            train_loss = np.mean(train_losses)
            val_loss = np.mean(val_losses)
            
            # åˆå¹¶ Batch ç»“æœ
            train_preds_flat = np.concatenate(train_preds_all)
            train_targets_flat = np.concatenate(train_targets_all)
            val_preds_flat = np.concatenate(val_preds_all)
            val_targets_flat = np.concatenate(val_targets_all)

            # è®¡ç®— IC (Spearman Rank Correlation)
            train_ic = spearmanr(train_preds_flat, train_targets_flat)[0]
            val_ic = spearmanr(val_preds_flat, val_targets_flat)[0]
            train_ic = train_ic if not np.isnan(train_ic) else 0.0
            val_ic = val_ic if not np.isnan(val_ic) else 0.0

            # è®¡ç®— Sharpe (è¿‘ä¼¼å€¼)
            val_returns = val_preds_flat * val_targets_flat
            val_sharpe = np.mean(val_returns) / (np.std(val_returns) + 1e-8) * np.sqrt(252 * 24) # å‡è®¾å°æ—¶çº¿æ•°æ®

            # è®°å½•å†å²
            self.training_history['train_loss'].append(train_loss)
            self.training_history['val_loss'].append(val_loss)
            self.training_history['train_ic'].append(train_ic)
            self.training_history['val_ic'].append(val_ic)
            self.training_history['val_sharpe'].append(val_sharpe)

            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(val_loss)

            # æ—©åœæ£€æŸ¥ (ä»¥ IC ä¸ºå‡†)
            current_score = val_ic
            if current_score > best_val_score:
                best_val_score = current_score
                best_val_loss = val_loss
                best_model_state = self.model.state_dict()
                no_improve_count = 0
                
                logger.info(f"âœ¨ Epoch {epoch+1}/{epochs} - New Best Val IC: {val_ic:.4f} (Loss: {val_loss:.5f})")
            else:
                no_improve_count += 1

            # å®šæœŸæ—¥å¿—
            if (epoch + 1) % 10 == 0:
                logger.info(f"Epoch {epoch+1} | Train IC: {train_ic:.4f} | Val IC: {val_ic:.4f} | Val Sharpe: {val_sharpe:.2f}")

            if no_improve_count >= early_stop_patience:
                logger.info(f"ğŸ›‘ Early stopping at epoch {epoch+1}")
                break

        # æ¢å¤æœ€ä½³æ¨¡å‹
        if best_model_state is not None:
            self.model.load_state_dict(best_model_state)
            logger.info(f"âœ… Restored best model with Val IC={best_val_score:.4f}")

        return {
            'best_val_loss': best_val_loss,
            'best_val_ic': best_val_score,
            'epochs_trained': epoch + 1,
            'final_val_ic': val_ic,
        }

    def predict(self, factor_matrix: pd.DataFrame, sequence_length: int = 20, batch_size: int = 1024) -> pd.Series:
        """
        ç”Ÿæˆé¢„æµ‹ä¿¡å· (æ‰¹é‡åŠ é€Ÿç‰ˆ)
        
        Args:
            factor_matrix: å› å­çŸ©é˜µ (å¿…é¡»å·²æ ‡å‡†åŒ–)
            sequence_length: åºåˆ—é•¿åº¦
            batch_size: æ¨ç†æ‰¹å¤§å°
            
        Returns:
            é¢„æµ‹ç»“æœ Series (ç´¢å¼•ä¸è¾“å…¥å¯¹é½ï¼Œå‰éƒ¨å¡«å……0)
        """
        if self.model is None:
            raise ValueError("Model not trained yet")

        self.model.eval()
        
        # ä½¿ç”¨ Dataset å°è£…æ•°æ®ï¼Œè‡ªåŠ¨å¤„ç†åˆ‡ç‰‡
        dataset = FactorSequenceDataset(factor_matrix.values, targets=None, sequence_length=sequence_length)
        
        # ä½¿ç”¨ DataLoader è¿›è¡Œæ‰¹é‡æ¨ç† (num_workers=0 é¿å…å¤šè¿›ç¨‹å¼€é”€ï¼Œå¯¹äºçº¯è®¡ç®—é€šå¸¸å¤Ÿå¿«)
        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=False)
        
        all_predictions = []
        
        with torch.no_grad():
            for x_batch in loader:
                x_batch = x_batch.to(self.device)
                # æ¨¡å‹è¾“å‡º [batch, seq_len, 1] -> å–æœ€åä¸€æ­¥ -> [batch]
                preds = self.model(x_batch)[:, -1]
                all_predictions.extend(preds.cpu().numpy())
                
        # æ•°æ®å¯¹é½ï¼šç”±äº Dataset ä¼šæ¶ˆè€—æ‰å‰ (sequence_length - 1) ä¸ªç‚¹ï¼Œéœ€è¦è¡¥ 0
        pad_length = sequence_length - 1
        full_predictions = [0.0] * pad_length + all_predictions
        
        # ç¡®ä¿é•¿åº¦ä¸€è‡´
        if len(full_predictions) != len(factor_matrix):
            logger.warning(f"Prediction length mismatch: {len(full_predictions)} vs {len(factor_matrix)}")
            # æˆªæ–­æˆ–å¡«å……é€»è¾‘
            full_predictions = full_predictions[:len(factor_matrix)]
        
        return pd.Series(full_predictions, index=factor_matrix.index)

    def generate_signals(self, predictions: pd.Series,
                        lookback: int = 100,
                        q_low: float = 0.3,
                        q_high: float = 0.7,
                        max_position: float = 1.0) -> pd.Series:
        """
        æ ¹æ®é¢„æµ‹å€¼ç”Ÿæˆäº¤æ˜“ä¿¡å· (Rolling Quantile Strategy)
        """
        # æ»šåŠ¨è®¡ç®—åˆ†ä½æ•°ï¼Œé€‚åº”å¸‚åœºä½“åˆ¶è½¬æ¢
        roll = predictions.rolling(lookback, min_periods=20)
        low_thresh = roll.quantile(q_low)
        high_thresh = roll.quantile(q_high)
        mid_val = roll.median()

        signals = pd.Series(0.0, index=predictions.index)

        # åšå¤šï¼šé¢„æµ‹å€¼ > é«˜åˆ†ä½æ•°
        signals[predictions > high_thresh] = max_position
        
        # åšç©ºï¼šé¢„æµ‹å€¼ < ä½åˆ†ä½æ•°
        signals[predictions < low_thresh] = -max_position

        # ä¸­æ€§åŒºåŸŸï¼šå¾®å¼±æŒä»“ (å¯é€‰)
        # mask_neutral = (predictions >= low_thresh) & (predictions <= high_thresh)
        # signals[mask_neutral & (predictions > mid_val)] = max_position * 0.1
        # signals[mask_neutral & (predictions < mid_val)] = -max_position * 0.1

        return signals.fillna(0.0)

    def evaluate(self, factor_matrix: pd.DataFrame, targets: pd.Series,
                sequence_length: int = 20) -> Dict:
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½ (Backtest)
        """
        # ç”Ÿæˆé¢„æµ‹
        predictions = self.predict(factor_matrix, sequence_length)

        # å¯¹é½æ•°æ®
        aligned = pd.DataFrame({
            'pred': predictions,
            'target': targets
        }).dropna()

        if len(aligned) < 100:
            return {'error': 'insufficient_data'}

        # 1. åŸºç¡€ IC
        ic = spearmanr(aligned['pred'], aligned['target'])[0]
        ic = 0.0 if np.isnan(ic) else ic

        # 2. é¢„æµ‹å€¼ Sharpe
        rets = aligned['pred'] * aligned['target']
        pred_sharpe = rets.mean() / (rets.std() + 1e-8) * np.sqrt(252 * 24)

        # 3. ç­–ç•¥å›æµ‹
        signals = self.generate_signals(predictions)
        # å¯¹é½ä¿¡å·å’Œæ”¶ç›Š (å‡è®¾ signal æ˜¯åŸºäº t æ—¶åˆ»ä¿¡æ¯ï¼Œtarget æ˜¯ t+1 æ”¶ç›Š)
        strat_rets = signals * targets
        strat_rets = strat_rets.dropna()
        
        strat_sharpe = 0.0
        cum_ret = 0.0
        if len(strat_rets) > 0:
            strat_sharpe = strat_rets.mean() / (strat_rets.std() + 1e-8) * np.sqrt(252 * 24)
            cum_ret = (1 + strat_rets).prod() - 1

        return {
            'ic': ic,
            'sharpe': pred_sharpe,
            'signal_sharpe': strat_sharpe,
            'cumulative_return': cum_ret,
            'n_samples': len(aligned),
        }

    def save_model(self, save_path: str):
        """ä¿å­˜æ¨¡å‹å’Œå…³é”®ç»Ÿè®¡é‡"""
        if self.model is None:
            logger.warning("No model to save")
            return

        save_dict = {
            'model_state': self.model.state_dict(),
            'n_factors': self.n_factors,
            'factor_stats': self.factor_stats, # ğŸ”¥ å¿…é¡»ä¿å­˜ç»Ÿè®¡é‡ï¼Œå¦åˆ™æ— æ³•æ­£ç¡®æ¨ç†
            'config': {
                'hidden_dim': self.config.combiner_hidden_dim,
                'lstm_layers': self.config.combiner_lstm_layers,
                'dropout': self.config.combiner_dropout,
            },
            'training_history': self.training_history,
        }

        torch.save(save_dict, save_path)
        logger.info(f"ğŸ’¾ Saved LSTM model and stats to {save_path}")

    def load_model(self, load_path: str):
        """åŠ è½½æ¨¡å‹"""
        save_dict = torch.load(load_path, map_location=self.device)

        self.n_factors = save_dict['n_factors']
        self.factor_stats = save_dict.get('factor_stats', {}) # ğŸ”¥ æ¢å¤ç»Ÿè®¡é‡
        
        # é‡å»ºæ¨¡å‹ç»“æ„
        self.build_model(self.n_factors)
        self.model.load_state_dict(save_dict['model_state'])
        self.training_history = save_dict.get('training_history', {})

        logger.info(f"ğŸ“‚ Loaded LSTM model from {load_path}")
        logger.info(f"   Factors: {self.n_factors}")
        logger.info(f"   Normalization Stats Present: {bool(self.factor_stats)}")

    def plot_training_history(self, save_path: str = 'lstm_training_history.png'):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        if not self.training_history['train_loss']:
            return

        fig, axes = plt.subplots(2, 2, figsize=(14, 10))

        # Loss
        axes[0, 0].plot(self.training_history['train_loss'], label='Train')
        axes[0, 0].plot(self.training_history['val_loss'], label='Val')
        axes[0, 0].set_title('Loss (MSE + IC)')
        axes[0, 0].legend()

        # IC
        axes[0, 1].plot(self.training_history['train_ic'], label='Train')
        axes[0, 1].plot(self.training_history['val_ic'], label='Val')
        axes[0, 1].set_title('IC (Spearman)')
        axes[0, 1].axhline(0, color='red', linestyle='--')
        axes[0, 1].legend()

        # Sharpe
        axes[1, 0].plot(self.training_history['val_sharpe'], label='Val Sharpe', color='green')
        axes[1, 0].set_title('Validation Sharpe Ratio')
        axes[1, 0].legend()

        # Summary
        axes[1, 1].axis('off')
        best_ic = max(self.training_history['val_ic']) if self.training_history['val_ic'] else 0
        info_text = f"Best Val IC: {best_ic:.4f}\nEpochs: {len(self.training_history['train_loss'])}"
        axes[1, 1].text(0.1, 0.5, info_text, fontsize=14)

        plt.tight_layout()
        plt.savefig(save_path)
        plt.close()